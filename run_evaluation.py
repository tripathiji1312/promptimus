import os
import subprocess
import json
import evaluate
import agent
import gc  # Import the garbage collector library

# --- 1. DEFINE PROMPTS AND REFERENCE ANSWERS ---
# (This section remains the same)
EVAL_PROMPTS = [
    {
        "id": 1,
        "prompt": "Create a new Git branch and switch to it.",
        "reference": "To create a new branch and switch to it in one command, use `git checkout -b <branch_name>`."
    },
    {
        "id": 2,
        "prompt": "Compress the folder `reports` into `reports.tar.gz`.",
        "reference": "To compress a folder into a .tar.gz archive, use the command `tar -czvf reports.tar.gz reports`."
    },
    {
        "id": 3,
        "prompt": "List all Python files in the current directory recursively.",
        "reference": "To recursively find all files with a .py extension, use the command `find . -type f -name \"*.py\"`."
    },
    {
        "id": 4,
        "prompt": "Set up a virtual environment and install requests.",
        "reference": "First, create a virtual environment using `python3 -m venv myenv`. Then, activate it and install packages with `pip install requests`."
    },
    {
        "id": 5,
        "prompt": "Fetch only the first ten lines of a file named `output.log`.",
        "reference": "To get the first ten lines of a file, use the command `head -n 10 output.log`."
    },
    {
        "id": 6,
        "prompt": "Explain what `git stash` does and when to use it.",
        "reference": "`git stash` temporarily shelves changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on."
    },
    {
        "id": 7,
        "prompt": "Find all files in `src` modified in the last 2 days and create a compressed archive of them named 'recent.tar.gz'.",
        "reference": "You can find the files using `find src -mtime -2 -type f` and then pipe the results to `tar` to create the archive, for example: `find src -mtime -2 -type f -print0 | tar -czvf recent.tar.gz --null -T -`."
    }
]


# --- 2. HELPER FUNCTIONS FOR REPORT GENERATION ---
# (This section remains the same)
def generate_static_report(results):
    report_content = "# Static Evaluation: Base vs. Fine-Tuned Model\n\n"
    report_content += "This document compares the raw text output of the base TinyLlama model against the model fine-tuned on our custom command-line dataset.\n\n---\n\n"

    for res in results:
        report_content += f"### {res['id']}. {res['prompt']}\n\n"
        report_content += f"*   **Base Model Output:**\n    ```\n{res['base_output']}\n    ```\n"
        report_content += f"*   **Fine-Tuned Model Output:**\n    ```\n{res['tuned_output']}\n    ```\n"
        report_content += f"*   **Reference Answer:** `{res['reference']}`\n"
        report_content += f"*   **ROUGE-L Score:** {res['rouge_l']:.4f}\n\n---\n\n"

    with open("eval_static.md", "w", encoding="utf-8") as f:
        f.write(report_content)
    print("✓ Successfully generated eval_static.md")


def generate_dynamic_report(results):
    report_content = "# Dynamic Evaluation: Agent Execution\n\n"
    report_content += "This document scores the quality of the plans generated by the final agent and its ability to identify and dry-run the correct command.\n\n"
    report_content += "| ID | Prompt | Agent Output (Plan & Dry-Run) | Plan Quality (0-2) | Justification |\n"
    report_content += "|----|--------|-------------------------------|--------------------|---------------|\n"

    for i, res in enumerate(results):
        agent_output_md = f"<pre>\n{res.replace('|', '|')}\n</pre>"
        report_content += f"| {i + 1} | {EVAL_PROMPTS[i]['prompt']} | {agent_output_md} | (manual scoring needed) | (manual justification needed) |\n"

    with open("eval_dynamic.md", "w", encoding="utf-8") as f:
        f.write(report_content)
    print("✓ Successfully generated eval_dynamic.md")


# --- 3. RESTRUCTURED MAIN EVALUATION LOGIC ---

def main():
    print("Starting memory-efficient evaluation process...")

    base_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    adapter_path = "./tinyllama-cmd-adapter-final"

    # --- Step 1: Evaluate BASE model ---
    print("\n--- Loading and Evaluating Base Model ---")
    base_model, base_tokenizer = agent.load_model(base_model_id, adapter_path, use_adapter=False)
    base_outputs = {}
    for item in EVAL_PROMPTS:
        print(f"  Processing prompt {item['id']} for BASE model...")
        base_outputs[item['id']] = agent.generate_plan(base_model, base_tokenizer, item['prompt'])

    # --- Release memory ---
    print("Releasing base model from memory...")
    del base_model
    del base_tokenizer
    gc.collect()

    # --- Step 2: Evaluate FINE-TUNED model ---
    print("\n--- Loading and Evaluating Fine-Tuned Model ---")
    tuned_model, tuned_tokenizer = agent.load_model(base_model_id, adapter_path, use_adapter=True)
    tuned_outputs = {}
    for item in EVAL_PROMPTS:
        print(f"  Processing prompt {item['id']} for TUNED model...")
        tuned_outputs[item['id']] = agent.generate_plan(tuned_model, tuned_tokenizer, item['prompt'])

    # --- Release memory ---
    print("Releasing fine-tuned model from memory...")
    del tuned_model
    del tuned_tokenizer
    gc.collect()

    # --- Step 3: Run DYNAMIC evaluation (less memory intensive) ---
    print("\n--- Running Dynamic Evaluation ---")
    dynamic_results = []
    for item in EVAL_PROMPTS:
        print(f"  Processing prompt {item['id']} for DYNAMIC evaluation...")
        process = subprocess.run(
            ['python', 'agent.py', item['prompt']],
            capture_output=True,
            text=True
        )
        dynamic_results.append(process.stdout)

    # --- Step 4: Calculate scores and generate reports ---
    print("\n--- Calculating Scores and Generating Reports ---")
    rouge = evaluate.load('rouge')
    static_results = []
    for item in EVAL_PROMPTS:
        rouge_scores = rouge.compute(
            predictions=[tuned_outputs[item['id']]],
            references=[item['reference']]
        )
        static_results.append({
            "id": item['id'],
            "prompt": item['prompt'],
            "base_output": base_outputs[item['id']],
            "tuned_output": tuned_outputs[item['id']],
            "reference": item['reference'],
            "rouge_l": rouge_scores['rougeL']
        })

    generate_static_report(static_results)
    generate_dynamic_report(dynamic_results)

    print("\nEvaluation complete!")


if __name__ == "__main__":
    main()