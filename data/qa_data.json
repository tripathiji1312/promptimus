[
  {
    "instruction": "How do I undo the most recent local commits in Git?",
    "response": "Undo a commit & redo\n$ git commit -m \"Something terribly misguided\" # (0: Your Accident)\n$ git reset HEAD~                              # (1)\n# === If you just want to undo the commit, stop here! ===\n[ edit files as necessary ]                    # (2)\n$ git add .                                    # (3)\n$ git commit -c ORIG_HEAD                      # (4)\n\n`git reset` is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You'll need to add them again before you can commit them again.\nMake corrections to working tree files.\n`git add` anything that you want to include in your new commit.\nCommit the changes, reusing the old commit message. `reset` copied the old head to `.git/ORIG_HEAD`; `commit` with `-c ORIG_HEAD` will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the `-C` option.\n\nAlternatively, to edit the previous commit (or just its commit message), `commit --amend` will add changes within the current index to the previous commit.\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with `git push origin main --force[-with-lease]` is necessary. It's almost always a bad idea to use `--force`; prefer `--force-with-lease` instead, and as noted in the git manual:\n\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\n\nFurther Reading\nYou can use `git reflog` to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\n\n`HEAD~` is the same as `HEAD~1`. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits.",
    "question_score": 27056,
    "answer_score": 29863,
    "question_id": 927358,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "git-commit",
      "undo"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I delete a Git branch locally and remotely?",
    "response": "Executive Summary\ngit push -d     # Delete remote\ngit branch -d                # Delete local\n\nNote: In most cases, `` will be `origin`.\nDelete Local Branch\nTo delete the local branch, use one of the following:\ngit branch -d \ngit branch -D \n\nThe `-d` option is an alias for `--delete`, which only deletes the branch if it has already been fully merged in its upstream branch.\nThe `-D` option is an alias for `--delete --force`, which deletes the branch \"irrespective of its merged status.\" [Source: `man git-branch`]\nAs of Git v2.3, `git branch -d` (delete) learned to honor the `-f` (force) flag.\nYou will receive an error if you try to delete the currently selected branch.\n\nDelete Remote Branch\nAs of Git v1.7.0, you can delete a remote branch using\n$ git push  --delete \n\nwhich might be easier to remember than\n$ git push  :\n\nwhich was added in Git v1.5.0 \"to delete a remote branch or a tag.\"\nStarting with Git v2.8.0, you can also use `git push` with the `-d` option as an alias for `--delete`. Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax.\nDelete Remote Branch [Original Answer from 5-Jan-2010]\nFrom Chapter 3 of Pro Git by Scott Chacon:\n\nDeleting Remote Branches\nSuppose you’re done with a remote branch — say, you and your collaborators are finished with a feature and have merged it into your remote’s main branch (or whatever branch your stable code-line is in). You can delete a remote branch using the rather obtuse syntax `git push [remotename] :[branch]`. If you want to delete your `serverfix` branch from the server, you run the following:\n$ git push origin :serverfix\nTo git@github.com:schacon/simplegit.git\n - [deleted]         serverfix\n\nBoom. No more branches on your server. You may want to dog-ear this page, because you’ll need that command, and you’ll likely forget the syntax. A way to remember this command is by recalling the `git push [remotename] [localbranch]:[remotebranch]` syntax that we went over a bit earlier. If you leave off the `[localbranch]` portion, then you’re basically saying, “Take nothing on my side and make it be `[remotebranch]`.”\n\nI ran `git push origin :bugfix`, and it worked beautifully. Scott Chacon was right—I will want to dog-ear that page (or virtually dog ear-by answering this on Stack Overflow).\nFetch changes\nFinally, execute the following on other machines to propagate changes:\n# Fetch changes from all remotes and locally delete \n# remote deleted branches/tags etc\n# --prune will do the job :-;\ngit fetch --all --prune",
    "question_score": 20365,
    "answer_score": 26561,
    "question_id": 2003505,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "git-branch",
      "git-push",
      "git-remote"
    ],
    "topic": "git"
  },
  {
    "instruction": "How can I rename a local Git branch?",
    "response": "To rename the current branch:\ngit branch -m \n\nTo rename a branch while pointed to any branch:\ngit branch -m  \n\n`-m` is short for `--move`.\n\nTo push the  local branch and reset the upstream branch:\ngit push origin -u \n\nTo delete the  remote branch:\ngit push origin --delete \n\nTo create a `git rename` alias:\ngit config --global alias.rename 'branch -m'\n\nOn Windows or another case-insensitive filesystem, use `-M` if there are only capitalization changes in the name. Otherwise, Git will throw a \"branch already exists\" error.\ngit branch -M",
    "question_score": 11932,
    "answer_score": 19516,
    "question_id": 6591213,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "git-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How to modify existing, unpushed commit messages?",
    "response": "Amending the most recent commit message\ngit commit --amend\n\nwill open your editor, allowing you to change the commit message of the most recent commit. Additionally, you can set the commit message directly in the command line with:\ngit commit --amend -m \"New commit message\"\n\n…however, this can make multi-line commit messages or small corrections more cumbersome to enter.\nMake sure you don't have any working copy changes staged before doing this or they will get committed too. (Unstaged changes will not get committed.)\nChanging the message of a commit that you've already pushed to your remote branch\nIf you've already pushed your commit up to your remote branch, then - after amending your commit locally (as described above) - you'll also need to force push the commit with:\ngit push   --force\n# Or\ngit push   -f\n\nWarning: force-pushing will overwrite the remote branch with the state of your local one. If there are commits on the remote branch that you don't have in your local branch, you will lose those commits.\nWarning: be cautious about amending commits that you have already shared with other people. Amending commits essentially rewrites them to have different SHA IDs, which poses a problem if other people have copies of the old commit that you've rewritten. Anyone who has a copy of the old commit will need to synchronize their work with your newly re-written commit, which can sometimes be difficult, so make sure you coordinate with others when attempting to rewrite shared commit history, or just avoid rewriting shared commits altogether.\n\nPerform an interactive rebase\nAnother option is to use interactive rebase.\nThis allows you to edit any message you want to update even if it's not the latest message.\nIn order to do a Git squash, follow these steps:\n// n is the number of commits up to the last commit you want to be able to edit\ngit rebase -i HEAD~n\n\nOnce you squash your commits - choose the `e/r` for editing the message:\n\nImportant note about interactive rebase\nWhen you use `git rebase -i HEAD~n` there can be more than n commits. Git will \"collect\" all the commits in the last n commits, and if there was a merge somewhere in between that range you will see all the commits as well, so the outcome will be n + .\nGood tip:\nIf you have to do it for more than a single branch and you might face conflicts when amending the content, set up `git rerere` and let Git resolve those conflicts automatically for you.\n\nDocumentation\n\ngit-commit(1) Manual Page\n\ngit-rebase(1) Manual Page\n\ngit-push(1) Manual Page",
    "question_score": 7641,
    "answer_score": 18993,
    "question_id": 179123,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-commit",
      "git-rewrite-history",
      "git-amend"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I undo 'git add' before commit?",
    "response": "To unstage a specific file\ngit reset \n\nThat will remove the file from the current index (the \"about to be committed\" list) without changing anything else.\nTo unstage all files from the current change set:\ngit reset\n\nIn old versions of Git, the above commands are equivalent to `git reset HEAD ` and `git reset HEAD` respectively, and will fail if `HEAD` is undefined (because you haven't yet made any commits in your repository) or ambiguous (because you created a branch called `HEAD`, which is a stupid thing that you shouldn't do). This was changed in Git 1.8.2, though, so in modern versions of Git you can use the commands above even prior to making your first commit:\n\n\"git reset\" (without options or parameters) used to error out when\nyou do not have any commits in your history, but it now gives you\nan empty index (to match non-existent commit you are not even on).\n\nDocumentation: git reset",
    "question_score": 11553,
    "answer_score": 13796,
    "question_id": 348170,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-commit",
      "undo",
      "git-add"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I force \"git pull\" to overwrite local files?",
    "response": "⚠ Warning:\nAny uncommitted local change to tracked files will be lost, even if staged.\nBut any local file that's not tracked by Git will not be affected.\n\nFirst, update all `origin/` refs to latest:\ngit fetch --all\n\nBackup your current branch (e.g. `main`):\ngit branch backup-main\n\nJump to the latest commit on `origin/main` and checkout those files:\ngit reset --hard origin/main\n\nExplanation:\n`git fetch` downloads the latest from remote without trying to merge or rebase anything.\n`git reset` resets the master branch to what you just fetched. The `--hard` option changes all the files in your working tree to match the files in `origin/main`.\n\nMaintain current local commits\n[*]: It's worth noting that it is possible to maintain current local commits by creating a branch from `main` before resetting:\ngit checkout main\ngit branch new-branch-to-save-current-commits\ngit fetch --all\ngit reset --hard origin/main\n\nAfter this, all of the old commits will be kept in `new-branch-to-save-current-commits`.\nUncommitted changes\nUncommitted changes, even if staged (with `git add`), will be lost. Make sure to `stash` or commit anything you need. For example, run the following:\ngit stash\n\nAnd later (after `git reset`), reapply these uncommitted changes:\ngit stash pop\n\nWhich may create merge conflicts.",
    "question_score": 9899,
    "answer_score": 13548,
    "question_id": 1125968,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "overwrite",
      "git-pull",
      "git-fetch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I revert a Git repository to a previous commit?",
    "response": "This depends a lot on what you mean by \"revert\".\nTemporarily switch to a different commit\nIf you want to temporarily go back to it, fool around, then come back to where you are, all you have to do is check out the desired commit:\n# This will detach your HEAD, that is, leave you with no branch checked out:\ngit checkout 0d1d7fc32\n\nOr if you want to make commits while you're there, go ahead and make a new branch while you're at it:\ngit checkout -b old-state 0d1d7fc32\n\nTo go back to where you were, just check out the branch you were on again. (If you've made changes, as always when switching branches, you'll have to deal with them as appropriate. You could reset to throw them away; you could stash, checkout, stash pop to take them with you; you could commit them to a branch there if you want a branch there.)\nHard delete unpublished commits\nIf, on the other hand, you want to really get rid of everything you've done since then, there are two possibilities. One, if you haven't published any of these commits, simply reset:\n# This will destroy any local modifications.\n# Don't do it if you have uncommitted work you want to keep.\ngit reset --hard 0d1d7fc32\n\n# Alternatively, if there's work to keep:\ngit stash\ngit reset --hard 0d1d7fc32\ngit stash pop\n# This saves the modifications, then reapplies that patch after resetting.\n# You could get merge conflicts, if you've modified things which were\n# changed since the commit you reset to.\n\nIf you mess up, you've already thrown away your local changes, but you can at least get back to where you were before by resetting again.\nUndo published commits with new commits\nOn the other hand, if you've published the work, you probably don't want to reset the branch, since that's effectively rewriting history. In that case, you could indeed revert the commits. In many enterprise organisations, the concept of \"protected\" branches will even prevent history from being rewritten on some major branches. In this case, reverting is your only option.\nWith Git, revert has a very specific meaning: create a commit with the reverse patch to cancel it out. This way you don't rewrite any history.\nFirst figure out what commits to revert. Depending on the technique chosen below, you want to either revert only the merge commits, or only the non-merge commits.\n\n# This lists all merge commits between 0d1d7fc and HEAD:\ngit log --merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\n# This lists all non merge commits between 0d1d7fc and HEAD:\ngit log --no-merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\nNote: if you revert multiple commits, the order matters. Start with the most recent commit.\n# This will create three separate revert commits, use non merge commits only:\ngit revert a867b4af 25eee4ca 0766c053\n\n# It also takes ranges. This will revert the last two commits:\ngit revert HEAD~2..HEAD\n\n# Similarly, you can revert a range of commits using commit hashes (non inclusive of first hash):\ngit revert 0d1d7fc..a867b4a\n\n# Reverting a merge commit. You can also use a range of merge commits here.\ngit revert -m 1 \n\n# To get just one, you could use `rebase -i` to squash them afterwards\n# Or, you could do it manually (be sure to do this at top level of the repo)\n# get your index and work tree into the desired state, without changing HEAD:\ngit checkout 0d1d7fc32 .\n\n# Then commit. Be sure and write a good message describing what you just did\ngit commit\n\nThe `git-revert` manpage actually covers a lot of this in its description. Another useful link is this git-scm.com section discussing git-revert.\nIf you decide you didn't want to revert after all, you can revert the revert (as described here) or reset back to before the revert (see the previous section).\nYou may also find this answer helpful in this case:\nHow can I move HEAD back to a previous location? (Detached head) & Undo commits",
    "question_score": 7608,
    "answer_score": 12522,
    "question_id": 4114095,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-checkout",
      "git-reset",
      "git-revert"
    ],
    "topic": "git"
  },
  {
    "instruction": "How to check out a remote Git branch?",
    "response": "The answer has been split depending on whether there is one remote repository configured or multiple. The reason for this is that for the single remote case, some of the commands can be simplified as there is less ambiguity.\nUpdated for Git 2.23: For older versions, see the section at the end.\nWith One Remote\nIn both cases, start by fetching from the remote repository to make sure you have all the latest changes downloaded.\n$ git fetch\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n$ git branch -v -a\n\n...\nremotes/origin/test\n\nThe branches that start with `remotes/*` can be thought of as read only copies of the remote branches. To work on a branch you need to create a local branch from it. This is done with the Git command `switch` (since Git 2.23) by giving it the name of the remote branch (minus the remote name):\n$ git switch test\n\nIn this case Git is guessing (can be disabled with `--no-guess`) that you are trying to checkout and track the remote branch with the same name.\nWith Multiple Remotes\nIn the case where multiple remote repositories exist, the remote repository needs to be explicitly named.\nAs before, start by fetching the latest remote changes:\n$ git fetch origin\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n$ git branch -v -a\n\nWith the remote branches in hand, you now need to check out the branch you are interested in with `-c` to create a new local branch:\n$ git switch -c test origin/test\n\nFor more information about using `git switch`:\n$ man git-switch\n\nPrior to Git 2.23\n`git switch` was added in Git 2.23, prior to this `git checkout` was used to switch branches.\nTo checkout out with only a single remote repository:\ngit checkout test\n\nif there are multiple remote repositories configured then it becomes a bit longer\ngit checkout -b test /test",
    "question_score": 8841,
    "answer_score": 11905,
    "question_id": 1783405,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-checkout",
      "remote-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "What is the difference between 'git pull' and 'git fetch'?",
    "response": "In the simplest terms, `git pull` does a `git fetch` followed by a `git merge`.\n\n`git fetch` updates your remote-tracking branches under `refs/remotes//`. This operation is safe to run at any time since it never changes any of your local branches under `refs/heads`.\n`git pull` brings a local branch up-to-date with its remote version, while also updating your other remote-tracking branches.\nFrom the Git documentation for `git pull`:\n\n`git pull` runs `git fetch` with the given parameters and then depending on configuration options or command line flags, will call either `git rebase` or `git merge` to reconcile diverging branches.",
    "question_score": 13992,
    "answer_score": 11640,
    "question_id": 292357,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "git-pull",
      "git-fetch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I change the URI (URL) for a remote Git repository?",
    "response": "First, view the existing remotes to verify which URL is currently set:\ngit remote -v\n\nThen, you can set it with:\ngit remote set-url origin \n\nSee `git help remote`. You also can edit `.git/config` and change the URLs there.\nYou're not in any danger of losing history unless you do something very silly (and if you're worried, just make a copy of your repo, since your repo is your history.)",
    "question_score": 6786,
    "answer_score": 10901,
    "question_id": 2432764,
    "has_full_answer": true,
    "tags": [
      "git",
      "url",
      "git-remote"
    ],
    "topic": "git"
  },
  {
    "instruction": "Reset local repository branch to be just like remote repository HEAD",
    "response": "Setting your branch to exactly match the remote branch can be done in two steps:\n\ngit fetch origin\ngit reset --hard origin/master\n\nIf you want to save your current branch's state before doing this (just in case), you can do:\ngit commit -a -m \"Saving my work, just in case\"\ngit branch my-saved-work\n\nNow your work is saved on the branch \"my-saved-work\" in case you decide you want it back (or want to look at it later or diff it against your updated branch).\nNote: the first example assumes that the remote repo's name is `origin` and that the branch named `master` in the remote repo matches the currently checked-out branch in your local repo, since that is in line with the example given in the question. If you are trying to reset to the default branch in a more recent repository, it is likely that it will be `main`.\nBTW, this situation that you're in looks an awful lot like a common case where a push has been done into the currently checked out branch of a non-bare repository. Did you recently push into your local repo? If not, then no worries -- something else must have caused these files to unexpectedly end up modified. Otherwise, you should be aware that it's not recommended to push into a non-bare repository (and not into the currently checked-out branch, in particular).",
    "question_score": 5942,
    "answer_score": 10217,
    "question_id": 1628088,
    "has_full_answer": true,
    "tags": [
      "git",
      "undo"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I remove local (untracked) files from the current Git working tree?",
    "response": "git-clean - Remove untracked files from the working tree\nSynopsis\ngit clean [-d] [-f] [-i] [-n] [-q] [-e ] [-x | -X] [--] …​\n\nDescription\nCleans the working tree by recursively removing files that are not under version control, starting from the current directory.\nNormally, only files unknown to Git are removed, but if the `-x` option is specified, ignored files are also removed. This can, for example, be useful to remove all build products.\nIf any optional `...` arguments are given, only those paths are affected.\n\nStep 1 is to show what will be deleted by using the `-n` option:\n\n# Print out the list of files and directories which will be removed (dry run)\ngit clean -n -d\n\nClean Step - beware: this will delete files:\n# Delete the files from the repository\ngit clean -f\n\nTo remove directories, run `git clean -f -d` or `git clean -fd`\nTo remove ignored files, run `git clean -f -X` or `git clean -fX`\nTo remove ignored and non-ignored files, run `git clean -f -x` or `git clean -fx`\n\nNote the case difference on the `X` for the two latter commands.\nIf `clean.requireForce` is set to \"true\" (the default) in your configuration, one needs to specify `-f` otherwise nothing will actually happen.\nAgain see the `git-clean` docs for more information.\n\nOptions\n`-f`, `--force`\nIf the Git configuration variable clean.requireForce is not set to\nfalse, git clean will refuse to run unless given `-f`, `-n` or `-i`.\n`-x`\nDon’t use the standard ignore rules read from .gitignore (per\ndirectory) and `$GIT_DIR/info/exclude`, but do still use the ignore\nrules given with `-e` options. This allows removing all untracked files,\nincluding build products. This can be used (possibly in conjunction\nwith git reset) to create a pristine working directory to test a clean\nbuild.\n`-X`\nRemove only files ignored by Git. This may be useful to rebuild\neverything from scratch, but keep manually created files.\n`-n`, `--dry-run`\nDon’t actually remove anything, just show what would be done.\n`-d`\nRemove untracked directories in addition to untracked files. If an\nuntracked directory is managed by a different Git repository, it is\nnot removed by default. Use `-f` option twice if you really want to\nremove such a directory.",
    "question_score": 8338,
    "answer_score": 10026,
    "question_id": 61212,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "git-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I make Git forget about a file that was tracked, but is now in .gitignore?",
    "response": "`.gitignore` will prevent untracked files from being added (without an `add -f`) to the set of files tracked by Git. However, Git will continue to track any files that are already being tracked.\nUpdated Answer in 2024\nDo NOT use `git rm --cached ` if you ever want to see that file again. It will remove it from git, and also your local machine.\nIf you want to keep the file locally, but remove it from git tracking, use the answer by Konstantin. In short, use the following instead of `git rm`:\n`git update-index --skip-worktree `\nHowever, according to the official git documentation:\n\nUsers often try to use the assume-unchanged and skip-worktree bits to tell Git to ignore changes to files that are tracked. This does not work as expected, since Git may still check working tree files against the index when performing certain operations. In general, Git does not provide a way to ignore changes to tracked files, so alternate solutions are recommended.\n\nTherefore, you should still consider using the original answer below.\nOriginal Answer\nWARNING: This will remove the physical file from your local machine and other developers' machines on your or their next `git pull`.\nTo stop tracking a file, we must remove it from the index:\ngit rm --cached \n\nTo remove a folder and all files in the folder recursively:\ngit rm -r --cached \n\nThe removal of the file from the head revision will happen on the next commit.",
    "question_score": 8536,
    "answer_score": 9174,
    "question_id": 1274057,
    "has_full_answer": true,
    "tags": [
      "git",
      "gitignore",
      "git-rm"
    ],
    "topic": "git"
  },
  {
    "instruction": "Move the most recent commit(s) to a new branch with Git",
    "response": "WARNING: You need to store uncommitted edits to your stash before doing this, using `git stash`. Once complete, you can retrieve the stashed uncommitted edits with `git stash pop`. git reset hard command will remove all changes!\nMoving to an existing branch\nIf you want to move your commits to an existing branch, it will look like this:\ngit checkout existingbranch\ngit merge branchToMoveCommitFrom\ngit checkout branchToMoveCommitFrom\ngit reset --hard HEAD~3 # Go back 3 commits. You *will* lose uncommitted work.\ngit checkout existingbranch\n\nMoving to a new branch\nWARNING: This method works because you are creating a new branch with the first command: `git branch newbranch`. If you want to move commits to an existing branch you need to merge your changes into the existing branch before executing `git reset --hard HEAD~3` (see Moving to an existing branch above). If you don't merge your changes first, they will be lost.\nUnless there are other circumstances involved, this can be easily done by branching and rolling back.\n\n# Note: Any changes not committed will be lost.\ngit branch newbranch      # Create a new branch, saving the desired commits\ngit checkout master       # checkout master, this is the place you want to go back\ngit reset --hard HEAD~3   # Move master back by 3 commits (Make sure you know how many commits you need to go back)\ngit checkout newbranch    # Go to the new branch that still has the desired commits\n\nBut do make sure how many commits to go back. Alternatively, you can instead of `HEAD~3`, simply provide the hash of the commit (or the reference like origin/master) you want to \"revert back to\" on the master (/current) branch, e.g:\ngit reset --hard a1b2c3d4\n\nNote: You will only be \"losing\" commits from the master branch, but don't worry, you'll have those commits in newbranch! An easy way to check that, after completing the 4 step sequence of commands above, is by looking at `git log -n4` which will show the history of newbranch actually retained the 3 commits (and the reason is that newbranch was created at the time those changes were already commited on master!). They have only been removed from master, as `git reset` only affected the branch that was checked out at the time of its execution, i.e. master (see git reset description: Reset current HEAD to the specified state). `git status` however will not show any checkouts on the newbranch, which might be surprising at first but that is actually expected.\nLastly, you may need to force push your latest changes to main repo:\ngit push origin master --force\n\nWARNING: With Git version 2.0 and later, if you later `git rebase` the new branch upon the original (`master`) branch, you may need an explicit `--no-fork-point` option during the rebase to avoid losing the carried-over commits.  Having `branch.autosetuprebase always` set makes this more likely.  See John Mellor's answer for details.",
    "question_score": 6721,
    "answer_score": 8572,
    "question_id": 1628563,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-branch",
      "branching-and-merging"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I discard unstaged changes in Git?",
    "response": "Another quicker way is:\n\ngit stash save --keep-index --include-untracked\n\nYou don't need to include `--include-untracked` if you don't want to be thorough about it.\n\nAfter that, you can drop that stash with a `git stash drop` command if you like.",
    "question_score": 6433,
    "answer_score": 3171,
    "question_id": 52704,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "git-restore"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I push a new local branch to a remote Git repository and track it too?",
    "response": "In Git 1.7.0 and later, you can checkout a new branch:\ngit checkout -b \n\nEdit files, add and commit. Then push with the `-u` (short for `--set-upstream`) option:\ngit push -u origin \n\nGit will set up the tracking information during the push.",
    "question_score": 5805,
    "answer_score": 8596,
    "question_id": 2765421,
    "has_full_answer": true,
    "tags": [
      "git",
      "repository",
      "git-branch",
      "git-push",
      "git-remote"
    ],
    "topic": "git"
  },
  {
    "instruction": "How can I delete a remote tag?",
    "response": "You can push an 'empty' reference to the remote tag name:\ngit push origin :tagname\n\nOr, more expressively, use the `--delete` option (or `-d` if your git version is older than 1.8.0):\ngit push --delete origin tagname\n\nNote that git has tag namespace and branch namespace so you may use the same name for a branch and for a tag. If you want to make sure that you cannot accidentally remove the branch instead of the tag, you can specify full ref which will never delete a branch:\ngit push origin :refs/tags/tagname\n\nIf you also need to delete the local tag, use:\ngit tag --delete tagname\n\nor\ngit tag -d tagname\n\nBackground\nPushing a branch, tag, or other ref to a remote repository involves specifying \"which repo, what source, what destination?\"\ngit push remote-repo source-ref:destination-ref\n\nA real world example where you push your master branch to the origin's master branch is:\ngit push origin refs/heads/master:refs/heads/master\n\nWhich because of default paths, can be shortened to:\ngit push origin master:master\n\nTags work the same way:\ngit push origin refs/tags/release-1.0:refs/tags/release-1.0\n\nWhich can also be shortened to:\ngit push origin release-1.0:release-1.0\n\nBy omitting the source ref (the part before the colon), you push 'nothing' to the destination, deleting the ref on the remote end.",
    "question_score": 5053,
    "answer_score": 8219,
    "question_id": 5480258,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-tag"
    ],
    "topic": "git"
  },
  {
    "instruction": "How to determine the URL that a local Git repository was originally cloned from",
    "response": "To obtain only the remote URL:\ngit config --get remote.origin.url\n\nIf you require full output, and you are on a network that can reach the remote repo where the origin resides:\ngit remote show origin\n\nWhen using `git clone` (from GitHub, or any source repository for that matter) the default name for the source of the clone is \"origin\". Using `git remote show` will display the information about this remote name. The first few lines should show:\nC:\\Users\\jaredpar\\VsVim> git remote show origin\n* remote origin\n  Fetch URL: git@github.com:jaredpar/VsVim.git\n  Push  URL: git@github.com:jaredpar/VsVim.git\n  HEAD branch: master\n  Remote branches:\n\nIf you want to use the value in a script, you would use the first command listed in this answer.",
    "question_score": 5680,
    "answer_score": 7931,
    "question_id": 4089430,
    "has_full_answer": true,
    "tags": [
      "git",
      "github",
      "git-remote"
    ],
    "topic": "git"
  },
  {
    "instruction": "How can I reset or revert a file to a specific revision?",
    "response": "Assuming the hash of the commit you want is `c5f567`:\ngit checkout c5f567 -- file1/to/restore file2/to/restore\n\nThe git checkout man page gives more information.\nIf you want to revert to the commit before `c5f567`, append `~1` (where 1 is the number of commits you want to go back, it can be anything):\ngit checkout c5f567~1 -- file1/to/restore file2/to/restore\n\nAs a side note, I've always been uncomfortable with this command because it's used for both ordinary things (changing between branches) and unusual, destructive things (discarding changes in the working directory).\nFor the meaning of `--` in the command, refer to In Git, what does `--` (dash dash) mean?\n\nThere is also a new `git restore` command that is specifically designed for restoring working copy files that have been modified. If your git is new enough you can use this command, but the documentation comes with a warning:\n\nTHIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.\n\nBecause `git restore` is experimental, it should not yet be promoted as the primary answer to this question. When the command is no longer marked as \"experimental\", then this answer can be amended to promote the use of `git restore`. [At the time of writing, the `git restore` command has been marked as \"experimental\" for at least four years.]",
    "question_score": 5860,
    "answer_score": 7867,
    "question_id": 215718,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "git-checkout",
      "git-restore"
    ],
    "topic": "git"
  },
  {
    "instruction": "Undoing a git rebase",
    "response": "The easiest way would be to find the head commit of the branch as it was immediately before the rebase started in the reflog...\ngit reflog\n\nand to reset the current branch to it.\nSuppose the old commit was `HEAD@{2}` in the ref log:\ngit reset --soft \"HEAD@{2}\"\n\n(If you do not want to retain the working copy changes, you can use `--hard` instead of `--soft`)\nYou can check the history of the candidate old head by just doing a `git log \"HEAD@{2}\"`.\nIf you've not disabled per branch reflogs you should be able to simply do `git reflog \"branchname@{1}\"` as a rebase detaches the branch head before reattaching to the final head. I would double-check this behavior, though, as I haven't verified it recently.\nPer default, all reflogs are activated for non-bare repositories:\n[core]\n```bash\nlogAllRefUpdates = true\n```",
    "question_score": 4429,
    "answer_score": 6157,
    "question_id": 134882,
    "has_full_answer": true,
    "tags": [
      "git",
      "rebase",
      "git-rebase",
      "undo"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I delete a commit from a branch?",
    "response": "Careful: `git reset --hard` WILL DELETE YOUR WORKING DIRECTORY CHANGES.\nBe sure to stash any local changes you want to keep before running this command.\nAssuming you are sitting on that commit, then this command will wack it...\ngit reset --hard HEAD~1\n\nThe `HEAD~1` means the commit before head.\nOr, you could look at the output of `git log`, find the commit id of the commit you want to back up to, and then do this:\ngit reset --hard \n\nIf you already pushed it, you will need to do a force push to get rid of it...\ngit push origin HEAD --force\n\nHowever, if others may have pulled it, then you would be better off starting a new branch.  Because when they pull, it will just merge it into their work, and you will get it pushed back up again.\nIf you already pushed, it may be better to use `git revert`, to create a \"mirror image\" commit that will undo the changes.  However, both commits will be in the log.\n\nFYI: `git reset --hard HEAD` is great if you want to get rid of WORK IN PROGRESS.It will reset you back to the most recent commit, and erase all the changes in your working tree and index.\n`git stash` does the same except you can restore it later if you need, versus permanently delete with reset hard mode. Check your stashes by using `git stash list` and `git stash show 'stash@123'`\n\nLastly, if you need to find a commit that you \"deleted\", it is typically present in `git reflog` unless you have garbage collected your repository.",
    "question_score": 4487,
    "answer_score": 5687,
    "question_id": 1338728,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-rebase",
      "git-reset"
    ],
    "topic": "git"
  },
  {
    "instruction": "Undo a Git merge that hasn't been pushed yet",
    "response": "With `git reflog` check which commit is one prior the merge (`git reflog` will be a better option than `git log`). Then you can reset it using:\n\ngit reset --hard commit_sha\n\nThere's also another way:\n\ngit reset --hard HEAD~1\n\nIt will get you back 1 commit.\n\nBe aware that any modified and uncommitted/unstashed files will be reset to their unmodified state. To keep them either stash changes away or see `--merge` option below.  \n\nAs @Velmont suggested below in his answer, in this direct case using:\n\ngit reset --hard ORIG_HEAD\n\nmight yield better results, as it should preserve your changes. `ORIG_HEAD` will point to a commit directly before merge has occurred, so you don't have to hunt for it yourself.\n\nA further tip is to use the `--merge` switch instead of `--hard` since it doesn't reset files unnecessarily:\n\ngit reset --merge ORIG_HEAD\n\n  --merge\n  \n  Resets the index and updates the files in the working tree that are different between  and HEAD, but keeps those which are different between the index and working tree (i.e. which have changes which have not been added).",
    "question_score": 4964,
    "answer_score": 5609,
    "question_id": 2389361,
    "has_full_answer": true,
    "tags": [
      "git",
      "undo",
      "git-merge"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I update or sync a forked repository on GitHub?",
    "response": "In your local clone of your forked repository, you can add the original GitHub repository as a \"remote\".  (\"Remotes\" are like nicknames for the URLs of repositories - `origin` is one, for example.)  Then you can fetch all the branches from that upstream repository, and rebase your work to continue working on the upstream version.  In terms of commands that might look like:\n# Add the remote, call it \"upstream\":\n\ngit remote add upstream https://github.com/whoever/whatever.git\n\n# Fetch all the branches of that remote into remote-tracking branches\n\ngit fetch upstream\n\n# Make sure that you're on your main branch:\n\ngit checkout main\n\n# Rewrite your main branch so that any commits of yours that\n# aren't already in upstream/main are replayed on top of that\n# other branch:\n\ngit rebase upstream/main\n\nIf you don't want to rewrite the history of your main branch, (for example because other people may have cloned it) then you should replace the last command with `git merge upstream/main`.  However, for making further pull requests that are as clean as possible, it's probably better to rebase.\n\nIf you've rebased your branch onto `upstream/main` you may need to force the push in order to push it to your own forked repository on GitHub.  You'd do that with:\ngit push -f origin main\n\nYou only need to use the `-f` the first time after you've rebased.",
    "question_score": 4792,
    "answer_score": 5492,
    "question_id": 7244321,
    "has_full_answer": true,
    "tags": [
      "git",
      "github",
      "synchronization",
      "repository",
      "git-fork"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I add an empty directory to a Git repository?",
    "response": "Another way to make a directory stay (almost) empty (in the repository) is to create a `.gitignore` file inside that directory that contains these four lines:\n\n# Ignore everything in this directory\n*\n# Except this file\n!.gitignore\n\nThen you don't have to get the order right the way that you have to do in m104's solution.\n\nThis also gives the benefit that files in that directory won't show up as \"untracked\" when you do a git status.\n\nMaking @GreenAsJade's comment persistent:\n\n  I think it's worth noting that this solution does precisely what the question asked for, but is not perhaps what many people looking at this question will have been looking for. This solution guarantees that the directory remains empty. It says \"I truly never want files checked in here\". As opposed to \"I don't have any files to check in here, yet, but I need the directory here, files may be coming later\".",
    "question_score": 5492,
    "answer_score": 5226,
    "question_id": 115983,
    "has_full_answer": true,
    "tags": [
      "git",
      "directory",
      "git-add"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I clone all remote branches?",
    "response": "First, clone a remote Git repository and `cd` into it:\n$ git clone git://example.com/myproject\n$ cd myproject\n\nNext, look at the local branches in your repository:\n$ git branch\n* master\n\nBut there are other branches hiding in your repository! See these using the `-a` flag:\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n\nTo take a quick peek at an upstream branch, check it out directly:\n$ git checkout origin/experimental\n\nTo work on that branch, create a local tracking branch, which is done automatically by:\n$ git checkout experimental\n\nBranch experimental set up to track remote branch experimental from origin.\nSwitched to a new branch 'experimental'\n\nHere, \"new branch\" simply means that the branch is taken from the index and created locally for you.  As the previous line tells you, the branch is being set up to track the remote branch, which usually means the origin/branch_name branch.\nYour local branches should now show:\n$ git branch\n* experimental\n  master\n\nYou can track more than one remote repository using `git remote`:\n$ git remote add win32 git://example.com/users/joe/myproject-win32-port\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n  remotes/win32/master\n  remotes/win32/new-widgets\n\nAt this point, things are getting pretty crazy, so run `gitk` to see what's going on:\n$ gitk --all &",
    "question_score": 4828,
    "answer_score": 5220,
    "question_id": 67699,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-branch",
      "git-clone",
      "remote-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I get the current branch name in Git?",
    "response": "git branch\n\nshould show all the local branches of your repo. The starred branch is your current branch.\n\nTo retrieve only the name of the branch you are on:\ngit rev-parse --abbrev-ref HEAD\n\nVersion 2.22 adds the `--show-current` option to ”print the name of the current branch”. The combination also works for freshly initialized repositories before the first commit:\ngit branch --show-current",
    "question_score": 4238,
    "answer_score": 4485,
    "question_id": 6245570,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "git-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I resolve merge conflicts in a Git repository?",
    "response": "Try:\ngit mergetool\n\nIt opens a GUI that steps you through each conflict, and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  It is much better than doing the whole thing by hand certainly.\n\nAs per Josh Glover's comment:\n\n[This command]\ndoesn't necessarily open a GUI unless you install one. Running `git mergetool` for me resulted in `vimdiff` being used. You can install\none of the following tools to use it instead: `meld`, `opendiff`,\n`kdiff3`, `tkdiff`, `xxdiff`, `tortoisemerge`, `gvimdiff`, `diffuse`,\n`ecmerge`, `p4merge`, `araxis`, `vimdiff`, `emerge`.\n\nBelow is a sample procedure using `vimdiff` to resolve merge conflicts, based on this link.\n\nRun the following commands in your terminal\ngit config merge.tool vimdiff\ngit config merge.conflictstyle diff3\ngit config mergetool.prompt false\n\nThis will set `vimdiff` as the default merge tool.\n\nRun the following command in your terminal\ngit mergetool\n\nYou will see a `vimdiff` display in the following format:\n  ╔═══════╦══════╦════════╗\n  ║       ║      ║        ║\n  ║ LOCAL ║ BASE ║ REMOTE ║\n  ║       ║      ║        ║\n  ╠═══════╩══════╩════════╣\n  ║                       ║\n  ║        MERGED         ║\n  ║                       ║\n  ╚═══════════════════════╝\n\nThese 4 views are\n\nLOCAL: this is the file from the current branch\nBASE: the common ancestor, how this file looked before both changes\nREMOTE: the file you are merging into your branch\nMERGED: the merge result; this is what gets saved in the merge commit and used in the future\n\nYou can navigate among these views using ctrl+w. You can directly reach the MERGED view using ctrl+w followed by j.\nMore information about `vimdiff` navigation is here and here.\n\nYou can edit the MERGED view like this:\n\nIf you want to get changes from REMOTE\n:diffg RE\n\nIf you want to get changes from BASE\n:diffg BA\n\nIf you want to get changes from LOCAL\n:diffg LO\n\nSave, Exit, Commit, and Clean up\n`:wqa` save and exit from vi\n`git commit -m \"message\"`\n`git clean` Remove extra files (e.g. `*.orig`). Warning: It will remove all untracked files, if you won't pass any arguments.",
    "question_score": 5429,
    "answer_score": 3444,
    "question_id": 161813,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-merge",
      "merge-conflict-resolution",
      "git-merge-conflict"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I squash my last N commits together?",
    "response": "Use `git rebase -i ` and replace \"pick\" on the second and subsequent commits with \"squash\" or \"fixup\", as described in the manual.\nIn this example, `` is either the SHA1 hash or the relative location from the HEAD of the current branch from which commits are analyzed for the rebase command. For example, if the user wishes to view 5 commits from the current HEAD in the past, the command is `git rebase -i HEAD~5`.",
    "question_score": 5741,
    "answer_score": 3087,
    "question_id": 5189560,
    "has_full_answer": true,
    "tags": [
      "git",
      "rebase",
      "squash",
      "git-squash"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I remove a submodule?",
    "response": "Since git1.8.3 (April 22d, 2013):\n\nThere was no Porcelain way to say \"I no longer am interested in this submodule\", once you express your interest in a submodule with \"`git submodule init`\".\n\"`git submodule deinit`\" is the way to do so.\n\nThe deletion process also uses `git rm` (since git1.8.5 October 2013).\nSummary\nThe 3-steps removal process would then be:\n0. mv a/submodule a/submodule_tmp\n\n1. git submodule deinit -f -- a/submodule    \n2. rm -rf .git/modules/a/submodule\n3. git rm -f a/submodule\n# Note: a/submodule (no trailing slash)\n\n# or, if you want to leave it in your working tree and have done step 0\n3.   git rm --cached a/submodule\n3bis mv a/submodule_tmp a/submodule\n\nExplanation\n`rm -rf`: This is mentioned in Daniel Schroeder's answer, and summarized by Eonil in the comments:\n\nThis leaves `.git/modules//` unchanged.\nSo if you once delete a submodule with this method and re-add them again, it will not be possible because repository already been corrupted.\n\n`git rm`: See commit 95c16418:\n\nCurrently using \"`git rm`\" on a submodule removes the submodule's work tree from that of the superproject and the gitlink from the index.\nBut the submodule's section in `.gitmodules` is left untouched, which is a leftover of the now removed submodule and might irritate users (as opposed to the setting in `.git/config`, this must stay as a reminder that the user showed interest in this submodule so it will be repopulated later when an older commit is checked out).\n\nLet \"`git rm`\" help the user by not only removing the submodule from the work tree but by also removing the \"`submodule.`\" section from the `.gitmodules` file and stage both.\n\n`git submodule deinit`: It stems from this patch:\n\nWith \"`git submodule init`\" the user is able to tell git they care about one or more submodules and wants to have it populated on the next call to \"`git submodule update`\".\nBut currently there is no easy way they can tell git they do not care about a submodule anymore and wants to get rid of the local work tree (unless the user knows a lot about submodule internals and removes the \"`submodule.$name.url`\" setting from `.git/config` together with the work tree himself).\n\nHelp those users by providing a '`deinit`' command.\nThis removes the whole `submodule.` section from `.git/config` either for the given\nsubmodule(s) (or for all those which have been initialized if '`.`' is given).\nFail if the current work tree contains modifications unless forced.\nComplain when for a submodule given on the command line the url setting can't be found in `.git/config`, but nonetheless don't fail.\n\nThis takes care if the (de)initialization steps (`.git/config` and `.git/modules/xxx`)\nSince git1.8.5, the `git rm` takes also care of the:\n\n'`add`' step which records the url of a submodule in the `.gitmodules` file: it is need to removed for you.\nthe submodule special entry (as illustrated by this question): the git rm removes it from the index:\n`git rm --cached path_to_submodule` (no trailing slash)\nThat will remove that directory stored in the index with a special mode \"160000\", marking it as a submodule root directory.\n\nIf you forget that last step, and try to add what was a submodule as a regular directory, you would get error message like:\ngit add mysubmodule/file.txt \nPath 'mysubmodule/file.txt' is in submodule 'mysubmodule'\n\nNote: since Git 2.17 (Q2 2018), git submodule deinit is no longer a shell script.\nIt is a call to a C function.\nSee commit 2e61273, commit 1342476 (14 Jan 2018) by Prathamesh Chavan (`pratham-pc`).\n(Merged by Junio C Hamano -- `gitster` -- in commit ead8dbe, 13 Feb 2018)\ngit ${wt_prefix:+-C \"$wt_prefix\"} submodule--helper deinit \\\n  ${GIT_QUIET:+--quiet} \\\n  ${prefix:+--prefix \"$prefix\"} \\\n  ${force:+--force} \\\n  ${deinit_all:+--all} \"$@\"",
    "question_score": 4715,
    "answer_score": 2687,
    "question_id": 1260748,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-submodules"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I clone a specific Git branch?",
    "response": "git clone --single-branch --branch  \n\nThe `--single-branch` option is valid from version 1.7.10 and later.\nPlease see also the other answer which many people prefer.\nYou may also want to make sure you understand the difference. And the difference is: by invoking `git clone --branch  url` you're fetching all the branches and checking out one. That may, for instance, mean that your repository has a 5kB documentation or wiki branch and 5GB data branch. And whenever you want to edit your frontpage, you may end up cloning 5GB of data.\nAgain, that is not to say `git clone --branch` is not the way to accomplish that, it's just that it's not always what you want to accomplish, when you're asking about cloning a specific branch.",
    "question_score": 4182,
    "answer_score": 2557,
    "question_id": 1911109,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "git-clone"
    ],
    "topic": "git"
  },
  {
    "instruction": "Remove a file from a Git repository without deleting it from the local filesystem",
    "response": "The `git rm` documentation states:\n\nWhen `--cached` is given, the staged content has to match either the tip of the branch or the file on disk, allowing the file to be removed from just the index.\n\nSo, for a single file:\ngit rm --cached file_to_remove.txt\n\nand for a single directory:\ngit rm --cached -r directory_to_remove",
    "question_score": 4018,
    "answer_score": 5604,
    "question_id": 1143796,
    "has_full_answer": true,
    "tags": [
      "git",
      "repository",
      "remote-server",
      "delete-file",
      "git-rm"
    ],
    "topic": "git"
  },
  {
    "instruction": "How can I change the commit author for a single commit?",
    "response": "Interactive rebase off of a point earlier in the history than the commit you need to modify (`git rebase -i `). In the list of commits being rebased, change the text from `pick` to `edit` next to the hash of the one you want to modify. Then when git prompts you to change the commit, use this:\n\ngit commit --amend --author=\"Author Name \" --no-edit\n\nFor example, if your commit history is `A-B-C-D-E-F` with `F` as `HEAD`, and you want to change the author of `C` and `D`, then you would...\n\nSpecify `git rebase -i B` (here is an example of what you will see after executing the `git rebase -i B` command)\n\nif you need to edit `A`, use `git rebase -i --root`\n\nChange the lines for both `C` and `D` from `pick` to `edit`\nExit the editor (for vim, this would be pressing Esc and then typing `:wq`).\nOnce the rebase started, it would first pause at `C`\nYou would `git commit --amend --author=\"Author Name \"`\nThen `git rebase --continue`\nIt would pause again at `D`\nThen you would `git commit --amend --author=\"Author Name \"` again\n`git rebase --continue`\nThe rebase would complete.\nUse `git push -f` to update your origin with the updated commits.",
    "question_score": 3266,
    "answer_score": 5252,
    "question_id": 3042437,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-commit"
    ],
    "topic": "git"
  },
  {
    "instruction": "Git is not working after macOS update (\"xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools\")",
    "response": "The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n\nDid not run into this on Sonoma.\n\nMaybe Apple fixed the process?\n\nUpdated for Ventura\n\nAfter opening the terminal after restarting, I tried to go to my code, and do a `git status`, and I got an error and prompt for command line software agreement:\nSo press space until you get to the `[agree, print, cancel]` option, so careful hit space to scroll down to the end, if you blow past It you have to run a command to get it back. Use `sudo xcodebuild -license` to get to it again.\nJust be careful on scrolling down and enter `agree` and press return and it will launch into an update.\n\nThen I tried to use git after the install, and it prompted me to install Xcode tools again.\nI followed my own advice from previous years (see below), and went to https://developer.apple.com/download/all and downloaded\n\"Command Line Tools for Xcode 14\" (You have to log in with your Apple ID and enter MFA code, so have all the devices you need for that handy. Then select \"Command Line Tools for Xcode 14\", or if you want to get into the alphas or betas, that's up to you. But stable releases are probably the best choice for software developers.\n\nYou have to either download the tools from CLI or the developer page and before you can use git, you need to reboot!!! Or you will get stuck in a loop of prompt & downloading\nRebooting will break the loop and complete the installation of your CLI tools including git so that you can get back to work\nSolutions for previous years, these may or may not be valid these days as the downloads page has changed significantly:\nPREVIOUS YEARS SOLUTIONS, probably #2 is most helpful.\n*** Solution #1:\nGo back to your terminal and enter:\nxcode-select --install\n\nYou'll then receive the following output:\nxcode-select: note: install requested for command line developer tools\n\nYou will then be prompted in a window to update Xcode Command Line tools. (which could take a while)\nOpen a new terminal window and your development tools should be returned.\nAddition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...\nAfter that restart your terminal\nAlternatively, IF that fails, and it might.... you'll get a pop-up box saying \"Software not found on server\", proceed to solution 2.\n*** Solution #2: (Preferred method)\nIf you hit `xcode-select --install` and it doesn't find the software, log into Apple Developer, and install it via webpage.\nLog in or sign up here:\nhttps://developer.apple.com/download/more/\nLook for: \"Command Line Tools for Xcode 14.x\" in the list of downloads\nThen click the dmg and download. (See previous image above) either way, you will probably wind up at an apple downloads webpage.",
    "question_score": 3509,
    "answer_score": 5071,
    "question_id": 52522565,
    "has_full_answer": true,
    "tags": [
      "xcode",
      "git",
      "macos",
      "command-line",
      "terminal"
    ],
    "topic": "git"
  },
  {
    "instruction": "Commit only part of a file's changes in Git",
    "response": "You can use:\ngit add --patch \n\nor for short:\ngit add -p \n\nGit will break down your file into what it thinks are sensible \"hunks\" (portions of the file). It will then prompt you with this question:\nStage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]?\n\nHere is a description of each option:\n\ny stage this hunk for the next commit\nn do not stage this hunk for the next commit\nq quit; do not stage this hunk or any of the remaining hunks\na stage this hunk and all later hunks in the file\nd do not stage this hunk or any of the later hunks in the file\ng select a hunk to go to\n/ search for a hunk matching the given regex\nj leave this hunk undecided, see next undecided hunk\nJ leave this hunk undecided, see next hunk\nk leave this hunk undecided, see previous undecided hunk\nK leave this hunk undecided, see previous hunk\ns split the current hunk into smaller hunks\ne manually edit the current hunk\n\nYou can then edit the hunk manually by replacing `+`/`-` by `#` (thanks veksen)\n\n? print hunk help\n\nIf the file is not in the repository yet, you can first do `git add -N `. Afterwards you can go on with `git add -p `.\nAfterwards, you can use:\n\n`git diff --staged` to check that you staged the correct changes\n`git reset -p` to unstage mistakenly added hunks\n`git commit -v` to view your commit while you edit the commit message.\n\nNote this is far different than the `git format-patch` command, whose purpose is to parse commit data into a `.patch` files.\nReference for future: Git Tools - Interactive Staging",
    "question_score": 3683,
    "answer_score": 5056,
    "question_id": 1085162,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-commit"
    ],
    "topic": "git"
  },
  {
    "instruction": "Make an existing Git branch track a remote branch?",
    "response": "Given a branch `foo` and a remote `upstream`:\nAs of Git 1.8.0:\ngit branch -u upstream/foo\n\nOr, if local branch `foo` is not the current branch:\ngit branch -u upstream/foo foo\n\nOr, if you like to type longer commands, these are equivalent to the above two:\ngit branch --set-upstream-to=upstream/foo\n\ngit branch --set-upstream-to=upstream/foo foo\n\nAs of Git 1.7.0 (before 1.8.0):\ngit branch --set-upstream foo upstream/foo\n\nNotes:\n\nAll of the above commands will cause local branch `foo` to track remote branch `foo` from remote `upstream`.\nThe old (1.7.x) syntax is deprecated in favor of the new (1.8+) syntax.  The new syntax is intended to be more intuitive and easier to remember.\nDefining an upstream branch will fail when run against newly-created remotes that have not already been fetched. In that case, run `git fetch upstream` beforehand.\n\nSee also: Why do I need to do `--set-upstream` all the time?",
    "question_score": 4109,
    "answer_score": 4944,
    "question_id": 520650,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "git-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I list all the files in a commit?",
    "response": "Preferred Way (because it's a plumbing command; meant to be programmatic):\n$ git diff-tree --no-commit-id --name-only bd61ad98 -r\nindex.html\njavascript/application.js\njavascript/ie6.js\n\nAnother Way (less preferred for scripts, because it's a porcelain command; meant to be user-facing)\n$ git show --pretty=\"\" --name-only bd61ad98    \nindex.html\njavascript/application.js\njavascript/ie6.js\n\nThe `--no-commit-id` suppresses the commit ID output.\nThe `--pretty` argument specifies an empty format string to avoid the cruft at the beginning.\nThe `--name-only` argument shows only the file names that were affected (Thanks Hank). Use `--name-status` instead, if you want to see what happened to each file (Deleted, Modified, Added)\nThe `-r` argument is to recurse into sub-trees",
    "question_score": 3670,
    "answer_score": 4926,
    "question_id": 424071,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-show"
    ],
    "topic": "git"
  },
  {
    "instruction": "Difference between \"git add -A\" and \"git add .\"",
    "response": "This answer only applies to Git version 1.x. For Git version 2.x, see other answers.\n\nSummary:\n\n`git add -A` stages all changes\n\n`git add .` stages new files and modifications, without deletions (on the current directory and its subdirectories).\n\n`git add -u` stages modifications and deletions, without new files\n\nDetail:\n`git add -A` is equivalent to  `git add .; git add -u`.\nThe important point about `git add .` is that it looks at the working tree and adds all those paths to the staged changes if they are either changed or are new and not ignored, it does not stage any 'rm' actions.\n`git add -u` looks at all the already tracked files and stages the changes to those files if they are different or if they have been removed. It does not add any new files, it only stages changes to already tracked files.\n`git add -A` is a handy shortcut for doing both of those.\nYou can test the differences out with something like this (note that for Git version 2.x your output for `git add .` `git status` will be different):\ngit init\necho Change me > change-me\necho Delete me > delete-me\ngit add change-me delete-me\ngit commit -m initial\n\necho OK >> change-me\nrm delete-me\necho Add me > add-me\n\ngit status\n# Changed but not updated:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit add .\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n# Changed but not updated:\n#   deleted:    delete-me\n\ngit reset\n\ngit add -u\ngit status\n\n# Changes to be committed:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit reset\n\ngit add -A\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n#   deleted:    delete-me",
    "question_score": 3491,
    "answer_score": 4914,
    "question_id": 572549,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-add"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I make git use the editor of my choice for editing commit messages?",
    "response": "Setting the default editor for Git\nPick one:\n\nSet `core.editor` in your Git config:\ngit config --global core.editor \"vim\"\n\nSet the `GIT_EDITOR` environment variable:\nexport GIT_EDITOR=vim\n\nSetting the default editor for all programs\nSet the standardized `VISUAL` and `EDITOR` environment variables*:\nexport VISUAL=vim\nexport EDITOR=\"$VISUAL\"\n\nNOTE: Setting both is not necessarily needed, but some programs may not use the more-correct `VISUAL`. See `VISUAL` vs. `EDITOR`.\n\nFixing compatibility issues\nSome editors require a `--wait` flag, or they will open a blank page. For example:\n\nSublime Text (if correctly set up; or use the full path to the executable in place of `subl`):\nexport VISUAL=\"subl --wait\"\n\nVS Code (after adding the shell command):\nexport VISUAL=\"code --wait\"",
    "question_score": 3477,
    "answer_score": 4714,
    "question_id": 2596805,
    "has_full_answer": true,
    "tags": [
      "git",
      "vim",
      "emacs",
      "editor",
      "commit-message"
    ],
    "topic": "git"
  },
  {
    "instruction": "Move existing, uncommitted work to a new branch in Git",
    "response": "Update 2020 / Git 2.23\nGit 2.23 adds the new `switch` subcommand in an attempt to clear some of the confusion that comes from the overloaded usage of `checkout` (switching branches, restoring files, detaching HEAD, etc.)\nStarting with this version of Git, replace the checkout command with:\ngit switch -c \n\nThe behavior is identical and remains unchanged.\n\nBefore Update 2020 / Git 2.23\nUse the following:\ngit checkout -b \n\nThis will leave your current branch as it is, create and checkout a new branch and keep all your changes. You can then stage changes in files to commit with:\ngit add \n\nand commit to your new branch with:\ngit commit -m \"\"\n\nThe changes in the working directory and changes staged in index do not belong to any branch yet. This changes the branch where those modifications would end in.\nYou don't reset your original branch, it stays as it is. The last commit on `` will still be the same. Therefore you `checkout -b` and then commit.",
    "question_score": 4008,
    "answer_score": 4701,
    "question_id": 1394797,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-branch",
      "git-stash",
      "git-reset"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I modify a specific commit?",
    "response": "Use `git rebase`. For example, to modify commit `bbc643cd`, run:\ngit rebase --interactive bbc643cd~\n\nPlease note the tilde `~` at the end of the command, because you need to reapply commits on top of the previous commit of `bbc643cd` (i.e. `bbc643cd~`).\nIn the default editor, modify `pick` to `edit` in the line mentioning `bbc643cd`.\nSave the file and exit. git will interpret and automatically execute the commands in the file. You will find yourself in the previous situation in which you just had created commit `bbc643cd`.\nAt this point, `bbc643cd` is your last commit and you can easily amend it. Make your changes and then commit them with the command:\ngit commit --all --amend --no-edit\n\nAfter that, return back to the previous HEAD commit using:\ngit rebase --continue\n\nWARNING: Note that this will change the SHA-1 of that commit as well as all children -- in other words, this rewrites the history from that point forward. You can break repos doing this if you push using the command `git push --force`.",
    "question_score": 3335,
    "answer_score": 4587,
    "question_id": 1186535,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-rewrite-history"
    ],
    "topic": "git"
  },
  {
    "instruction": "What does cherry-picking a commit with Git mean?",
    "response": "Cherry-picking in Git means choosing a commit from one branch and applying it to another.\nThis contrasts with other ways such as `merge` and `rebase` which normally apply many commits to another branch.\nIt's also possible to cherry-pick multiple commits but `merge` is the preferred way over cherry-picking.\n\nMake sure you are on the branch you want to apply the commit to.\ngit switch master\n\nExecute the following:\ngit cherry-pick \n\nN.B.:\n\nIf you cherry-pick from a public branch, you should consider using\ngit cherry-pick -x \n\nThis will generate a standardized commit message. This way, you (and your co-workers) can still keep track of the origin of the commit and may avoid merge conflicts in the future.\n\nIf you have notes attached to the commit they do not follow the cherry-pick. To bring them over as well, You have to use:\ngit notes copy  \n\nAdditional links:\n\ngit official guide page\ngit cherry-pick guide",
    "question_score": 3464,
    "answer_score": 4175,
    "question_id": 9339429,
    "has_full_answer": true,
    "tags": [
      "git",
      "cherry-pick",
      "git-cherry-pick"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I stash only one file out of multiple files that have changed?",
    "response": "git stash push -p -m \"my commit message\"\n\n`-p` let's you select the hunks that should be stashed; whole files can be selected as well.\nYou'll be prompted with a few actions for each hunk:\n   y - stash this hunk\n   n - do not stash this hunk\n   q - quit; do not stash this hunk or any of the remaining ones\n   a - stash this hunk and all later hunks in the file\n   d - do not stash this hunk or any of the later hunks in the file\n   g - select a hunk to go to\n   / - search for a hunk matching the given regex\n   j - leave this hunk undecided, see next undecided hunk\n   J - leave this hunk undecided, see next hunk\n   k - leave this hunk undecided, see previous undecided hunk\n   K - leave this hunk undecided, see previous hunk\n   s - split the current hunk into smaller hunks\n   e - manually edit the current hunk\n   ? - print help",
    "question_score": 3850,
    "answer_score": 3651,
    "question_id": 3040833,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-stash"
    ],
    "topic": "git"
  },
  {
    "instruction": "View the change history of a file using Git versioning",
    "response": "For a graphical view, use `gitk`:\ngitk [filename]\n\nTo follow the file across file renames:\ngitk --follow [filename]",
    "question_score": 3977,
    "answer_score": 2726,
    "question_id": 278192,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-log"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I create a remote Git branch?",
    "response": "Simple Git 2.0+ solution:\nAs of Git 2.0, the behavior has become simpler:\nYou can configure git with `push.default = current` to make life easier:\nI added this so now I can just push a new branch upstream with\n$ git push -u\n\n`-u` will track remote branch of the same name. Now with this configuration, you will auto-guess the remote reference to git push. From git.config documentation:\n\npush.default\nDefines the action git push should take if no refspec is explicitly given.\n`push.default = current` - push the current branch to update a branch with the\nsame name on the receiving end. Works in both central and non-central workflows.\n\nFor me, this is a good simplification of my day-to-day Git workflow. The configuration setting takes care of the 'usual' use case where you add a branch locally and want to create it remotely. Also, I can just as easily create local branches from remotes by just doing `git co remote_branch_name` (as opposed to using `--set-upstream-to` flag).\nI know this question and the accepted answers are rather old, but the behavior has changed so that now configuration options exist to make your workflow simpler.\nTo add to your global Git configuration, run this on the command line:\n$ git config --global push.default current",
    "question_score": 3738,
    "answer_score": 428,
    "question_id": 1519006,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "git-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do you push a tag to a remote repository using Git?",
    "response": "To push specific, one tag do following\n`git push origin tag_name`",
    "question_score": 3635,
    "answer_score": 368,
    "question_id": 5195859,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-push",
      "git-tag"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I find and restore a deleted file in a Git repository?",
    "response": "Find the last commit that affected the given path. As the file isn't in the HEAD commit, that previous commit must have deleted it.\n\ngit rev-list -n 1 HEAD -- \n\nThen checkout the version at the commit before, using the caret (`^`) symbol:\ngit checkout ^ -- \n\nOr in one command, if `$file` is the file in question.\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")^ -- \"$file\"\n\nIf you are using zsh and have the EXTENDED_GLOB option enabled, the caret symbol won't work. You can use `~1` instead.\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")~1 -- \"$file\"",
    "question_score": 3242,
    "answer_score": 3554,
    "question_id": 953481,
    "has_full_answer": true,
    "tags": [
      "git",
      "file-io",
      "git-checkout"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I make Git ignore file mode (chmod) changes?",
    "response": "Try:\ngit config core.fileMode false\n\nFrom git-config(1):\n\ncore.fileMode\n```bash\nTells Git if the executable bit of files in the working tree\nis to be honored.\n```\n\n```bash\nSome filesystems lose the executable bit when a file that is\nmarked as executable is checked out, or checks out a\nnon-executable file with executable bit on. git-clone(1)\nor git-init(1) probe the filesystem to see if it handles the \nexecutable bit correctly and this variable is automatically\nset as necessary.\n```\n\n```bash\nA repository, however, may be on a filesystem that handles\nthe filemode correctly, and this variable is set to true when\ncreated, but later may be made accessible from another\nenvironment that loses the filemode (e.g. exporting ext4\nvia CIFS mount, visiting a Cygwin created repository with Git\nfor Windows or Eclipse). In such a case it may be necessary\nto set this variable to false. See git-update-index(1).\n```\n\n```bash\nThe default is true (when core.filemode is not specified\nin the config file).\n```\n\nThe `-c` flag can be used to set this option for one-off commands:\ngit -c core.fileMode=false diff\n\nTyping the `-c core.fileMode=false` can be bothersome and so you can set this flag for all git repos or just for one git repo:\n# this will set your the flag for your user for all git repos (modifies `$HOME/.gitconfig`)\n# WARNING: this will be override by local config, fileMode value is automatically selected with latest version of git.\n# This mean that if git detect your current filesystem is compatible it will set local core.fileMode to true when you clone or init a repository.\n# Tool like cygwin emulation will be detected as compatible and so your local setting WILL BE SET to true no matter what you set in global setting.\ngit config --global core.fileMode false\n\n# this will set the flag for one git repo (modifies `$current_git_repo/.git/config`)\ngit config core.fileMode false\n\nAdditionally, `git clone` and `git init` explicitly set `core.fileMode` to `true` in the repo config as discussed in Git global core.fileMode false overridden locally on clone\nWarning\n`core.fileMode` is not the best practice and should be used carefully. This setting only covers the executable bit of mode and never the read/write bits. In many cases you think you need this setting because you did something like `chmod -R 777`, making all your files executable. But in most projects most files don't need and should not be executable for security reasons.\nThe proper way to solve this kind of situation is to handle folder and file permission separately, with something like:\nfind . -type d -exec chmod a+rwx {} \\; # Make folders traversable and read/write\nfind . -type f -exec chmod a+rw {} \\;  # Make files read/write\n\nIf you do that, you'll never need to use `core.fileMode`, except in very rare environment.",
    "question_score": 2944,
    "answer_score": 4883,
    "question_id": 1580596,
    "has_full_answer": true,
    "tags": [
      "git",
      "ignore",
      "chmod"
    ],
    "topic": "git"
  },
  {
    "instruction": "Ignore files that have already been committed to a Git repository",
    "response": "To untrack a single file that has already been added/initialized to your repository, i.e., stop tracking the file but not delete it from your system use: `git rm --cached filename`\nTo untrack every file that is now in your `.gitignore`:\nFirst commit any outstanding code changes, and then, run this command:\ngit rm -r --cached .\n\nThis removes any changed files from the index(staging area), then just run:\ngit add .\n\nCommit it:\ngit commit -m \".gitignore is now working\"\n\nTo undo `git rm --cached filename`, use `git add filename`.\n\nMake sure to commit all your important changes before running `git add .`\nOtherwise, you will lose any changes to other files.\n\nPlease be careful, when you push this to a repository and pull from somewhere else into a state where those files are still tracked, the files will be DELETED\n\nTo remove all files that do not match the paths listed in some path spec file (something that has the same format as a .gitignore file):\ngit rm --cached --ignore-unmatch -r --pathspec-from-file .ignorelist \n\nNote: unlike `.gitignore`, which can have new lines, `.ignorelist` cannot have new lines because `git` will complain about empty patterns not being valid path spec items.\nExplanation:\n\n`-r`: to allow recursive removal when directories are involved\n`--pathspec-from-file`: path to file containing the path specifications for files to be removed\n`--ignore-unmatch`: tell `git` to ignore any path specifications that do not have a matching cache hit (otherwise it will abort with an error on that path spec)",
    "question_score": 2877,
    "answer_score": 4797,
    "question_id": 1139762,
    "has_full_answer": true,
    "tags": [
      "git",
      "caching",
      "version-control",
      "gitignore",
      "git-rm"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I revert all local changes in Git managed project to previous state?",
    "response": "To revert changes made to your working copy, do this:\ngit checkout .\n\nOr equivalently, for git version >= 2.23:\ngit restore .\n\nTo revert changes made to the index (i.e., that you have added), do this. Warning this will reset all of your unpushed commits to master!:\ngit reset\n\nTo revert a change that you have committed:\ngit revert  \n\nTo remove untracked files (e.g., new files, generated files):\ngit clean -f\n\nOr untracked directories (e.g., new or automatically generated directories):\ngit clean -fd",
    "question_score": 2731,
    "answer_score": 4699,
    "question_id": 1146973,
    "has_full_answer": true,
    "tags": [
      "git",
      "revert",
      "git-checkout"
    ],
    "topic": "git"
  },
  {
    "instruction": "How can I save username and password in Git?",
    "response": "Attention:\nThis method saves the credentials in plaintext on your PC's disk. Everyone on your computer can access it, e.g. malicious NPM modules.\nRun:\ngit config --global credential.helper store\n\nthen:\ngit pull\n\nprovide a username and password and those details will then be remembered later. The credentials are stored in a file on the disk, with the disk permissions of \"just user readable/writable\" but still in plaintext.\nIf you want to change the password later:\ngit pull\n\nWill fail, because the password is incorrect, git then removes the offending user+password from the `~/.git-credentials` file, so now re-run:\ngit pull\n\nto provide a new password so it works as earlier.",
    "question_score": 2798,
    "answer_score": 4510,
    "question_id": 35942754,
    "has_full_answer": true,
    "tags": [
      "git",
      "credentials",
      "git-config",
      "git-extensions"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I clone a Git repository into a specific folder?",
    "response": "Option A:\ngit clone git@github.com:whatever folder-name\n\nErgo, for `right here` use:\ngit clone git@github.com:whatever .\n\nOption B:\nMove the `.git` folder, too. Note that the `.git` folder is hidden in most graphical file explorers, so be sure to show hidden files.\nmv /where/it/is/right/now/* /where/I/want/it/\nmv /where/it/is/right/now/.* /where/I/want/it/\n\nThe first line grabs all normal files, the second line grabs dot-files. It is also possibe to do it in one line by enabling dotglob (i.e. `shopt -s dotglob`) but that is probably a bad solution if you are asking the question this answer answers.\nBetter yet:\nKeep your working copy somewhere else, and create a symbolic link. Like this:\nln -s /where/it/is/right/now /the/path/I/want/to/use\n\nFor your case this would be something like:\nln -sfn /opt/projectA/prod/public /httpdocs/public\n\nWhich easily could be changed to test if you wanted it, i.e.:\nln -sfn /opt/projectA/test/public /httpdocs/public\n\nwithout moving files around. Added `-fn` in case someone is copying these lines (`-f` is force,  `-n` avoid some often unwanted interactions with already and non-existing links).\nIf you just want it to work, use Option A, if someone else is going to look at what you have done, use Option C.",
    "question_score": 3165,
    "answer_score": 4455,
    "question_id": 651038,
    "has_full_answer": true,
    "tags": [
      "git",
      "repository",
      "git-clone"
    ],
    "topic": "git"
  },
  {
    "instruction": "`git fetch` a remote branch",
    "response": "Update: Using Git Switch\nAll of the information written below was accurate, but a new command, `git switch` has been added that simplifies the effort.\nIf `daves_branch` exists on the remote repository, but not on your local branch, you can simply type:\ngit switch daves_branch\n\nSince you do not have the branch locally, this will automatically make `switch` look on the remote repo.  It will then also automatically set up remote branch tracking.\nNote that if `daves_branch` doesn't exist locally you'll need to `git fetch` first before using `switch`.\n\nOriginal Post\nYou need to create a local branch that tracks a remote branch. The following command will create a local branch named daves_branch, tracking the remote branch origin/daves_branch. When you push your changes the remote branch will be updated.\nFor most recent versions of Git:\ngit checkout --track origin/daves_branch\n\n`--track` is shorthand for `git checkout -b [branch] [remotename]/[branch]` where [remotename] is origin in this case and [branch] is twice the same, daves_branch in this case.\nFor Git 1.5.6.5 you needed this:\ngit checkout --track -b daves_branch origin/daves_branch\n\nFor Git 1.7.2.3 and higher, this is enough (it might have started earlier, but this is the earliest confirmation I could find quickly):\ngit checkout daves_branch\n\nNote that with recent Git versions, this command will not create a local branch and will put you in a 'detached HEAD' state. If you want a local branch, use the `--track` option.\nFull details are here: 3.5 Git Branching - Remote Branches, Tracking Branches",
    "question_score": 3219,
    "answer_score": 4335,
    "question_id": 9537392,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "git-branch",
      "git-fetch"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I get the hash for the current commit in Git?",
    "response": "To turn any extended object reference into a hash, use `git-rev-parse`:\ngit rev-parse HEAD\n\nor\ngit rev-parse --verify HEAD\n\nAs noted by Alexander's answer, you can also retrieve the short hash:\ngit rev-parse --short HEAD\n\nTo turn references (e.g. branches and tags) into hashes, use `git show-ref` and `git for-each-ref`.",
    "question_score": 2826,
    "answer_score": 4110,
    "question_id": 949314,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-hash"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I delete all Git branches which have been merged?",
    "response": "NOTE: You can add other branches to exclude like master and dev if your workflow has those as a possible ancestor. Usually I branch off of a \"sprint-start\" tag and `master`, `dev` and `qa` are not ancestors.\n\nFirst, list locally-tracking branches that were merged in remote (consider using `-r` flag to list all remote-tracking branches).\ngit branch --merged\n\nYou might see a few branches you don't want to remove. We can add arguments to skip important branches that we don't want to delete like master or a develop. The following command will skip the `master`/`main` branch and anything that has 'dev' in it.\ngit branch --merged | grep -Ev \"(^\\*|^\\+|master|main|dev)\"\n\nThe first part (`^\\*|^+`) excludes the current branch and any branch checked out in another worktree.\nIf you want to skip a branch, you can add it to the grep command as below. The branch `skip_branch_name` will not be deleted.\ngit branch --merged | grep -Ev \"(^\\*|^\\+|master|main|dev|skip_branch_name)\"\n\nTo delete all local branches that are already merged into the currently checked out branch:\ngit branch --merged | grep -Ev \"(^\\*|^\\+|master|main|dev)\" | xargs --no-run-if-empty git branch -d\n\nYou can see that `master` and `dev` are excluded in case they are an ancestor.\n\nYou can delete a merged local branch with:\ngit branch -d branchname\n\nTo force deletion of an unmerged branch, use:\ngit branch -D branchname\n\nTo delete it from the remote use:\ngit push --delete origin branchname\n\ngit push origin :branchname    # for really old git\n\nOnce you delete the branch from the remote, you can prune to get rid of remote tracking branches with:\ngit remote prune origin\n\nor prune individual remote tracking branches, as the other answer suggests, with:\ngit branch -dr branchname",
    "question_score": 2751,
    "answer_score": 4093,
    "question_id": 6127328,
    "has_full_answer": true,
    "tags": [
      "git",
      "version-control",
      "branch",
      "feature-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "See what's in a stash without applying it",
    "response": "From `man git-stash` (which can also be obtained via `git help stash`):\n\nThe modifications stashed away by this command can be listed with `git stash list`, inspected with `git stash show`, and ...\n\nshow []\n```bash\nShow the changes recorded in the stash as a diff between the stashed\nstate and its original parent. When no <stash> is given, shows the\nlatest one. By default, the command shows the diffstat, but it will\naccept any format known to git diff (e.g., git stash show -p stash@{1}\nto view the second most recent stash in patch form).\n```\n\nNote: the `-p` option generates a patch, as per `git-diff` documentation.\nList the stashes:\ngit stash list\n\nShow the files in the most recent stash:\ngit stash show\n\nShow the changes of the most recent stash:\ngit stash show -p\n\nShow the changes of the named stash:\ngit stash show -p stash@{1}\n\nOr in short:\ngit stash show -p 1 \n\nIf you want to view changes of only the last stash:\ngit stash show -p 0",
    "question_score": 2847,
    "answer_score": 3916,
    "question_id": 10725729,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-stash"
    ],
    "topic": "git"
  },
  {
    "instruction": "How do I \"git clone\" a repo, including its submodules?",
    "response": "With version 2.13 of Git and later, `--recurse-submodules` can be used instead of `--recursive`:\n\ngit clone --recurse-submodules -j8 git://github.com/foo/bar.git\ncd bar\n\nEditor’s note: `-j8` is an optional performance optimization that became available in version 2.8, and fetches up to 8 submodules at a time in parallel — see `man git-clone`.\n\nWith version 1.9 of Git up until version 2.12 (`-j` flag only available in version 2.8+):\n\ngit clone --recursive -j8 git://github.com/foo/bar.git\ncd bar\n\nWith version 1.6.5 of Git and later, you can use:\n\ngit clone --recursive git://github.com/foo/bar.git\ncd bar\n\nFor already cloned repos, or older Git versions, use:\n\ngit clone git://github.com/foo/bar.git\ncd bar\ngit submodule update --init --recursive",
    "question_score": 2794,
    "answer_score": 3902,
    "question_id": 3796927,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-submodules"
    ],
    "topic": "git"
  },
  {
    "instruction": "How to branch from a previous commit",
    "response": "Create the branch using a commit hash:\ngit branch branch_name \n\nOr by using a symbolic reference:\ngit branch branch_name HEAD~3\n\nTo checkout the branch while creating it, use:\ngit checkout -b branch_name",
    "question_score": 2716,
    "answer_score": 3872,
    "question_id": 2816715,
    "has_full_answer": true,
    "tags": [
      "git",
      "branch",
      "commit",
      "git-branch"
    ],
    "topic": "git"
  },
  {
    "instruction": "Pull latest changes for all git submodules",
    "response": "If it's the first time you check-out a repo you need to use `--init` first:\n\ngit submodule update --init --recursive\n\nFor git 1.8.2 or above, the option `--remote` was added to support updating to latest tips of remote branches:\n\ngit submodule update --recursive --remote\n\nThis has the added benefit of respecting any \"non default\" branches specified in the `.gitmodules` or `.git/config` files (if you happen to have any, default is origin/master, in which case some of the other answers here would work as well).\n\nFor git 1.7.3 or above you can use (but the below gotchas around what update does still apply):\n\ngit submodule update --recursive\n\nor:\n\ngit pull --recurse-submodules\n\nif you want to pull your submodules to latest commits instead of the current commit the repo points to.\n\nSee git-submodule(1) for details",
    "question_score": 2762,
    "answer_score": 3831,
    "question_id": 1030169,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-submodules"
    ],
    "topic": "git"
  },
  {
    "instruction": "How to list only the names of files that changed between two commits",
    "response": "git diff --name-only SHA1 SHA2\n\nwhere you only need to include enough of the SHA hash to identify the commits. The order of the SHAs does not matter. The output (which includes the relative path, not just the file name) follows this format:\n dir 1/dir 2/filename.ext\n dir 3/dir 4/other filename.ext\n\nYou can also do, for example\ngit diff --name-only HEAD~10 HEAD~5\n\nto see the differences between the tenth latest commit and the fifth latest (or so).",
    "question_score": 2791,
    "answer_score": 3700,
    "question_id": 1552340,
    "has_full_answer": true,
    "tags": [
      "git",
      "git-diff"
    ],
    "topic": "git"
  },
  {
    "instruction": "Make .gitignore ignore everything except a few files",
    "response": "An optional prefix `!` which negates the pattern; any matching file excluded by\na previous pattern will become included again. If a negated pattern matches,\nthis will override lower precedence patterns sources.\n\n# Ignore everything\n*\n\n# But not these files...\n!.gitignore\n!script.pl\n!template.latex\n# etc...\n\n# ...even if they are in subdirectories\n!*/",
    "question_score": 2741,
    "answer_score": 3648,
    "question_id": 987142,
    "has_full_answer": true,
    "tags": [
      "git",
      "gitignore"
    ],
    "topic": "git"
  },
  {
    "instruction": "How to correctly add a path to PATH?",
    "response": "The simple stuff\nPATH=$PATH:~/opt/bin\n\nor\nPATH=~/opt/bin:$PATH\n\ndepending on whether you want to add `~/opt/bin` at the end (to be searched after all other directories, in case there is a program by the same name in multiple directories) or at the beginning (to be searched before all other directories).\nYou can add multiple entries at the same time. `PATH=$PATH:~/opt/bin:~/opt/node/bin` or variations on the ordering work just fine. Don't put `export` at the beginning of the line as it has additional complications (see below under “Notes on shells other than bash”).\nIf your `PATH` gets built by many different components, you might end up with duplicate entries. See How to add home directory path to be discovered by Unix which command? and Remove duplicate $PATH entries with awk command to avoid adding duplicates or remove them.\nSome distributions automatically put `~/bin` in your PATH if it exists, by the way.\nWhere to put it\nPut the line to modify `PATH` in `~/.profile`, or in `~/.bash_profile` or if that's what you have. (If your login shell is zsh and not bash, put it in `~/.zprofile` instead.)\nThe profile file is read by login shells, so it will only take effect the next time you log in. (Some systems configure terminals to read a login shell; in that case you can start a new terminal window, but the setting will take effect only for programs started via a terminal, and how to set `PATH` for all programs depends on the system.)\nNote that `~/.bash_rc` is not read by any program, and `~/.bashrc` is the configuration file of interactive instances of bash. You should not define environment variables in `~/.bashrc`. The right place to define environment variables such as `PATH` is `~/.profile` (or `~/.bash_profile` if you don't care about shells other than bash). See What's the difference between them and which one should I use?\nDon't put it in `/etc/environment` or `~/.pam_environment`: these are not shell files, you can't use substitutions like `$PATH` in there. In these files, you can only override a variable, not add to it.\nPotential complications in some system scripts\nYou don't need `export` if the variable is already in the environment: any change of the value of the variable is reflected in the environment.¹ `PATH` is pretty much always in the environment; all unix systems set it very early on (usually in the very first process, in fact).\nAt login time, you can rely on `PATH` being already in the environment, and already containing some system directories. If you're writing a script that may be executed early while setting up some kind of virtual environment, you may need to ensure that `PATH` is non-empty and exported: if `PATH` is still unset, then something like `PATH=$PATH:/some/directory` would set `PATH` to `:/some/directory`, and the empty component at the beginning means the current directory (like `.:/some/directory`).\nif [ -z \"${PATH-}\" ]; then export PATH=/usr/local/bin:/usr/bin:/bin; fi\n\nNotes on shells other than bash\nIn bash, ksh and zsh, `export` is special syntax, and both `PATH=~/opt/bin:$PATH` and `export PATH=~/opt/bin:$PATH` do the right thing even. In other Bourne/POSIX-style shells such as dash (which is `/bin/sh` on many systems), `export` is parsed as an ordinary command, which implies two differences:\n\n`~` is only parsed at the beginning of a word, except in assignments (see How to add home directory path to be discovered by Unix which command? for details);\n`$PATH` outside double quotes breaks if `PATH` contains whitespace or `\\[*?`.\n\nSo in shells like dash, `export PATH=~/opt/bin:$PATH` sets `PATH` to the literal string `~/opt/bin/:` followed by the value of `PATH` up to the first space.\n`PATH=~/opt/bin:$PATH` (a bare assignment) doesn't require quotes and does the right thing. If you want to use `export` in a portable script, you need to write `export PATH=\"$HOME/opt/bin:$PATH\"`, or `PATH=~/opt/bin:$PATH; export PATH` (or `PATH=$HOME/opt/bin:$PATH; export PATH` for portability to even the Bourne shell that didn't accept `export var=value` and didn't do tilde expansion).\n¹  This wasn't true in Bourne shells (as in the actual Bourne shell, not modern POSIX-style shells), but you're highly unlikely to encounter such old shells these days.",
    "question_score": 1406,
    "answer_score": 1577,
    "question_id": 26047,
    "has_full_answer": true,
    "tags": [
      "bash",
      "environment-variables",
      "path",
      "bashrc"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to cycle through reverse-i-search in Bash?",
    "response": "If I understand the question correctly you should be able to cycle through\nalternatives by repeatedly hitting Ctrl + R.\nE.g.:\n\nCtrl + R\n`grep`\nCtrl + R\nCtrl + R\n...\n\nThat searches backwards through your history.  To search forward instead, use Ctrl + S, but you may need to have set: `stty -ixon` (either by `.bash_profile` or manually) prior to that to disable the XON/XOFF feature which takes over Ctrl + S. If it happens anyway, use Ctrl + Q to re-enable screen output (More details here.)",
    "question_score": 857,
    "answer_score": 1149,
    "question_id": 73498,
    "has_full_answer": true,
    "tags": [
      "bash"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Using \"${a:-b}\" for variable assignment in scripts",
    "response": "This technique allows for a variable to be assigned a value if another variable is either empty or is undefined. NOTE: This \"other variable\" can be the same or another variable.\nexcerpt\n${parameter:-word}\n```bash\nIf parameter is unset or null, the expansion of word is substituted. \nOtherwise, the value of parameter is substituted.\n```\n\nNOTE: This form also works, `${parameter-word}`. According to the Bash documentation, for all such expansions:\n\nOmitting the colon results in a test only for a parameter that is unset. Put another way, if the colon is included, the operator tests for both parameter’s existence and that its value is not null; if the colon is omitted, the operator tests only for existence.\n\nIf you'd like to see a full list of all forms of parameter expansion available within Bash then I highly suggest you take a look at this topic in the Bash Hacker's wiki titled: \"Parameter expansion\".\nExamples\nvariable doesn't exist\n$ echo \"$VAR1\"\n\n$ VAR1=\"${VAR1:-default value}\"\n$ echo \"$VAR1\"\ndefault value\n\nvariable exists\n$ VAR1=\"has value\"\n$ echo \"$VAR1\"\nhas value\n\n$ VAR1=\"${VAR1:-default value}\"\n$ echo \"$VAR1\"\nhas value\n\nThe same thing can be done by evaluating other variables, or running commands within the default value portion of the notation.\n$ VAR2=\"has another value\"\n$ echo \"$VAR2\"\nhas another value\n$ echo \"$VAR1\"\n\n$\n\n$ VAR1=\"${VAR1:-$VAR2}\"\n$ echo \"$VAR1\"\nhas another value\n\nMore Examples\nYou can also use a slightly different notation where it's just `VARX=${VARX-}`.\n$ echo \"${VAR1-0}\"\nhas another value\n$ echo \"${VAR2-0}\"\nhas another value\n$ echo \"${VAR3-0}\"\n0\n\nIn the above `$VAR1` & `$VAR2` were already defined with the string \"has another value\" but `$VAR3` was undefined, so the default value was used instead, `0`.\nAnother Example\n$ VARX=\"${VAR3-0}\"\n$ echo \"$VARX\"\n0\n\nChecking and assigning using `:=` notation\nLastly I'll mention the handy operator, `:=`. This will do a check and assign a value if the variable under test is empty or undefined.\nExample\nNotice that `$VAR1` is now set. The operator `:=` did the test and the assignment in a single operation.\n$ unset VAR1\n$ echo \"$VAR1\"\n\n$ echo \"${VAR1:=default}\"\ndefault\n$ echo \"$VAR1\"\ndefault\n\nHowever if the value is set prior, then it's left alone.\n$ VAR1=\"some value\"\n$ echo \"${VAR1:=default}\"\nsome value\n$ echo \"$VAR1\"\nsome value\n\nHandy Dandy Reference Table\n\nParameter set and not null\nParameter set but null\nParameter unset\n\n`${parameter:-word}`\nsubstitute parameter\nsubstitute word\nsubstitute word\n\n`${parameter-word}`\nsubstitute parameter\nsubstitute null\nsubstitute word\n\n`${parameter:=word}`\nsubstitute parameter\nassign word\nassign word\n\n`${parameter=word}`\nsubstitute parameter\nsubstitute null\nassign word\n\n`${parameter:?word}`\nsubstitute parameter\nerror, exit\nerror, exit\n\n`${parameter?word}`\nsubstitute parameter\nsubstitute null\nerror, exit\n\n`${parameter:+word}`\nsubstitute word\nsubstitute null\nsubstitute null\n\n`${parameter+word}`\nsubstitute word\nsubstitute word\nsubstitute null\n\n(Screenshot of source table)\nThis makes the difference between assignment and substitution explicit: Assignment sets a value for the variable whereas substitution doesn't.\nReferences\n\nParameter Expansions - Bash Hackers Wiki\n10.2. Parameter Substitution\nBash Parameter Expansions",
    "question_score": 659,
    "answer_score": 1077,
    "question_id": 122845,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "scripting",
      "variable"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to get execution time of a script effectively?",
    "response": "Just use `time` when you call the script:\ntime yourscript.sh\n\nOutput (\"# comments not really in the output\"):\nreal    2m5.034s # <-- Actual time taken from start to finish.\nuser    0m0.000s # <-- CPU time user-space.\nsys     0m0.003s # <-- CPU time kernel-space.",
    "question_score": 743,
    "answer_score": 972,
    "question_id": 52313,
    "has_full_answer": true,
    "tags": [
      "bash",
      "profiling"
    ],
    "topic": "bash"
  },
  {
    "instruction": "In a bash script, using the conditional \"or\" in an \"if\" statement",
    "response": "If you want to say `OR` use double pipe (`||`).\n\nif [ \"$fname\" = \"a.txt\" ] || [ \"$fname\" = \"c.txt\" ]\n\n(The original OP code using `|` was simply piping the output of the left side to the right side, in the same way any ordinary pipe works.)\n\nAfter many years of comments and misunderstanding, allow me to clarify.\n\nTo do `OR` you use `||`.\n\nWhether you use `[` or `[[` or `test` or `((` all depends on what you need on a case by case basis. It's wrong to say that one of those is preferred in all cases. Sometimes `[` is right and `[[` is wrong. But that's not what the question was. OP asked why `|` didn't work. The answer is because it should be `||` instead.",
    "question_score": 603,
    "answer_score": 917,
    "question_id": 47584,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How can I resolve a hostname to an IP address in a Bash script?",
    "response": "You can use `getent`, which comes with `glibc` (so you almost certainly have it on Linux). This resolves using gethostbyaddr/gethostbyname2, and so also will check `/etc/hosts`/NIS/etc:\n\ngetent hosts unix.stackexchange.com | awk '{ print $1 }'\n\nOr, as Heinzi said below, you can use `dig` with the `+short` argument (queries DNS servers directly, does not look at `/etc/hosts`/NSS/etc) :\n\ndig +short unix.stackexchange.com\n\nIf `dig +short` is unavailable, any one of the following should work. All of these query DNS directly and ignore other means of resolution:\n\nhost unix.stackexchange.com | awk '/has address/ { print $4 }'\nnslookup unix.stackexchange.com | awk '/^Address: / { print $2 }'\ndig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 }'\n\nIf you want to only print one IP, then add the `exit` command to `awk`'s workflow.\n\ndig +short unix.stackexchange.com | awk '{ print ; exit }'\ngetent hosts unix.stackexchange.com | awk '{ print $1 ; exit }'\nhost unix.stackexchange.com | awk '/has address/ { print $4 ; exit }'\nnslookup unix.stackexchange.com | awk '/^Address: / { print $2 ; exit }'\ndig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 ; exit }'",
    "question_score": 631,
    "answer_score": 856,
    "question_id": 20784,
    "has_full_answer": true,
    "tags": [
      "linux",
      "bash",
      "networking",
      "dns"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to remove a single line from history?",
    "response": "Preventative measures\n\nIf you want to run a command without saving it in history, prepend it with an extra space\n\nprompt$ echo saved\nprompt$  echo not saved \\\n> #     ^ extra space\n\nFor this to work you need either `ignorespace` or `ignoreboth` in `HISTCONTROL`.  For example, run\n\nHISTCONTROL=ignorespace\n\nTo make this setting persistent, put it in your `.bashrc`.\n\nPost-mortem clean-up\n\nIf you've already run the command, and want to remove it from history, first use\n\nhistory\n\nto display the list of commands in your history.  Find the number next to the one you want to delete (e.g. 1234) and run \n\nhistory -d 1234\n\nAdditionally, if the line you want to delete has already been written to your $HISTFILE (which typically happens when you end a session by default), you will need to write back to $HISTFILE, or the line will reappear when you open a new session:\n\nhistory -w",
    "question_score": 446,
    "answer_score": 709,
    "question_id": 49214,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "command-line",
      "command-history"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How do I change the extension of multiple files?",
    "response": "Straight from Greg's Wiki:\n# Rename all *.txt to *.text\nfor file in *.txt; do\n```bash\nmv -- \"$file\" \"${file%.txt}.text\"\n```\ndone\n\n`*.txt` is a globbing pattern, using `*` as a wildcard to match any string. `*.txt` matches all filenames ending with '.txt'.\n`--` marks the end of the option list. This avoids issues with filenames starting with hyphens.\n`${file%.txt}` is a parameter expansion, replaced by the value of the `file` variable with `.txt` removed from the end.\nAlso see the entry on why you shouldn't parse `ls`.\nIf you have to use `basename`, your syntax would be:\nfor file in *.txt; do\n```bash\nmv -- \"$file\" \"$(basename -- \"$file\" .txt).text\"\n```\ndone",
    "question_score": 463,
    "answer_score": 573,
    "question_id": 19654,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "rename"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to define 'tab' delimiter with 'cut' in Bash?",
    "response": "Two ways:\nPress Ctrl+V and then Tab to use \"verbatim\" quoted insert.\ncut -f2 -d'   ' infile\n\nor write it like this to use ANSI-C quoting:\ncut -f2 -d$'\\t' infile\n\nThe `$'...'` form of quotes isn't part of the POSIX shell language (not yet), but works at least in ksh, mksh, zsh and Busybox in addition to Bash.",
    "question_score": 378,
    "answer_score": 529,
    "question_id": 35369,
    "has_full_answer": true,
    "tags": [
      "bash",
      "cut"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Why does my shell script choke on whitespace or other special characters?",
    "response": "Always use double quotes around variable substitutions and command substitutions: `\"$foo\"`, `\"$(foo)\"`\nIf you use `$foo` unquoted, your script will choke on input or parameters (or command output, with `$(foo)`) containing whitespace or `\\[*?`.\nThere, you can stop reading. Well, ok, here are a few more:\n\n`read` — To read input line by line with the `read` builtin, use `while IFS= read -r line; do …`\nPlain `read` treats backslashes and whitespace specially.\n`xargs` — Avoid `xargs`. If you must use `xargs`, make that `xargs -0`. Instead of `find … | xargs`, prefer `find … -exec …`.\n`xargs` treats whitespace and the characters `\\\"'` specially.\n\nThis answer applies to Bourne/POSIX-style shells (`sh`, `ash`, `dash`, `bash`, `ksh`, `mksh`, `yash`…). Zsh users should skip it and read the end of When is double-quoting necessary? instead. If you want the whole nitty-gritty, read the standard or your shell's manual.\n\nNote that the explanations below contains a few approximations (statements that are true in most conditions but can be affected by the surrounding context or by configuration).\nWhy do I need to write `\"$foo\"`? What happens without the quotes?\n`$foo` does not mean “take the value of the variable `foo`”. It means something much more complex:\n\nFirst, take the value of the variable.\nField splitting: treat that value as a whitespace-separated list of fields, and build the resulting list. For example, if the variable contains `foo *  bar ​` then the result of this step is the 3-element list `foo`, `*`, `bar`.\nFilename generation: treat each field as a glob, i.e. as a wildcard pattern, and replace it by the list of file names that match this pattern. If the pattern doesn't match any files, it is left unmodified. In our example, this results in the list containing `foo`, following by the list of files in the current directory, and finally `bar`. If the current directory is empty, the result is `foo`, `*`, `bar`.\n\nNote that the result is a list of strings. There are two contexts in shell syntax: list context and string context. Field splitting and filename generation only happen in list context, but that's most of the time. Double quotes delimit a string context: the whole double-quoted string is a single string, not to be split. (Exception: `\"$@\"` to expand to the list of positional parameters, e.g. `\"$@\"` is equivalent to `\"$1\" \"$2\" \"$3\"` if there are three positional parameters. See What is the difference between $* and $@?)\nThe same happens to command substitution with `$(foo)` or with ``foo``. On a side note, don't use ``foo``: its quoting rules are weird and non-portable, and all modern shells support `$(foo)` which is absolutely equivalent except for having intuitive quoting rules.\nThe output of arithmetic substitution also undergoes the same expansions, but that isn't normally a concern as it only contains non-expandable characters (assuming `IFS` doesn't contain digits or `-`).\nSee When is double-quoting necessary? for more details about the cases when you can leave out the quotes.\nUnless you mean for all this rigmarole to happen, just remember to always use double quotes around variable and command substitutions. Do take care: leaving out the quotes can lead not just to errors but to security holes.\nHow do I process a list of file names?\nIf you write `myfiles=\"file1 file2\"`, with spaces to separate the files, this can't work with file names containing spaces. Unix file names can contain any character other than `/` (which is always a directory separator) and null bytes (which you can't use in shell scripts with most shells).\nSame problem with `myfiles=*.txt; … process $myfiles`. When you do this, the variable `myfiles` contains the 5-character string `*.txt`, and it's when you write `$myfiles` that the wildcard is expanded. This example will actually work, until you change your script to be `myfiles=\"$someprefix*.txt\"; … process $myfiles`. If `someprefix` is set to `final report`, this won't work.\nTo process a list of any kind (such as file names), put it in an array. This requires mksh, ksh93, yash or bash (or zsh, which doesn't have all these quoting issues); a plain POSIX shell (such as ash or dash) doesn't have array variables.\nmyfiles=(\"$someprefix\"*.txt)\nprocess \"${myfiles[@]}\"\n\nKsh88 has array variables with a different assignment syntax `set -A myfiles \"someprefix\"*.txt` (see assignation variable under different ksh environment if you need ksh88/bash portability). Bourne/POSIX-style shells have a single one array, the array of positional parameters `\"$@\"` which you set with `set` and which is local to a function:\nset -- \"$someprefix\"*.txt\nprocess -- \"$@\"\n\nWhat about file names that begin with `-`?\nOn a related note, keep in mind that file names can begin with a `-` (dash/minus), which most commands interpret as denoting an option. Some commands (like `sh`, `set` or `sort`) also accept options that start with `+`. If you have a file name that begins with a variable part, be sure to pass `--` before it, as in the snippet above. This indicates to the command that it has reached the end of options, so anything after that is a file name even if it starts with `-` or `+`.\nAlternatively, you can make sure that your file names begin with a character other than `-`. Absolute file names begin with `/`, and you can add `./` at the beginning of relative names. The following snippet turns the content of the variable `f` into a “safe” way of referring to the same file that's guaranteed not to start with `-` nor `+`.\ncase \"$f\" in -* | +*) \"f=./$f\";; esac\n\nOn a final note on this topic, beware that some commands interpret `-` as meaning standard input or standard output, even after `--`. If you need to refer to an actual file named `-`, or if you're calling such a program and you don't want it to read from stdin or write to stdout, make sure to rewrite `-` as above. See What is the difference between \"du -sh *\" and \"du -sh ./*\"? for further discussion.\nHow do I store a command in a variable?\n“Command” can mean three things: a command name (the name as an executable, with or without full path, or the name of a function, builtin or alias), a command name with arguments, or a piece of shell code. There are accordingly different ways of storing them in a variable.\nIf you have a command name, just store it and use the variable with double quotes as usual.\ncommand_path=\"$1\"\n…\n\"$command_path\" --option --message=\"hello world\"\n\nIf you have a command with arguments, the problem is the same as with a list of file names above: this is a list of strings, not a string. You can't just stuff the arguments into a single string with spaces in between, because if you do that you can't tell the difference between spaces that are part of arguments and spaces that separate arguments. If your shell has arrays, you can use them.\ncmd=(/path/to/executable --option --message=\"hello world\" --)\ncmd=(\"${cmd[@]}\" \"$file1\" \"$file2\")\n\"${cmd[@]}\"\n\nWhat if you're using a shell without arrays? You can still use the positional parameters, if you don't mind modifying them.\nset -- /path/to/executable --option --message=\"hello world\" --\nset -- \"$@\" \"$file1\" \"$file2\"\n\"$@\"\n\nWhat if you need to store a complex shell command, e.g. with redirections, pipes, etc.? Or if you don't want to modify the positional parameters? Then you can build a string containing the command, and use the `eval` builtin.\ncode='/path/to/executable --option --message=\"hello world\" -- /path/to/file1 | grep \"interesting stuff\"'\neval \"$code\"\n\nNote the nested quotes in the definition of `code`: the single quotes `'…'` delimit a string literal, so that the value of the variable `code` is the string `/path/to/executable --option --message=\"hello world\" -- /path/to/file1`. The `eval` builtin tells the shell to parse the string passed as an argument as if it appeared in the script, so at that point the quotes and pipe are parsed, etc.\nUsing `eval` is tricky. Think carefully about what gets parsed when. In particular, you can't just stuff a file name into the code: you need to quote it, just like you would if it was in a source code file. There's no direct way to do that. Something like `code=\"$code $filename\"` breaks if the file name contains any shell special character (spaces, `$`, `;`, `|`, ``, etc.). `code=\"$code \\\"$filename\\\"\"` still breaks on `\"$\\``. Even `code=\"$code '$filename'\"` breaks if the file name contains a `'`. There are two solutions.\n\nAdd a layer of quotes around the file name. The easiest way to do that is to add single quotes around it, and replace single quotes by `'\\''`.\n quoted_filename=$(printf %s. \"$filename\" | sed \"s/'/'\\\\\\\\''/g\")\n code=\"$code '${quoted_filename%.}'\"\n\nKeep the variable expansion inside the code, so that it's looked up when the code is evaluated, not when the code fragment is built. This is simpler but only works if the variable is still around with the same value at the time the code is executed, not e.g. if the code is built in a loop.\n code=\"$code \\\"\\$filename\\\"\"\n\nFinally, do you really need a variable containing code? The most natural way to give a name to a code block is to define a function.\nWhat's up with `read`?\nWithout `-r`, `read` allows continuation lines — this is a single logical line of input:\nhello \\\nworld\n\n`read` splits the input line into fields delimited by characters in `$IFS` (without `-r`, backslash also escapes those). For example, if the input is a line containing three words, then `read first second third` sets `first` to the first word of input, `second` to the second word and `third` to the third word. If there are more words, the last variable contains everything that's left after setting the preceding ones. Leading and trailing whitespace are trimmed.\nSetting `IFS` to the empty string avoids any trimming. See Why is `while IFS= read` used so often, instead of `IFS=; while read..`? for a longer explanation.\nWhat's wrong with `xargs`?\nThe input format of `xargs` is whitespace-separated strings which can optionally be single- or double-quoted. No standard tool outputs this format.\n`xargs -L1` or `xargs -l` is not to split the input on lines, but to run one command per line of input (that line still split to make up the arguments, and continued on the next line if ending in blanks).\n`xargs -I PLACEHOLDER` does use one line of input to substitute the `PLACEHOLDER` but quotes and backslashes are still processed and leading blanks trimmed.\nYou can use `xargs -r0` where applicable (and where available: GNU (Linux, Cygwin), BusyBox, BSDs, OSX, but it isn't in POSIX). That's safe, because null bytes can't appear in most data, in particular in file names and external command arguments. To produce a null-separated list of file names, use `find … -print0` (or you can use `find … -exec …` as explained below).\nHow do I process files found by `find`?\nfind … -exec some_command a_parameter another_parameter {} +\n\n`some_command` needs to be an external command, it can't be a shell function or alias. If you need to invoke a shell to process the files, call `sh` explicitly.\nfind … -exec sh -c '\n  for x do\n```bash\n… # process the file \"$x\"\n```\n  done\n' find-sh {} +\n\nI have some other question\nBrowse the quoting tag on this site, or shell or shell-script. (Click on “learn more…” to see some general tips and a hand-selected list of common questions.) If you've searched and you can't find an answer, ask away.",
    "question_score": 377,
    "answer_score": 482,
    "question_id": 131766,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "shell-script",
      "quoting",
      "whitespace"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How do I clear Bash's cache of paths to executables?",
    "response": "`bash` does cache the full path to a command.  You can verify that the command you are trying to execute is hashed with the `type` command:\n\n$ type svnsync\nsvnsync is hashed (/usr/local/bin/svnsync)\n\nTo clear the entire cache:\n\n$ hash -r\n\nOr just one entry:\n\n$ hash -d svnsync\n\nFor additional information, consult `help hash` and `man bash`.",
    "question_score": 376,
    "answer_score": 474,
    "question_id": 5609,
    "has_full_answer": true,
    "tags": [
      "bash",
      "executable",
      "cache"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Preserve bash history in multiple terminal windows",
    "response": "Add the following to your `~/.bashrc`:\n# Avoid duplicates\nHISTCONTROL=ignoredups:erasedups\n# When the shell exits, append to the history file instead of overwriting it\nshopt -s histappend\n\n# After each command, append to the history file and reread it\nPROMPT_COMMAND=\"${PROMPT_COMMAND:+$PROMPT_COMMAND$'\\n'}history -a; history -c; history -r\"",
    "question_score": 724,
    "answer_score": 468,
    "question_id": 1288,
    "has_full_answer": true,
    "tags": [
      "bash",
      "command-history"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How can I get the size of a file in a bash script?",
    "response": "Your best bet if on a GNU system:\nstat --printf=\"%s\" file.any\n\nFrom man stat:\n\n%s total size, in bytes\n\nIn a bash script :\n#!/bin/bash\nFILENAME=/home/heiko/dummy/packages.txt\nFILESIZE=$(stat -c%s \"$FILENAME\")\necho \"Size of $FILENAME = $FILESIZE bytes.\"\n\nNOTE: see @chbrown's answer for how to use `stat` on BSD or macOS systems.",
    "question_score": 430,
    "answer_score": 431,
    "question_id": 16640,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "files"
    ],
    "topic": "bash"
  },
  {
    "instruction": "In Bash, when to alias, when to script and when to write a function?",
    "response": "An alias should effectively not (in general) do more than change the default options of a command. It is nothing more than simple text replacement on the command name.  It can't do anything with arguments but pass them to the command it actually runs. So if you simply need to add an argument at the front of a single command, an alias will work.  Common examples are\n\n# Make ls output in color by default.\nalias ls=\"ls --color=auto\"\n# make mv ask before overwriting a file by default\nalias mv=\"mv -i\"\n\nA function should be used when you need to do something more complex than an alias but that wouldn't be of use on its own.  For example, take this answer on a question I asked about changing `grep`'s default behavior depending on whether it's in a pipeline:\n\ngrep() { \n```bash\nif [[ -t 1 ]]; then \n```\n        command grep -n \"$@\"\n```bash\nelse \n```\n        command grep \"$@\"\n```bash\nfi\n```\n}\n\nIt's a perfect example of a function because it is too complex for an alias (requiring different defaults based on a condition), but it's not something you'll need in a non-interactive script.  \n\nIf you get too many functions or functions too big, put them into separate files in a hidden directory, and source them in your `~/.bashrc`:\n\nif [ -d ~/.bash_functions ]; then\n```bash\nfor file in ~/.bash_functions/*; do\n```\n        . \"$file\"\n```bash\ndone\n```\nfi\n\nA script should stand on its own. It should have value as something that can be re-used, or used for more than one purpose.",
    "question_score": 438,
    "answer_score": 275,
    "question_id": 30925,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "alias",
      "function"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How do I loop through only directories in bash?",
    "response": "If you need to select more specific files than only directories use `find` and pass it to `while read`:\n\nshopt -s dotglob\nfind * -prune -type d | while IFS= read -r d; do \n```bash\necho \"$d\"\n```\ndone\n\nUse `shopt -u dotglob` to exclude hidden directories (or `setopt dotglob`/`unsetopt dotglob` in zsh).\n\n`IFS=` to avoid splitting filenames containing one of the `$IFS`, for example: `'a b'`\n\nsee AsymLabs answer below for more `find` options\n\nedit:\nIn case you need to create an exit value from within the while loop, you can circumvent the extra subshell by this trick:\n\nwhile IFS= read -r d; do \n```bash\nif [ \"$d\" == \"something\" ]; then exit 1; fi\n```\ndone < <(find * -prune -type d)",
    "question_score": 538,
    "answer_score": 35,
    "question_id": 86722,
    "has_full_answer": true,
    "tags": [
      "bash",
      "files",
      "directory"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How can I make \"Press any key to continue\"",
    "response": "You can use the `read` command. If you are using `bash`:\nread -p \"Press enter to continue\"\n\nIn other shells, you can do:\nprintf \"%s \" \"Press enter to continue\"\nread ans\n\nAs mentioned in the comments above, this command does actually require the user to press enter; a solution that works with any key in `bash` would be:\nread -n 1 -s -r -p \"Press any key to continue\"\n\nExplanation by Rayne and wchargin\n`-n` defines the required character count to stop reading\n`-s` hides the user's input\n`-r` causes the string to be interpreted \"raw\" (without considering backslash escapes)",
    "question_score": 347,
    "answer_score": 557,
    "question_id": 293940,
    "has_full_answer": true,
    "tags": [
      "bash"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How can I get the current working directory?",
    "response": "There's no need to do that, it's already in a variable:\n$ echo \"$PWD\"\n/home/terdon\n\nThe `PWD` variable is defined by POSIX and will work on all POSIX-compliant shells:\n\nPWD\n\nSet by the shell and by the cd utility. In the shell the value\nshall be initialized from the environment as follows. If a value for\nPWD is passed to the shell in the environment when it is executed, the\nvalue is an absolute pathname of the current working directory that is\nno longer than {PATH_MAX} bytes including the terminating null byte,\nand the value does not contain any components that are dot or dot-dot,\nthen the shell shall set PWD to the value from the environment.\nOtherwise, if a value for PWD is passed to the shell in the\nenvironment when it is executed, the value is an absolute pathname of\nthe current working directory, and the value does not contain any\ncomponents that are dot or dot-dot, then it is unspecified whether the\nshell sets PWD to the value from the environment or sets PWD to the\npathname that would be output by pwd -P. Otherwise, the sh utility\nsets PWD to the pathname that would be output by pwd -P. In cases\nwhere PWD is set to the value from the environment, the value can\ncontain components that refer to files of type symbolic link. In cases\nwhere PWD is set to the pathname that would be output by pwd -P, if\nthere is insufficient permission on the current working directory, or\non any parent of that directory, to determine what that pathname would\nbe, the value of PWD is unspecified. Assignments to this variable may\nbe ignored. If an application sets or unsets the value of PWD, the\nbehaviors of the cd and pwd utilities are unspecified.\n\nFor the more general answer, the way to save the output of a command in a variable is to enclose the command in `$()` or `` `` (backticks):\nvar=$(command)\n\nor\nvar=`command`\n\nOf the two, the `$()` is preferred since it is easier to build complex commands like:\ncommand0 \"$(command1 \"$(command2 \"$(command3)\")\")\"\n\nWhose backtick equivalent would look like:\ncommand0 \"`command1 \\\"\\`command2 \\\\\\\"\\\\\\`command3\\\\\\`\\\\\\\"\\`\\\"`\"",
    "question_score": 354,
    "answer_score": 522,
    "question_id": 188182,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "scripting",
      "directory"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to get the pid of the last executed command in shell script?",
    "response": "The PID of the most recently executed background (asynchronous) command is in the `$!` shell variable:\nmy-app &\necho $!",
    "question_score": 351,
    "answer_score": 459,
    "question_id": 30370,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "process",
      "background-process"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How can I test if a variable is empty or contains only spaces?",
    "response": "First, note that the `-z` test is explicitly for:\n\n  the length of string is zero\n\nThat is, a string containing only spaces should not be true under `-z`, because it has a non-zero length.\n\nWhat you want is to remove the spaces from the variable using the pattern replacement parameter expansion:\n\n[[ -z \"${param// }\" ]]\n\nThis expands the `param` variable and replaces all matches of the pattern `` (a single space) with nothing, so a string that has only spaces in it will be expanded to an empty string.\n\nThe nitty-gritty of how that works is that `${var/pattern/string}` replaces the first longest match of `pattern` with `string`. When `pattern` starts with `/` (as above) then it replaces all the matches. Because the replacement is empty, we can omit the final `/` and the `string` value:\n\n  ${parameter/pattern/string}\n  \n  The pattern is expanded to produce a pattern just as in filename expansion. Parameter is expanded and the longest match of pattern against its value is replaced with string. If pattern begins with ‘/’, all matches of pattern are replaced with string. Normally only the first match is replaced. ... If string is null, matches of pattern are deleted and the / following pattern may be omitted.\n\nAfter all that, we end up with `${param// }` to delete all spaces.\n\nNote that though present in `ksh` (where it originated), `zsh` and `bash`, that syntax is not POSIX and should not be used in `sh` scripts.",
    "question_score": 320,
    "answer_score": 375,
    "question_id": 146942,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "shell-script",
      "string"
    ],
    "topic": "bash"
  },
  {
    "instruction": "What does \"rc\" in .bashrc stand for?",
    "response": "As is often the case with obscure terms, the Jargon File has an answer:\n\n  [Unix: from runcom files on the CTSS system 1962-63, via the startup script /etc/rc] Script file containing startup instructions for an application program (or an entire operating system), usually a text file containing commands of the sort that might have been invoked manually once the system was running but are to be executed automatically each time the system starts up.\n\nThus, it would seem that the \"rc\" part stands for \"runcom\", which I believe can be expanded to \"run commands\".  In fact, this is exactly what the file contains, commands that bash should run.",
    "question_score": 353,
    "answer_score": 356,
    "question_id": 3467,
    "has_full_answer": true,
    "tags": [
      "bash",
      "bashrc",
      "history"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to colorize output of git?",
    "response": "You can create a section `[color]` in your `~/.gitconfig` with e.g. the following content\n[color]\n  diff = auto\n  status = auto\n  branch = auto\n  interactive = auto\n  ui = true\n  pager = true\n\nYou can also fine control what you want to have coloured in what way, e.g.\n[color \"status\"]\n  added = green\n  changed = red bold\n  untracked = magenta bold\n\n[color \"branch\"]\n  remote = yellow\n\nI hope this gets you started. And of course, you need a terminal which supports colour.\nAlso see this answer for a way to add colorization directly from the command line.",
    "question_score": 331,
    "answer_score": 349,
    "question_id": 44266,
    "has_full_answer": true,
    "tags": [
      "bash",
      "colors",
      "git"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Is there a one-liner that allows me to create a directory and move into it at the same time?",
    "response": "This is the one-liner that you need. No other config needed:\n\nmkdir longtitleproject && cd $_\n\nThe `$_` variable, in bash, is the last argument given to the previous command. In this case, the name of the directory you just created. As explained in `man bash`:\n\n_         At  shell  startup,  set to the absolute pathname used to invoke\n          the shell or shell script being executed as passed in the  envi‐\n          ronment  or  argument  list.   Subsequently, expands to the last\n          argument to the previous command, after expansion.  Also set  to\n          the  full  pathname  used  to  invoke  each command executed and\n          placed in the environment exported to that command.  When check‐\n          ing  mail,  this  parameter holds the name of the mail file cur‐\n          rently being checked.\"$_\" is the last argument of the previous command.\n\nUse `cd $_` to retrieve the last argument of the previous command instead of `cd !$` because `cd !$` gives the last argument of previous command in the shell history:\n\ncd ~/\nmkdir folder && cd !$\n\nyou end up home (or ~/ )\n\ncd ~/\nmkdir newfolder && cd $_\n\nyou end up in newfolder under home !! ( or ~/newfolder )",
    "question_score": 284,
    "answer_score": 301,
    "question_id": 9123,
    "has_full_answer": true,
    "tags": [
      "bash",
      "cd-command",
      "mkdir"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Passing named arguments to shell scripts",
    "response": "If you don't mind being limited to single-letter argument names i.e. `my_script -p '/some/path' -a5`, then in bash you could use the built-in `getopts`, e.g.\n#!/bin/bash\n\nwhile getopts \":a:p:\" opt; do\n  case $opt in\n```bash\na) arg_1=\"$OPTARG\"\n;;\np) p_out=\"$OPTARG\"\n;;\n\\?) echo \"Invalid option -$OPTARG\" >&2\nexit 1\n;;\n```\n  esac\n\n  case $OPTARG in\n```bash\n-*) echo \"Option $opt needs a valid argument\"\nexit 1\n;;\n```\n  esac\ndone\n\nprintf \"Argument p_out is %s\\n\" \"$p_out\"\nprintf \"Argument arg_1 is %s\\n\" \"$arg_1\"\n\nThen you can do\n$ ./my_script -p '/some/path' -a5\nArgument p_out is /some/path\nArgument arg_1 is 5\n\nThere is a helpful Small getopts tutorial or you can type `help getopts` at the shell prompt.\nEdit: The second `case` statement in `while` loop triggers if the `-p` option has no arguments and is followed by another option, e.g. `my_script -p -a5`, and `exit`s the program.",
    "question_score": 284,
    "answer_score": 280,
    "question_id": 129391,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "zsh",
      "arguments"
    ],
    "topic": "bash"
  },
  {
    "instruction": "What is the \"eval\" command in bash?",
    "response": "`eval` is part of POSIX. It's an interface which can be a shell built-in.\nIt's described in the \"POSIX Programmer's Manual\": http://www.unix.com/man-page/posix/1posix/eval/\neval - construct command by concatenating arguments\n\nIt will take an argument and construct a command of it, which will then be executed by the shell. This is the example from the manpage:\nfoo=10 x=foo    # 1\ny='$'$x         # 2\necho $y         # 3\n$foo\neval y='$'$x    # 5\necho $y         # 6\n10\n\nIn the first line you define `$foo` with the value `'10'` and `$x` with the value `'foo'`.\nNow define `$y`, which consists of the string `'$foo'`. The dollar sign must be escaped\nwith `'$'`.\nTo check the result, `echo $y`.\nThe result of 1)-3) will be the string `'$foo'`\nNow we repeat the assignment with `eval`. It will first evaluate `$x` to the string `'foo'`. Now we have the statement `y=$foo` which will get evaluated to `y=10`.\nThe result of `echo $y` is now the value `'10'`.\n\nThis is a common function in many languages:\n\nPerl\nJavascript",
    "question_score": 339,
    "answer_score": 253,
    "question_id": 23111,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "eval"
    ],
    "topic": "bash"
  },
  {
    "instruction": "What is the purpose of .bashrc and how does it work?",
    "response": "`.bashrc` is a Bash shell script that Bash runs whenever it is started interactively. It initializes an interactive shell session. You can put any command in that file that you could type at the command prompt.\nYou put commands here to set up the shell for use in your particular environment, or to customize things to your preferences. A common thing to put in `.bashrc` are aliases that you want to always be available.\n`.bashrc` runs on every interactive shell launch. If you say:\n$ bash ; bash ; bash\n\nand then hit Ctrl-D three times, `.bashrc` will run three times.  But if you say this instead:\n$ bash -c exit ; bash -c exit ; bash -c exit\n\nthen `.bashrc` won't run at all, since `-c` makes the Bash call non-interactive. The same is true when you run a shell script from a file.\nContrast `.bash_profile` and `.profile` which are only run at the start of a new login shell. (`bash -l`) You choose whether a command goes in `.bashrc` vs `.bash_profile` depending on whether you want it to run once or for every interactive shell start.\nAs a counterexample to aliases, which I prefer to put in `.bashrc`, you want to do `PATH` adjustments in `.bash_profile` instead, since these changes are typically not idempotent:\nexport PATH=\"$PATH:/some/addition\"\n\nIf you put that in `.bashrc` instead, every time you launched an interactive sub-shell, `:/some/addition` would get tacked onto the end of the `PATH` again, creating extra work for the shell when you mistype a command.\nYou get a new interactive Bash shell whenever you shell out of `vi` with `:sh`, for example.",
    "question_score": 281,
    "answer_score": 253,
    "question_id": 129143,
    "has_full_answer": true,
    "tags": [
      "bash",
      "bashrc"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Why doesn't my Bash script recognize aliases?",
    "response": "First of all, as ddeimeke said, aliases by default are not expanded in non-interactive shells.\n\nSecond, `.bashrc` is not read by non-interactive shells unless you set the `BASH_ENV` environment variable.\n\nBut most importantly: don't do that! Please? One day you will move that script somewhere where the necessary aliases are not set and it will break again.\n\nInstead set and use variables as shortcuts in your script:\n\n#!/bin/bash\n\nCMDA=/path/to/gizmo\nCMDB=/path/to/huzzah.sh\n\nfor file in \"$@\"\ndo\n```bash\n$CMDA \"$file\"\n$CMDB \"$file\"\n```\ndone",
    "question_score": 353,
    "answer_score": 166,
    "question_id": 1496,
    "has_full_answer": true,
    "tags": [
      "bash",
      "alias"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Colorizing your terminal and shell environment?",
    "response": "Here are a couple of things you can do:\n\nEditors + Code\nA lot of editors have syntax highlighting support. `vim` and `emacs` have it on by default. You can also enable it under `nano`.\n\nYou can also syntax highlight code on the terminal by using Pygments as a command-line tool.\n\ngrep\n`grep --color=auto` highlights all matches. You can also use `export GREP_OPTIONS='--color=auto'` to make it persistent without an alias. If you use `--color=always`, it'll use colour even when piping, which confuses things.\n\nls\n\n`ls --color=always`\n\nColors specified by:\n\nexport LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33'\n\n(hint: `dircolors` can be helpful)\n\nPS1\nYou can set your PS1 (shell prompt) to use colours. For example:\n\nPS1='\\e[33;1m\\u@\\h: \\e[31m\\W\\e[0m\\$ '\n\nWill produce a PS1 like:\n\n[yellow]lucas@ubuntu: [red]~[normal]$ \n\nYou can get really creative with this. As an idea:\n\nPS1='\\e[s\\e[0;0H\\e[1;33m\\h    \\t\\n\\e[1;32mThis is my computer\\e[u[\\u@\\h:  \\w]\\$ '\n\nPuts a bar at the top of your terminal with some random info. (For best results, also use `alias clear=\"echo -e '\\e[2J\\n\\n'\"`.)\n\nGetting Rid of Escape Sequences\n\nIf something is stuck outputting colour when you don't want it to, I use this `sed` line to strip the escape sequences:\n\nsed \"s/\\[^[[0-9;]*[a-zA-Z]//gi\"\n\nIf you want a more authentic experience, you can also get rid of lines starting with `\\e[8m`, which instructs the terminal to hide the text. (Not widely supported.)\n\nsed \"s/^\\[^[8m.*$//gi\"\n\nAlso note that those ^[s should be actual, literal ^[s. You can type them by pressing ^V^[ in bash, that is Ctrl + V, Ctrl + [.",
    "question_score": 326,
    "answer_score": 155,
    "question_id": 148,
    "has_full_answer": true,
    "tags": [
      "shell",
      "bash",
      "colors",
      "prompt"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Execute a command once per line of piped input?",
    "response": "That's what `xargs` does.\n\n... | xargs command",
    "question_score": 284,
    "answer_score": 128,
    "question_id": 7558,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "fish"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to add a newline to the end of a file?",
    "response": "To recursively sanitize a project I use this oneliner:\ngit ls-files -z | while IFS= read -rd '' f; do if file --brief --mime-encoding \"$f\" | grep -qv binary; then tail -c1 > \"$f\"; fi; done\n\nExplanation:\n\n`git ls-files -z` lists files in the repository. It takes an optional pattern as additional parameter which might be useful in some cases if you want to restrict the operation to certain files/directories. As an alternative, you could use `find -print0 ...` or similar programs to list affected files - just make sure it emits `NUL`-delimited entries.\n\n`while IFS= read -rd '' f; do ... done` iterates through the entries, safely handling filenames that include whitespace and/or newlines.\n\n`if file --brief --mime-encoding \"$f\" | grep -qv binary` checks whether the file is in a binary format (such as images) and skips those.\n\n`tail -c1 \n\n`read -r _` exits with a nonzero exit status if a trailing newline is missing.\n\n`|| echo >> \"$f\"` appends a newline to the file if the exit status of the previous command was nonzero.",
    "question_score": 290,
    "answer_score": 102,
    "question_id": 31947,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "text-processing",
      "newlines"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to determine where an environment variable came from?",
    "response": "If you use the `env` command to display the variables, they should show up roughly in the order in which they were created.  You can use this as a guide to if they were set by the system very early in the boot, or by a later .profile or other configuration file.  In my experience, the `set` and `export` commands will sort their variables by alphabetical order, so that listing isn't as useful.",
    "question_score": 308,
    "answer_score": 90,
    "question_id": 813,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "environment-variables"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How to use watch command with a piped chain of commands/programs",
    "response": "watch 'command | othertool | yet-another-tool'",
    "question_score": 235,
    "answer_score": 374,
    "question_id": 318859,
    "has_full_answer": true,
    "tags": [
      "bash",
      "pipe",
      "watch"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How can I delete a word backward at the command line (bash and zsh)?",
    "response": "Ctrl+W is the standard \"kill word\" (aka `werase`).\nCtrl+U kills the whole line (`kill`).\nYou can change them with `stty`.\n-bash-4.2$ stty -a\nspeed 38400 baud; 24 rows; 80 columns;\nlflags: icanon isig iexten echo echoe -echok echoke -echonl echoctl\n        -echoprt -altwerase -noflsh -tostop -flusho pendin -nokerninfo\n        -extproc -xcase\niflags: -istrip icrnl -inlcr -igncr -iuclc ixon -ixoff ixany imaxbel\n        -ignbrk brkint -inpck -ignpar -parmrk\noflags: opost onlcr -ocrnl -onocr -onlret -olcuc oxtabs -onoeot\ncflags: cread cs8 -parenb -parodd hupcl -clocal -cstopb -crtscts -mdmbuf\ncchars: discard = ^O; dsusp = ^Y; eof = ^D; eol = ;\n        eol2 = ; erase = ^?; intr = ^C; kill = ^U; lnext = ^V;\n        min = 1; quit = ^\\; reprint = ^R; start = ^Q; status = ;\n        stop = ^S; susp = ^Z; time = 0; werase = ^W;\n-bash-4.2$ stty werase ^p\n-bash-4.2$ stty kill ^a\n-bash-4.2$\n\nNote that one does not have to put the actual control character on the line, `stty` understands putting `^` and then the character you would hit with control.\nAfter doing this, if I hit Ctrl+P it will erase a word from the line.  And if I hit Ctrl+A, it will erase the whole line.",
    "question_score": 276,
    "answer_score": 362,
    "question_id": 94331,
    "has_full_answer": true,
    "tags": [
      "bash",
      "keyboard-shortcuts"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How do I remove a directory and all its contents?",
    "response": "The following command will do it for you. Use caution though if this isn't your intention as this also removes files in the directory and subdirectories.\nrm -rf directoryname",
    "question_score": 244,
    "answer_score": 355,
    "question_id": 45676,
    "has_full_answer": true,
    "tags": [
      "bash",
      "rm"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Looping through files with spaces in the names?",
    "response": "Short answer (closest to your answer, but handles spaces)\nOIFS=\"$IFS\"\nIFS=$'\\n'\nfor file in `find . -type f -name \"*.csv\"`  \ndo\n     echo \"file = $file\"\n     diff \"$file\" \"/some/other/path/$file\"\n     read line\ndone\nIFS=\"$OIFS\"\n\nBetter answer (also handles wildcards and newlines in file names)\nfind . -type f -name \"*.csv\" -print0 | while IFS= read -r -d '' file; do\n```bash\necho \"file = $file\"\ndiff \"$file\" \"/some/other/path/$file\"\nread line </dev/tty\n```\ndone\n\nBest answer (based on Gilles' answer)\nfind . -type f -name '*.csv' -exec sh -c '\n  file=\"$0\"\n  echo \"$file\"\n  diff \"$file\" \"/some/other/path/$file\"\n  read line </dev/tty\n' exec-sh {} ';'\n\nOr even better, to avoid running one `sh` per file:\nfind . -type f -name '*.csv' -exec sh -c '\n  for file do\n```bash\necho \"$file\"\ndiff \"$file\" \"/some/other/path/$file\"\nread line </dev/tty\n```\n  done\n' exec-sh {} +\n\nLong answer\nYou have three problems:\n\nBy default, the shell splits the output of a command on spaces, tabs, and newlines\nFilenames could contain wildcard characters which would get expanded\nWhat if there is a directory whose name ends in `*.csv`?\n\n1. Splitting only on newlines\nTo figure out what to set `file` to, the shell has to take the output of `find` and interpret it somehow, otherwise `file` would just be the entire output of `find`.\nThe shell reads the `IFS` variable, which is set to `` by default.\nThen it looks at each character in the output of `find`.  As soon as it sees any character that's in `IFS`, it thinks that marks the end of the file name, so it sets `file` to whatever characters it saw until now and runs the loop.  Then it starts where it left off to get the next file name, and runs the next loop, etc., until it reaches the end of output.\nSo it's effectively doing this:\nfor file in \"zquery\" \"-\" \"abc\" ...\n\nTo tell it to only split the input on newlines, you need to do\nIFS=$'\\n'\n\nbefore your `for ... find` command.\nThat sets `IFS` to a single newline, so it only splits on newlines, and not spaces and tabs as well.\nIf you are using `sh` or `dash` instead of `ksh93`, `bash` or `zsh`, you need to write `IFS=$'\\n'` like this instead:\nIFS='\n'\n\nThat is probably enough to get your script working, but if you're interested to handle some other corner cases properly, read on...\n2. Expanding `$file` without wildcards\nInside the loop where you do\ndiff $file /some/other/path/$file\n\nthe shell tries to expand `$file` (again!).\nIt could contain spaces, but since we already set `IFS` above, that won't be a problem here.\nBut it could also contain wildcard characters such as `*` or `?`, which would lead to unpredictable behavior.  (Thanks to Gilles for pointing this out.)\nTo tell the shell not to expand wildcard characters, put the variable inside double quotes, e.g.\ndiff \"$file\" \"/some/other/path/$file\"\n\nThe same problem could also bite us in\nfor file in `find . -name \"*.csv\"`\n\nFor example, if you had these three files\nfile1.csv\nfile2.csv\n*.csv\n\n(very unlikely, but still possible)\nIt would be as if you had run\nfor file in file1.csv file2.csv *.csv\n\nwhich will get expanded to\nfor file in file1.csv file2.csv *.csv file1.csv file2.csv\n\ncausing `file1.csv` and `file2.csv` to be processed twice.\nInstead, we have to do\nfind . -name \"*.csv\" -print | while IFS= read -r file; do\n```bash\necho \"file = $file\"\ndiff \"$file\" \"/some/other/path/$file\"\nread line </dev/tty\n```\ndone\n\n`read` reads lines from standard input, splits the line into words according to `IFS` and stores them in the variable names that you specify.\nHere, we're telling it not to split the line into words, and to store the line in `$file`.\nAlso note that `read line` has changed to `read line \nThis is because inside the loop, standard input is coming from `find` via the pipeline.\nIf we just did `read`, it would be consuming part or all of a file name, and some files would be skipped.\n`/dev/tty` is the terminal where the user is running the script from.  Note that this will cause an error if the script is run via cron, but I assume this is not important in this case.\nThen, what if a file name contains newlines?\nWe can handle that by changing `-print` to `-print0` and using `read -d ''` on the end of a pipeline:\nfind . -name \"*.csv\" -print0 | while IFS= read -r -d '' file; do\n```bash\necho \"file = $file\"\ndiff \"$file\" \"/some/other/path/$file\"\nread char </dev/tty\n```\ndone\n\nThis makes `find` put a null byte at the end of each file name.  Null bytes are the only characters not allowed in file names, so this should handle all possible file names, no matter how weird.\nTo get the file name on the other side, we use `IFS= read -r -d ''`.\nWhere we used `read` above, we used the default line delimiter of newline, but now, `find` is using null as the line delimiter. In `bash`, you can't pass a NUL character in an argument to a command (even builtin ones), but `bash` understands `-d ''` as meaning NUL delimited. So we use `-d ''` to make `read` use the same line delimiter as `find`. Note that `-d $'\\0'`, incidentally, works as well, because `bash` not supporting NUL bytes treats it as the empty string.\nTo be correct, we also add `-r`, which says don't handle backslashes in file names specially.  For example, without `-r`, `\\` are removed, and `\\n` is converted into `n`.\nA more portable way of writing this that doesn't require `bash` or `zsh` or remembering all the above rules about null bytes (again, thanks to Gilles):\nfind . -name '*.csv' -exec sh -c '\n  file=\"$0\"\n  echo \"$file\"\n  diff \"$file\" \"/some/other/path/$file\"\n  read char </dev/tty\n' exec-sh {} ';'\n\n*3. Skipping directories whose names end in .csv\nfind . -name \"*.csv\"\n\nwill also match directories that are called `something.csv`.\nTo avoid this, add `-type f` to the `find` command.\nfind . -type f -name '*.csv' -exec sh -c '\n  file=\"$0\"\n  echo \"$file\"\n  diff \"$file\" \"/some/other/path/$file\"\n  read line </dev/tty\n' exec-sh {} ';'\n\nAs glenn jackman points out, in both of these examples, the commands to execute for each file are being run in a subshell, so if you change any variables inside the loop, they will be forgotten.\nIf you need to set variables and have them still set at the end of the loop, you can rewrite it to use process substitution like this:\ni=0\nwhile IFS= read -r -d '' file; do\n```bash\necho \"file = $file\"\ndiff \"$file\" \"/some/other/path/$file\"\nread line </dev/tty\ni=$((i+1))\n```\ndone < <(find . -type f -name '*.csv' -print0)\necho \"$i files processed\"\n\nNote that if you try copying and pasting this at the command line, `read line` will consume the `echo \"$i files processed\"`, so that command won't get run.\nTo avoid this, you could remove `read line \n\nNOTES\nI removed the semi-colons (`;`) inside the loop.  You can put them back if you want, but they are not needed.\nThese days, `$(command)` is more common than ``command``.  This is mainly because it's easier to write `$(command1 $(command2))` than ``command1 \\`command2\\```.\n`read char` doesn't really read a character.  It reads a whole line so I changed it to `read line`.",
    "question_score": 240,
    "answer_score": 342,
    "question_id": 9496,
    "has_full_answer": true,
    "tags": [
      "bash",
      "scripting",
      "text-processing",
      "find",
      "filenames"
    ],
    "topic": "bash"
  },
  {
    "instruction": "There are stopped jobs (on bash exit)",
    "response": "A stopped job is one that has been temporarily put into the background and is no longer running, but is still using resources (i.e. system memory). Because that job is not attached to the current terminal, it cannot produce output and is not receiving input from the user.\n\nYou can see jobs you have running using the `jobs` builtin command in bash, probably other shells as well. Example:\n\nuser@mysystem:~$ jobs\n[1] + Stopped                python\nuser@mysystem:~$ \n\nYou can resume a stopped job by using the `fg` (foreground) bash built-in command. If you have multiple commands that have been stopped you must specify which one to resume by passing jobspec number on the command line with `fg`. If only one program is stopped, you may use `fg` alone:\n\nuser@mysystem:~$ fg 1\npython\n\nAt this point you are back in the python interpreter and may exit by using control-D.\n\nConversely, you may `kill` the command with either it's jobspec or PID. For instance:\n\nuser@mysystem:~$ ps\n  PID TTY          TIME CMD\n16174 pts/3    00:00:00 bash\n17781 pts/3    00:00:00 python\n18276 pts/3    00:00:00 ps\nuser@mysystem:~$ kill 17781\n[1]+  Killed                  python\nuser@mysystem:~$ \n\nTo use the jobspec, precede the number with the percent (%) key:\n\nuser@mysystem:~$ kill %1\n[1]+  Terminated              python\n\nIf you issue an exit command with stopped jobs, the warning you saw will be given. The jobs will be left running for safety. That's to make sure you are aware you are attempting to kill jobs you might have forgotten you stopped. The second time you use the exit command the jobs are terminated and the shell exits. This may cause problems for some programs that aren't intended to be killed in this fashion.\n\nIn bash it seems you can use the `logout` command which will kill stopped processes and exit. This may cause unwanted results.\n\nAlso note that some programs may not exit when terminated in this way, and your system could end up with a lot of orphaned processes using up resources if you make a habit of doing that.\n\nNote that you can create background process that will stop if they require user input:\n\nuser@mysystem:~$ python &\n[1] 19028\nuser@mysystem:~$ jobs\n[1]+  Stopped                 python\n\nYou can resume and kill these jobs in the same way you did jobs that you stopped with the `Ctrl-z` interrupt.",
    "question_score": 250,
    "answer_score": 326,
    "question_id": 116959,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "process"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Can I redirect output to a log file and background a process at the same time?",
    "response": "One problem with your first command is that you redirect stderr to where stdout is (if you changed the $ to a & as suggested in the comment) and then, you redirected stdout to some log file, but that does not pull along the redirected stderr. You must do it in the other order, first send stdout to where you want it to go, and then send stderr to the address stdout is at\n\nsome_cmd > some_file 2>&1 &\n\nand then you could throw the & on to send it to the background. Jobs can be accessed with the `jobs` command. `jobs` will show you the running jobs, and number them. You could then talk about the jobs using a % followed by the number like `kill %1` or so.  \n\nAlso, without the & on the end you can suspend the command with Ctrlz, use the `bg` command to put it in the background and `fg` to bring it back to the foreground.  In combination with the `jobs` command, this is powerful.\n\nto clarify the above part about the order you write the commands. Suppose stderr is address 1002, stdout is address 1001, and the file is 1008. The command reads left to right, so the first thing it sees in yours is `2>&1` which moves stderr to the address 1001, it then sees `> file` which moves stdout to 1008, but keeps stderr at 1001. It does not pull everything pointing at 1001 and move it to 1008, but simply references stdout and moves it to the file.\nThe other way around, it moves stdout to 1008, and then moves stderr to the point that stdout is pointing to, 1008 as well. This way both can point to the single file.",
    "question_score": 232,
    "answer_score": 307,
    "question_id": 74520,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "shell-script"
    ],
    "topic": "bash"
  },
  {
    "instruction": "What does <<< mean?",
    "response": "Others have answered the basic question: What is it? (Answer: It's a here string.)\nLet's look at why it's useful.\nYou can also feed a string to a command's stdin like this:\necho \"$string\" | command\n\nHowever in Bash, introducing a pipe means the individual commands are run in subshells. Consider this:\necho \"hello world\" | read first second\necho $second $first\n\nThe output of the 2nd echo command prints just a single space. Whaaaa? What happened to my variables? Because the read command is in a pipeline, it is run in a subshell. It correctly reads 2 words from its stdin and assigns to the variables. But then the command completes, the subshell exits and the variables are lost.\nSometimes you can work around this with braces:\necho \"hello world\" | {\n```bash\nread first second\necho $second $first\n```\n}\n\nThat's OK if your need for the values is contained, but you still don't have those variables in the current shell of your script.\nTo remedy this confusing situation, use a here string:\nread first second <<< \"hello world\"\necho $second $first\n\nAh, much better!",
    "question_score": 234,
    "answer_score": 304,
    "question_id": 80362,
    "has_full_answer": true,
    "tags": [
      "bash",
      "command-line",
      "sed",
      "command"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Security implications of forgetting to quote a variable in bash/POSIX shells",
    "response": "Preamble\nFirst, I'd say it's not the right way to address the problem.\nIt's a bit like saying \"you should not murder people because\notherwise you'll go to jail\".\nSimilarly, you don't quote your variable because otherwise\nyou're introducing security vulnerabilities. You quote your\nvariables because it is wrong not to (but if the fear of the jail can help, why not).\nA little summary for those who've just jumped on the train.\nIn most shells, leaving a variable expansion unquoted (though\nthat (and the rest of this answer) also applies to command\nsubstitution (``...`` or `$(...)`) and arithmetic expansion (`$((...))` or `$[...]`)) has a very special\nmeaning. A good way to describe it is that it is like\ninvoking some sort of implicit split+glob operator¹.\ncmd $var\n\nin another language would be written something like:\ncmd(glob(split($var)))\n\n`$var` is first split into a list of words according to complex\nrules involving the `$IFS` special parameter (the split part)\nand then each word resulting of that splitting is considered as\na pattern which is expanded to a list of files that match it\n(the glob part).\nAs an example, if `$var` contains `*.txt,/var/*.xml` and `$IFS`\ncontains `,`, `cmd` would be called with a number of arguments,\nthe first one being `cmd` and the next ones being the `txt`\nfiles in the current directory and the `xml` files in `/var`.\nIf you wanted to call `cmd` with just the two literal arguments `cmd`\nand `*.txt,/var/*.xml`, you'd write:\ncmd \"$var\"\n\nwhich would be in your other more familiar language:\ncmd($var)\n\nWhat do we mean by vulnerability in a shell?\nAfter all, it's been known since the dawn of time that shell\nscripts should not be used in security-sensitive contexts.\nSurely,  OK, leaving a variable unquoted is a bug but that can't\ndo that much harm, can it?\nWell, despite the fact that anybody would tell you that shell\nscripts should never be used for web CGIs, or that thankfully\nmost systems don't allow setuid/setgid shell scripts nowadays,\none thing that shellshock (the remotely exploitable bash bug\nthat made the headlines in September 2014) revealed is that\nshells are still extensively used where they probably shouldn't:\nin CGIs, in DHCP client hook scripts, in sudoers commands,\ninvoked by (if not as) setuid commands...\nSometimes unknowingly. For instance `system('cmd $PATH_INFO')`\nin a `php`/`perl`/`python` CGI script does invoke a shell to interpret that command line (not to\nmention the fact that `cmd` itself may be a shell script and its\nauthor may have never expected it to be called from a CGI).\nYou've got a vulnerability when there's a path for privilege\nescalation, that is when someone (let's call him the attacker)\nis able to do something they are not meant to.\nInvariably that means the attacker providing data, that data\nbeing processed by a privileged user/process which inadvertently\ndoes something it shouldn't be doing, in most of the cases because\nof a bug.\nBasically, you've got a problem when your buggy code processes\ndata under the control of the attacker.\nNow, it's not always obvious where that data may come from,\nand it's often hard to tell if your code will ever get to\nprocess untrusted data.\nAs far as variables are concerned, In the case of a CGI script,\nit's quite obvious, the data are the CGI GET/POST parameters and\nthings like cookies, path, host... parameters.\nFor a setuid script (running as one user when invoked by\nanother), it's the arguments or environment variables.\nAnother very common vector is file names. If you're getting a\nfile list from a directory, it's possible that files have been\nplanted there by the attacker.\nIn that regard, even at the prompt of an interactive shell, you\ncould be vulnerable (when processing files in `/tmp` or `~/tmp`\nfor instance).\nEven a `~/.bashrc` can be vulnerable (for instance, `bash` will\ninterpret it when invoked over `ssh` to run a `ForcedCommand`\nlike in `git` server deployments with some variables under the\ncontrol of the client).\nNow, a script may not be called directly to process untrusted\ndata, but it may be called by another command that does. Or your\nincorrect code may be copy-pasted into scripts that do (by you 3\nyears down the line or one of your colleagues). One place where it's\nparticularly critical is in answers in Q&A sites as you'll\nnever know where copies of your code may end up.\nDown to business; how bad is it?\nLeaving a variable (or command substitution) unquoted is by far\nthe number one source of security vulnerabilities associated\nwith shell code. Partly because those bugs often translate to\nvulnerabilities but also because it's so common to see unquoted\nvariables.\nActually, when looking for vulnerabilities in shell code, the\nfirst thing to do is look for unquoted variables. It's easy to\nspot, often a good candidate, generally easy to track back to\nattacker-controlled data.\nThere's an infinite number of ways an unquoted variable can turn\ninto a vulnerability. I'll just give a few common trends here.\nInformation disclosure\nMost people will bump into bugs associated with unquoted\nvariables because of the split part (for instance, it's\ncommon for files to have spaces in their names nowadays and space\nis in the default value of `$IFS`). Many people will overlook the\nglob part. The glob part is at least as dangerous as the\nsplit part.\nGlobbing done upon unsanitised external input means the\nattacker can make you read the content of any directory.\nIn:\necho You entered: $unsanitised_external_input\n\nif `$unsanitised_external_input` contains `/*`, that means the\nattacker can see the content of `/`. No big deal. It becomes\nmore interesting though with `/home/*` which gives you a list of\nuser names on the machine, `/tmp/*`,  `/home/*/.forward` for\nhints at other dangerous practises, `/etc/rc*/*` for enabled\nservices... No need to name them individually. A value of `/* /*/* /*/*/*...` will just list the whole file system.\nDenial of service vulnerabilities.\nTaking the previous case a bit too far and we've got a DoS.\nActually, any unquoted variable in list context with unsanitized\ninput is at least a DoS vulnerability.\nEven expert shell scripters commonly forget to quote things\nlike:\n#! /bin/sh -\n: ${QUERYSTRING=$1}\n\n`:` is the no-op command. What could possibly go wrong?\nThat's meant to assign `$1` to `$QUERYSTRING` if `$QUERYSTRING`\nwas unset. That's a quick way to make a CGI script callable from\nthe command line as well.\nThat `$QUERYSTRING` is still expanded though and because it's\nnot quoted, the split+glob operator is invoked.\nNow, there are some globs that are particularly expensive to\nexpand. The `/*/*/*/*` one is bad enough as it means listing\ndirectories up to 4 levels down. In addition to the disk and CPU\nactivity, that means storing tens of thousands of file paths\n(40k here on a minimal server VM, 10k of which directories).\nNow `/*/*/*/*/../../../../*/*/*/*` means 40k x 10k and\n`/*/*/*/*/../../../../*/*/*/*/../../../../*/*/*/*` is enough to\nbring even the mightiest machine to its knees.\nTry it for yourself (though be prepared for your machine to\ncrash or hang):\na='/*/*/*/*/../../../../*/*/*/*/../../../../*/*/*/*' sh -c ': ${a=foo}'\n\nOf course, if the code is:\necho $QUERYSTRING > /some/file\n\nThen you can fill up the disk.\nJust do a google search on shell\ncgi or bash\ncgi or ksh\ncgi, and you'll find\na few pages that show you how to write CGIs in shells. Notice\nhow half of those that process parameters are vulnerable.\nEven David Korn's\nown\none\nis vulnerable (look at the cookie handling).\nup to arbitrary code execution vulnerabilities\nArbitrary code execution is the worst type of vulnerability,\nsince if the attacker can run any command, there's no limit on\nwhat they may do.\nThat's generally the split part that leads to those. That\nsplitting results in several arguments to be passed to commands\nwhen only one is expected. While the first of those will be used\nin the expected context, the others will be in a different context\nso potentially interpreted differently. Better with an example:\nawk -v foo=$external_input '$2 == foo'\n\nHere, the intention was to assign the content of the\n`$external_input` shell variable to the `foo` `awk` variable.\nNow:\n$ external_input='x BEGIN{system(\"uname\")}'\n$ awk -v foo=$external_input '$2 == foo'\nLinux\n\nThe second word resulting of the splitting of `$external_input`\nis not assigned to `foo` but considered as `awk` code (here that\nexecutes an arbitrary command: `uname`).\nThat's especially a problem for commands that can execute other\ncommands (`awk`, `env`, `sed` (GNU one), `perl`, `find`...) especially\nwith the GNU variants (which accept options after arguments).\nSometimes, you wouldn't suspect commands to be able to execute\nothers like `ksh`, `bash` or `zsh`'s `[` or `printf`...\nfor file in *; do\n  [ -f $file ] || continue\n  something-that-would-be-dangerous-if-$file-were-a-directory\ndone\n\nIf we create a directory called `x -o yes`, then the test\nbecomes positive, because it's a completely different\nconditional expression we're evaluating.\nWorse, if we create a file called `x -a a[0$(uname>&2)] -gt 1`,\nwith all ksh implementations at least (which includes the `sh`\nof most commercial Unices and some BSDs), that executes `uname`\nbecause those shells perform arithmetic evaluation on the\nnumerical comparison operators of the `[` command.\n$ touch x 'x -a a[0$(uname>&2)] -gt 1'\n$ ksh -c 'for f in *; do [ -f $f ]; done'\nLinux\n\nSame with `bash` for a filename like `x -a -v a[0$(uname>&2)]`.\nOf course, if they can't get arbitrary execution, the attacker may\nsettle for lesser damage (which may help to get arbitrary\nexecution). Any command that can write files or change\npermissions, ownership or have any main or side effect could be exploited.\nAll sorts of things can be done with file names.\n$ touch -- '-R ..'\n$ for file in *; do [ -f \"$file\" ] && chmod +w $file; done\n\nAnd you end up making `..` writeable (recursively with GNU\n`chmod`).\nScripts doing automatic processing of files in publicly writable areas like `/tmp` are to be written very carefully.\nWhat about `[ $# -gt 1 ]`\nThat's something I find exasperating. Some people go down all\nthe trouble of wondering whether a particular expansion may be\nproblematic to decide if they can omit the quotes.\nIt's like saying. Hey, it looks like `$#` cannot be subject to\nthe split+glob operator, let's ask the shell to split+glob it.\nOr Hey, let's write incorrect code just because the bug is\nunlikely to be hit.\nNow how unlikely is it? OK, `$#` (or `$!`, `$?` or any\narithmetic substitution) may only contain digits (or `-` for\nsome²) so the glob part is out. For the split part to do\nsomething though, all we need is for `$IFS` to contain digits (or `-`).\nWith some shells, `$IFS` may be inherited from the environment,\nbut if the environment is not safe, it's game over anyway.\nNow if you write a function like:\nmy_function() {\n  [ $# -eq 2 ] || return\n  ...\n}\n\nWhat that means is that the behaviour of your function depends\non the context in which it is called. Or in other words, `$IFS`\nbecomes one of the inputs to it. Strictly speaking, when you\nwrite the API documentation for your function, it should be\nsomething like:\n# my_function\n#   inputs:\n#     $1: source directory\n#     $2: destination directory\n#   $IFS: used to split $#, expected not to contain digits...\n\nAnd code calling your function needs to make sure `$IFS` doesn't\ncontain digits. All that because you didn't feel like typing\nthose 2 double-quote characters.\nNow, for that `[ $# -eq 2 ]` bug to become a vulnerability,\nyou'd need somehow for the value of `$IFS` to become under\ncontrol of the attacker. Conceivably, that would not normally\nhappen unless the attacker managed to exploit another bug.\nThat's not unheard of though. A common case is when people\nforget to sanitize data before using it in arithmetic\nexpression. We've already seen above that it can allow\narbitrary code execution in some shells, but in all of them, it allows\nthe attacker to give any variable an integer value.\nFor instance:\nn=$(($1 + 1))\nif [ $# -gt 2 ]; then\n  echo >&2 \"Too many arguments\"\n  exit 1\nfi\n\nAnd with a `$1` with value `(IFS=-1234567890)`, that arithmetic\nevaluation has the side effect of settings `$IFS` and the next `[`\ncommand fails which means the check for too many args is\nbypassed.\nWhat about when the split+glob operator is not invoked?\nThere's another case where quotes are needed around variables and other expansions: when it's used as a pattern.\n[[ $a = $b ]]   # a `ksh` construct also supported by `bash`\ncase $a in ($b) ...; esac\n\ndo not test whether `$a` and `$b` are the same (except with `zsh`) but if `$a` matches the pattern in `$b`. And you need to quote `$b` if you want to compare as strings (same thing in `\"${a#$b}\"` or `\"${a%$b}\"` or `\"${a##*$b*}\"` where `$b` should be quoted if it's not to be taken as a pattern).\nWhat that means is that `[[ $a = $b ]]` may return true in cases where `$a` is different from `$b` (for instance when `$a` is `anything` and `$b` is `*`) or may return false when they are identical (for instance when both `$a` and `$b` are `[a]`).\nCan that make for a security vulnerability? Yes, like any bug. Here, the attacker can alter your script's logical code flow and/or break the assumptions that your script are making. For instance, with a code like:\nif [[ $1 = $2 ]]; then\n   echo >&2 '$1 and $2 cannot be the same or damage will incur'\n   exit 1\nfi\n\nThe attacker can bypass the check by passing `'[a]' '[a]'`.\nNow, if neither that pattern matching nor the split+glob operator apply, what's the danger of leaving a variable unquoted?\nI have to admit that I do write:\na=$b\ncase $a in...\n\nThere, quoting doesn't harm but is not strictly necessary.\nHowever, one side effect of omitting quotes in those cases (for instance in Q&A answers) is that it can send a wrong message to beginners: that it may be all right not to quote variables.\nFor instance, they may start thinking that if `a=$b` is OK, then `export a=$b` would be as well (which it's not in many shells as it's in arguments to the `export` command so in list context) or `env a=$b`.\nThere are a few places though where quotes are not accepted. The main one being inside Korn-style arithmetic expressions in many shells like in `echo \"$(( $1 + 1 ))\" \"${array[$1 + 1]}\" \"${var:$1 + 1}\"` where the `$1` must not be quoted (being in a list context --the arguments to a simple command-- the overall expansions still needs to be quoted though).\nInside those, the shell understands a separate language altogether inspired from C. In AT&T `ksh` for instance `$(( 'd' - 'a' ))` expands to 3 like it does in C and not the same as `$(( d - a ))` would. Double quotes are ignored in ksh93 but cause a syntax error in many other shells. In C, `\"d\" - \"a\"` would return the difference between pointers to C strings. Doing the same in shell would not make sense.\nWhat about `zsh`?\n`zsh` did fix most of those design awkwardnesses. In `zsh` (at least when not in sh/ksh emulation mode), if you want splitting, or globbing, or pattern matching, you have to request it explicitly: `$=var` to split, and `$~var` to glob or for the content of the variable to be treated as a pattern.\nHowever, splitting (but not globbing) is still done implicitly upon unquoted command substitution (as in `echo $(cmd)`).\nAlso, a sometimes unwanted side effect of not quoting variable is the empties removal. The `zsh` behaviour is similar to what you can achieve in other shells by disabling globbing altogether (with `set -f`) and splitting (with `IFS=''`). Still, in:\ncmd $var\n\nThere will be no split+glob, but if `$var` is empty, instead of receiving one empty argument, `cmd` will receive no argument at all.\nThat can cause bugs (like the obvious `[ -n $var ]`). That can possibly break a script's expectations and assumptions and cause vulnerabilities.\nAs the empty variable can cause an argument to be just removed, that means the  next argument could be interpreted in the wrong context.\nAs an example,\nprintf '[%d] \\n' 1 $attacker_supplied1 2 $attacker_supplied2\n\nIf `$attacker_supplied1` is empty,  then `$attacker_supplied2` will be interpreted as an arithmetic expression (for `%d`) instead of a string (for `%s`) and any unsanitized data used in an arithmetic expression is a command injection vulnerability in Korn-like shells such as zsh.\n$ attacker_supplied1='x y' attacker_supplied2='*'\n$ printf '[%d] \\n' 1 $attacker_supplied1 2 $attacker_supplied2\n[1] \n[2] \n\nfine, but:\n$ attacker_supplied1='' attacker_supplied2='psvar[$(uname>&2)0]'\n$ printf '[%d] \\n' 1 $attacker_supplied1 2 $attacker_supplied2\nLinux\n[1] \n[0] <>\n\nThe `uname` arbitrary command was run.\nAlso note that while `zsh` doesn't do globbing upon substitutions by default, as globs in zsh are much more powerful than in other shells, that means they can do a lot more damage if ever you enabled the `globsubst` option at the same time of the `extendedglob` one, or without disabling `bareglobqual` and left some variables unintentionally unquoted.\nFor instance, even:\nset -o globsubst\necho $attacker_controlled\n\nWould be an arbitrary command execution vulnerability, because commands can be executed as part of glob expansions, for instance with the `e`valuation glob qualifier:\n$ set -o globsubst\n$ attacker_controlled='.(e[uname])'\n$ echo $attacker_controlled\nLinux\n.\n\nemulate sh # or ksh\necho $attacker_controlled\n\ndoesn't cause an ACE vulnerability (though it still a DoS one like in sh) because `bareglobqual` is disabled in sh/ksh emulation. There's no good reason to enable `globsubst` other than in those sh/ksh emulations when wanting to interpret sh/ksh code.\nWhat about when you do need the split+glob operator?\nYes, that's typically when you do want to leave your variable unquoted. But then you need to make sure you tune your split and glob operators correctly before using it. If you only want the split part and not the glob part (which is the case most of the time), then you do need to disable globbing (`set -o noglob`/`set -f`) and fix `$IFS`. Otherwise you'll cause vulnerabilities as well (like David Korn's CGI example mentioned above).\nConclusion\nIn short, leaving a variable (or command substitution or\narithmetic expansion) unquoted in shells can be very dangerous\nindeed especially when done in the wrong contexts, and it's very\nhard to know which are those wrong contexts.\nThat's one of the reasons why it is considered bad practice.\nThanks for reading so far. If it goes over your head, don't\nworry. One can't expect everyone to understand all the implications of\nwriting their code the way they write it. That's why we have\ngood practice recommendations, so they can be followed without\nnecessarily understanding why.\n(and in case that's not obvious yet, please avoid writing\nsecurity sensitive code in shells).\nAnd please quote your variables on your answers on this site!\n\n¹In `ksh93` and `pdksh` and derivatives, brace expansion is also performed unless globbing is disabled (in the case of `ksh93` versions up to ksh93u+, even when the `braceexpand` option is disabled).\n² In `ksh93` and `yash`, arithmetic expansions can also include things like `1,2`, `1e+66`, `inf`, `nan`. There are even more in `zsh`, including `#` which is a glob operator with `extendedglob`, but `zsh` never does split+glob upon arithmetic expansion, even in `sh` emulation",
    "question_score": 280,
    "answer_score": 299,
    "question_id": 171346,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "shell-script",
      "security",
      "quoting"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Terminal prompt not wrapping correctly",
    "response": "Non-printable sequences should be enclosed in `\\[` and `\\]`. Looking at your PS1 it has a unenclosed sequence after `\\W`. But, the second entry is redundant as well as it repeats the previous statement \"1;34\".\n\\[\\033[01;32m\\]\\u:\\[\\033[01;34m\\] \\W\\033[01;34m \\$\\[\\033[00m\\]\n                  |_____________|               |_|\n                         |                       |\n                         +--- Let this apply to this as well.\n\nAs such this should have intended coloring:\n\\[\\033[1;32m\\]\\u:\\[\\033[1;34m\\] \\W \\$\\[\\033[0m\\]\n                               |_____|\n                                  |\n                                  +---- Bold blue.\n\nKeeping the \"original\" this should also work:\n\\[\\033[1;32m\\]\\u:\\[\\033[1;34m\\] \\W\\[\\033[1;34m\\] \\$\\[\\033[0m\\]\n                                  |_|         |_|\n                                   |           |\n                                   +-----------+-- Enclose in \\[ \\]\n\nEdit:\nThe reason for the behavior is because `bash` believes the prompt is longer then it actually is. As a simple example, if one use:\nPS1=\"\\033[0;34m$\"\n       1 2345678\n\nThe prompt is believed to be 8 characters and not 1. As such if terminal window is 20 columns, after typing 12 characters, it is believed to be 20 and wraps around. This is also evident if one then try to do backspace or Ctrl+u. It stops at column 9.\nHowever it also does not start new line unless one are on last column, as a result the first line is overwritten.\nIf one keep typing the line should wrap to next line after 32 characters.",
    "question_score": 251,
    "answer_score": 287,
    "question_id": 105958,
    "has_full_answer": true,
    "tags": [
      "bash",
      "terminal",
      "prompt"
    ],
    "topic": "bash"
  },
  {
    "instruction": "remove particular characters from a variable using bash",
    "response": "There is no need to execute an external program. `bash`'s string manipulation can handle it (also available in `ksh93` (where it comes from), `zsh` and recent versions of `mksh`, `yash` and busybox `sh` (at least)):\n\n$ VERSION='2.3.3'\n$ echo \"${VERSION//.}\"\n233\n\n(In those shells' manuals you can generally find this in the parameter expansion section.)",
    "question_score": 239,
    "answer_score": 265,
    "question_id": 104881,
    "has_full_answer": true,
    "tags": [
      "bash",
      "text-processing"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Are there naming conventions for variables in shell scripts?",
    "response": "Environment variables or shell variables introduced by the operating system, shell startup scripts, or the shell itself, etc., are usually all in `CAPITALS`1.\nTo prevent your variables from conflicting with these variables, it is a good practice to use `lower_case` variable names.\n\n1A notable exception that may be worth knowing about is the `path` array, used by the `zsh` shell.  This is the same as the common `PATH` variable but represented as an array.",
    "question_score": 245,
    "answer_score": 240,
    "question_id": 42847,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "shell",
      "variable"
    ],
    "topic": "bash"
  },
  {
    "instruction": "What does env x='() { :;}; command' bash do and why is it insecure?",
    "response": "bash stores exported function definitions as environment variables. Exported functions look like this:\n\n$ foo() { bar; }\n$ export -f foo\n$ env | grep -A1 foo\nfoo=() {  bar\n}\n\nThat is, the environment variable `foo` has the literal contents:\n\n() {  bar\n}\n\nWhen a new instance of bash launches, it looks for these specially crafted environment variables, and interprets them as function definitions. You can even write one yourself, and see that it still works:\n\n$ export foo='() { echo \"Inside function\"; }'\n$ bash -c 'foo'\nInside function\n\nUnfortunately, the parsing of function definitions from strings (the environment variables) can have wider effects than intended. In unpatched versions, it also interprets arbitrary commands that occur after the termination of the function definition. This is due to insufficient constraints in the determination of acceptable function-like strings in the environment. For example:\n\n$ export foo='() { echo \"Inside function\" ; }; echo \"Executed echo\"'\n$ bash -c 'foo'\nExecuted echo\nInside function\n\nNote that the echo outside the function definition has been unexpectedly executed during bash startup. The function definition is just a step to get the evaluation and exploit to happen, the function definition itself and the environment variable used are arbitrary. The shell looks at the environment variables, sees `foo`, which looks like it meets the constraints it knows about what a function definition looks like, and it evaluates the line, unintentionally also executing the echo (which could be any command, malicious or not).\n\nThis is considered insecure because variables are not typically allowed or expected, by themselves, to directly cause the invocation of arbitrary code contained in them. Perhaps your program sets environment variables from untrusted user input. It would be highly unexpected that those environment variables could be manipulated in such a way that the user could run arbitrary commands without your explicit intent to do so using that environment variable for such a reason declared in the code.\n\nHere is an example of a viable attack. You run a web server that runs a vulnerable shell, somewhere, as part of its lifetime. This web server passes environment variables to a bash script, for example, if you are using CGI, information about the HTTP request is often included as environment variables from the web server. For example, `HTTP_USER_AGENT` might be set to the contents of your user agent. This means that if you spoof your user agent to be something like '() { :; }; echo foo', when that shell script runs, `echo foo` will be executed. Again, `echo foo` could be anything, malicious or not.",
    "question_score": 250,
    "answer_score": 214,
    "question_id": 157329,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shellshock",
      "vulnerability"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Combined `mkdir` and `cd`?",
    "response": "Function?\nmkcdir ()\n{\n```bash\nmkdir -p -- \"$1\" &&\n```\n       cd -P -- \"$1\"\n}\n\nPut the above code in the `~/.bashrc`, `~/.zshrc` or another file sourced by your shell. Then source it by running e.g. `source ~/.bashrc` to apply changes.\nAfter that simply run `mkcdir foo` or `mkcdir \"nested/path/in quotes\"`.\nNotes:\n\n`\"$1\"` is the first argument of the `mkcdir` command. Quotes around it protects the argument if it has spaces or other special characters.\n`--` makes sure the passed name for the new directory is not interpreted as an option to `mkdir` or `cd`, giving the opportunity to create a directory that starts with `-` or `--`.\n`-p` used on `mkdir` makes it create extra directories if they do not exist yet, and `-P` used makes `cd` resolve symbolic links.\nInstead of `source`-ing the rc, you may also restart the terminal emulator/shell.",
    "question_score": 259,
    "answer_score": 195,
    "question_id": 125385,
    "has_full_answer": true,
    "tags": [
      "bash",
      "cd-command",
      "mkdir"
    ],
    "topic": "bash"
  },
  {
    "instruction": "Is there any way to execute commands from history?",
    "response": "In bash, just `!636` will be ok.",
    "question_score": 215,
    "answer_score": 366,
    "question_id": 275053,
    "has_full_answer": true,
    "tags": [
      "bash",
      "command-history"
    ],
    "topic": "bash"
  },
  {
    "instruction": "How do I grep for multiple patterns with pattern having a pipe character?",
    "response": "First, you need to protect the pattern from expansion by the shell. The easiest way to do that is to put single quotes around it. Single quotes prevent expansion of anything between them (including backslashes); the only thing you can't do then is have single quotes in the pattern.\n\ngrep -- 'foo*' *.txt\n\n(also note the `--` end-of-option-marker to stop some `grep` implementations including GNU `grep` from treating a file called `-foo-.txt` for instance (that would be expanded by the shell from `*.txt`) to be taken as an option (even though it follows a non-option argument here)).\n\nIf you do need a single quote, you can write it as `'\\''` (end string literal, literal quote, open string literal).\n\ngrep -- 'foo*'\\''bar' *.txt\n\nSecond, grep supports at least¹ two syntaxes for patterns. The old, default syntax (basic regular expressions) doesn't support the alternation (`|`) operator, though some versions have it as an extension, but written with a backslash.\n\ngrep -- 'foo\\|bar' *.txt\n\nThe portable way is to use the newer syntax, extended regular expressions. You need to pass the `-E` option to `grep` to select it (formerly that was done with the `egrep` separate command²)\n\ngrep -E -- 'foo|bar' *.txt\n\nAnother possibility when you're just looking for any of several patterns (as opposed to building a complex pattern using disjunction) is to pass multiple patterns to `grep`. You can do this by preceding each pattern with the `-e` option.\n\ngrep -e foo -e bar -- *.txt\n\nOr put patterns on several lines:\n\ngrep -- 'foo\nbar' *.txt\n\nOr store those patterns in a file, one per line and run\n\ngrep -f that-file -- *.txt\n\nNote that if `*.txt` expands to a single file, `grep` won't prefix matching lines with its name like it does when there are more than one file. To work around that, with some `grep` implementations like GNU `grep`, you can use the `-H` option, or with any implementation, you can pass `/dev/null` as an extra argument.\n\n¹ some `grep` implementations support even more like perl-compatible ones with `-P`, or augmented ones with `-X`, `-K` for ksh wildcards...\n\n² while `egrep` has been deprecated by POSIX and is sometimes no longer found on some systems, on some other systems like Solaris when the POSIX or GNU utilities have not been installed, then `egrep` is your only option as its `/bin/grep` supports none of `-e`, `-f`, `-E`, `\\|` or multi-line patterns",
    "question_score": 810,
    "answer_score": 1086,
    "question_id": 37313,
    "has_full_answer": true,
    "tags": [
      "shell",
      "grep",
      "regular-expression",
      "quoting"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Can grep output only specified groupings that match?",
    "response": "GNU grep has the `-P` option for perl-style regexes, and the `-o` option to print only what matches the pattern. These can be combined using look-around assertions (described under Extended Patterns in the perlre manpage) to remove part of the grep pattern from what is determined to have matched for the purposes of `-o`.\n\n$ grep -oP 'foobar \\K\\w+' test.txt\nbash\nhappy\n$\n\nThe `\\K` is the short-form (and more efficient form) of `(?\n\nFor instance, if you wanted to match the word between `foo` and `bar`, you could use:\n\n$ grep -oP 'foo \\K\\w+(?= bar)' test.txt\n\nor (for symmetry)\n\n$ grep -oP '(?<=foo )\\w+(?= bar)' test.txt",
    "question_score": 592,
    "answer_score": 652,
    "question_id": 13466,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "grep",
      "regular-expression"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Count total number of occurrences using grep",
    "response": "grep's `-o` will only output the matches, ignoring lines; `wc` can count them:\n\ngrep -o 'needle' file | wc -l\n\nThis will also match 'needles' or 'multineedle'.\n\nTo match only single words use one of the following commands:\n\ngrep -ow 'needle' file | wc -l\ngrep -o '\\bneedle\\b' file | wc -l\ngrep -o '\\' file | wc -l",
    "question_score": 434,
    "answer_score": 623,
    "question_id": 6979,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How can I prevent 'grep' from showing up in ps results?",
    "response": "Turns out there's a solution found in keychain.\n\n$ ps aux | grep \"[f]nord\"\n\nBy putting the brackets around the letter and quotes around the string you search for the regex, which says, \"Find the character 'f' followed by 'nord'.\"\n\nBut since you put the brackets in the pattern 'f' is now followed by ']', so `grep` won't show up in the results list. Neato!",
    "question_score": 421,
    "answer_score": 609,
    "question_id": 74185,
    "has_full_answer": true,
    "tags": [
      "grep",
      "ps"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Can grep return true/false or are there alternative methods",
    "response": "`grep` returns a different exit code if it found something (zero) vs. if it hasn't found anything (non-zero). In an `if` statement, a zero exit code is mapped to \"true\" and a non-zero exit code is mapped to false. In addition, grep has a `-q` argument to not output the matched text (but only return the exit status code)\n\nSo, you can use grep like this:\n\nif grep -q PATTERN file.txt; then\n```bash\necho found\n```\nelse\n```bash\necho not found\n```\nfi\n\nAs a quick note, when you do something like `if [ -z \"$var\" ]…`, it turns out that `[` is actually a command you're running, just like grep. On my system, it's `/usr/bin/[`. (Well, technically, your shell probably has it built-in, but that's an optimization. It behaves as if it were a command). It works the same way, `[` returns a zero exit code for true, a non-zero exit code for false. (`test` is the same thing as `[`, except for the closing `]`)",
    "question_score": 266,
    "answer_score": 385,
    "question_id": 48535,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "What is the difference between `grep`, `egrep`, and `fgrep`?",
    "response": "`egrep` is 100% equivalent to `grep -E`\n`fgrep` is 100% equivalent to `grep -F`\n\nHistorically these switches were provided in separate binaries. On some really old Unix systems you will find that you need to call the separate binaries, but on all modern systems the switches are preferred. The man page for grep has details about this. \n\nAs for what they do, `-E` switches grep into a special mode so that the expression is evaluated as an ERE (Extended Regular Expression) as opposed to its normal pattern matching. Details of this syntax are on the man page.\n\n  `-E, --extended-regexp`\n     Interpret PATTERN as an extended regular expression\n\nThe `-F` switch switches grep into a different mode where it accepts a pattern to match, but then splits that pattern up into one search string per line and does an OR search on any of the strings without doing any special pattern matching.\n\n  `-F, --fixed-strings`\n     Interpret  PATTERN  as  a  list of fixed strings, separated by newlines, any of which is to be matched.\n\nHere are some example scenarios:\n\nYou have a file with a list of say ten Unix usernames in plain text. You want to search the group file on your machine to see if any of the ten users listed are in any special groups:\n\ngrep -F -f user_list.txt /etc/group\n\nThe reason the `-F` switch helps here is that the usernames in your pattern file are interpreted as plain text strings. Dots for example would be interpreted as dots rather than wild-cards.\nYou want to search using a fancy expression. For example parenthesis `()` can be used to indicate groups with `|` used as an OR operator. You could run this search using `-E`:\n\ngrep -E '^no(fork|group)' /etc/group\n\n...to return lines that start with either \"nofork\" or \"nogroup\". Without the `-E` switch you would have to escape the special characters involved because with normal pattern matching they would just search for that exact pattern;\n\ngrep '^no\\(fork\\|group\\)' /etc/group",
    "question_score": 317,
    "answer_score": 341,
    "question_id": 17949,
    "has_full_answer": true,
    "tags": [
      "grep",
      "regular-expression"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How can I grep in PDF files?",
    "response": "Install the package `pdfgrep`, then use the command:\n\nfind /path -iname '*.pdf' -exec pdfgrep pattern {} +\n\n——————\n\nSimplest way to do that:\n\npdfgrep 'pattern' *.pdf\npdfgrep 'pattern' file.pdf",
    "question_score": 289,
    "answer_score": 322,
    "question_id": 6704,
    "has_full_answer": true,
    "tags": [
      "grep",
      "search",
      "pdf"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Return only the portion of a line after a matching pattern",
    "response": "The canonical tool for that would be `sed`.\nsed -n -e 's/^.*stalled: //p'\n\nDetailed explanation:\n\n`-n` means not to print anything by default.\n`-e` is followed by a sed command.\n`s` is the pattern replacement command.\nThe regular expression `^.*stalled: ` matches the pattern you're looking for, plus any preceding text (`.*` meaning any text, with an initial `^` to say that the match begins at the beginning of the line). Note that if `stalled: ` occurs several times on the line, this will match the last occurrence.\nThe match, i.e. everything on the line up to `stalled: `, is replaced by the empty string (i.e. deleted).\nThe final `p` means to print the transformed line.\n\nIf you want to retain the matching portion, use a backreference: `\\1` in the replacement part designates what is inside a group `\\(…\\)` in the pattern. Here, you could write `stalled: ` again in the replacement part; this feature is useful when the pattern you're looking for is more general than a simple string.\nsed -n -e 's/^.*\\(stalled: \\)/\\1/p'\n\nSometimes you'll want to remove the portion of the line after the match. You can include it in the match by including `.*$` at the end of the pattern (any text `.*` followed by the end of the line `$`). Unless you put that part in a group that you reference in the replacement text, the end of the line will not be in the output.\nAs a further illustration of groups and backreferences, this command swaps the part before the match and the part after the match.\nsed -n -e 's/^\\(.*\\)\\(stalled: \\)\\(.*\\)$/\\3\\2\\1/p'\n\nTo get the part after the first occurrence of the string instead of last (for those lines where the string can occur several times), a common trick is to replace that string once with a newline character (which is the one character that won't occur inside a line), and then remove everything up to that newline:\nsed -n '\n  /stalled: / {\n```bash\ns//\\\n```\n/\n```bash\ns/.*\\n//p\n```\n  }'\n\nWith some `sed` implementations, the first `s` command can be written `s//\\n/` though that's not standard/portable.",
    "question_score": 213,
    "answer_score": 270,
    "question_id": 24140,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "sed",
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Convince grep to output all lines, not just those with matches",
    "response": "grep --color -E \"test|$\" yourfile\n\nWhat we're doing here is matching against the `$` pattern and the test pattern, obviously `$` doesn't have anything to colourize so only the test pattern gets color.  The `-E` just turns on extended regex matching.\n\nYou can create a function out of it easily like this:\n\nhighlight () { grep --color -E \"$1|$\" \"${@:1}\" ; }",
    "question_score": 170,
    "answer_score": 245,
    "question_id": 366,
    "has_full_answer": true,
    "tags": [
      "grep",
      "highlighting"
    ],
    "topic": "grep"
  },
  {
    "instruction": "grep on a variable",
    "response": "Have `grep` read on its standard input. There you go, using a pipe...\n\n$ echo \"$line\" | grep select\n\n... or a here string...\n\n$ grep select <<< \"$line\"\n\nAlso, you might want to replace spaces by newlines before grepping :\n\n$ echo \"$line\" | tr ' ' '\\n' | grep select\n\n... or you could ask `grep` to print the match only:\n\n$ echo \"$line\" | grep -o select\n\nThis will allow you to get rid of the rest of the line when there's a match.\n\nEdit: Oops, read a little too fast, thanks Marco. In order to count the occurences, just pipe any of these to `wc(1)` ;)\n\nAnother edit made after lzkata's comment, quoting `$line` when using `echo`.",
    "question_score": 154,
    "answer_score": 207,
    "question_id": 163810,
    "has_full_answer": true,
    "tags": [
      "shell",
      "grep",
      "string"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How can I count the number of lines of a file with common tools?",
    "response": "If you have already collected the `grep` output in a file, you could output a numbered list with:\ncat -n myfile\n\nIf you only want the number of lines, simply do:\nwc -l myfile\n\nThere is absolutely no reason to do:\ncat myfile | wc -l\n\n...as this needlessly does I/O (the `cat`) that `wc` has to repeat.  Besides, you have two processes where one suffices.\nIf you want to `grep` to your terminal and print a count of the matches at the end, you can do:\ngrep whatever myfile | tee /dev/tty | wc -l\n\nNote: `/dev/tty` is the controlling terminal. From the `tty(4)` man page:\n\nThe file `/dev/tty` is a character file with major number 5 and minor number 0, usually of mode 0666 and `owner.group` `root.tty`. It is a synonym for the controlling terminal of a process, if any.\nIn addition to the `ioctl(2)` requests supported by the device that tty refers to, the `ioctl(2)` request `TIOCNOTTY` is supported.",
    "question_score": 167,
    "answer_score": 206,
    "question_id": 25344,
    "has_full_answer": true,
    "tags": [
      "files",
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "What makes grep consider a file to be binary?",
    "response": "If there is a `NUL` character anywhere in the file, grep will consider it as a binary file.\n\nThere might a workaround like this `cat file | tr -d '\\000' | yourgrep` to eliminate all null first, and then to search through file.",
    "question_score": 244,
    "answer_score": 175,
    "question_id": 19907,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How to run grep with multiple AND patterns?",
    "response": "To find the lines that match each and everyone of a list of patterns, `agrep` (the original one, now shipped with glimpse, not the unrelated one in the TRE regexp library) can do it with this syntax:\nagrep 'pattern1;pattern2'\n\nWith GNU `grep`, when built with PCRE support, you can use several lookahead assertions:\ngrep -P '^(?=.*pattern1)(?=.*pattern2)'\n\nWith ast `grep`:\ngrep -X '.*pattern1.*&.*pattern2.*'\n\n(adding `.*`s as `&` matches strings that match both `` and `` exactly, `a&b` would never match as there's no such string that can be both `a` and `b` at the same time).\nIf the patterns don't overlap, you may also be able to do:\ngrep -e 'pattern1.*pattern2' -e 'pattern2.*pattern1'\n\nThe best portable way is probably with `awk` as already mentioned:\nawk '/pattern1/ && /pattern2/'\n\nOr with `sed`:\nsed -e '/pattern1/!d' -e '/pattern2/!d'\n\nOr `perl`:\nperl -ne 'print if /pattern1/ && /pattern2/'\n\nPlease beware that all those will have different regular expression syntaxes.\nThe `awk`/`sed`/`perl` ones don't reflect whether any line matched the patterns in their exit status. To so that you need:\nawk '/pattern1/ && /pattern2/ {print; found = 1}\n     END {exit !found}'\n\nperl -ne 'if (/pattern1/ && /pattern2/) {print; $found = 1}\n          END {exit !$found}'\n\nOr pipe the command to `grep '^'`.\nFor potentially gzip-compressed files, you can use `zgrep` which is generally a shell script wrapper around `grep`, and use one of the `grep` solutions above (not the ast-open one as that `grep` implementation cannot be use by `zgrep`) or you could use the `PerlIO::gzip` module of `perl` which can transparently uncompress files upon input:\nperl -MPerlIO::gzip -Mopen='IN,gzip(autopop)' -ne '\n  print \"$ARGV:$_\" if /pattern1/ && /pattern2/' -- *.gz\n\n(which if the files are small enough at least is even going to be more efficient than `zgrep` as the decompression is done internally without having to run `gunzip` for each file).",
    "question_score": 181,
    "answer_score": 152,
    "question_id": 55359,
    "has_full_answer": true,
    "tags": [
      "grep",
      "regular-expression"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Grep: how to add an \"OR\" condition?",
    "response": "I'm also fairly new to regex, but since noone else has answered I'll give it a shot.\nThe pipe-operator \"|\" is used for an OR operator.\n\nThe following REGEX should get you going somewhere.\n\n.+((JPG)$|(JPEG)$)\n\n(Match anything one or more times followed by either \"JPG\" or \"JPEG\" at the end)\n\nExtended answer (editted after learning a bit about (e)grep):\nAssuming you have a folder with following files in them:\n\ntest.jpeg, test.JpEg, test.JPEG, test.jpg, test.JPG, test.notimagefile, test.gif\n\n(Not that creative with names...)\n\nFirst we start by defining what we know about our pattern:\n   We know that we are looking for the end of the name. Ergo we use the \"$\" operand to define that each line has to end with the defined pattern.\n   We know that the pattern needs to be either JPEG or JPG. To this we use the pipeline \"|\" as an or operand.\n   Our pattern is now:\n\n((JPEG)|(JPG))$\n\n(Match any line ending with EITHER \"JPEG\" or \"JPG\")\n\nHowever we see that in this example, the only difference is the optional \"E\". To this we can use the \"?\" operand (meaning optional).\n   We write:\n\n(JP(E)?G)$\n\n(Mach any file ending with a pattern like: \"J\", followed by \"P\", followed by an optional \"E\", followed by a \"G\").\n\nHowever we might also like to match files with lowercase letters in file name. To this we introduce the character-class \"[...]\". meaning match either of the following.\n   We write:\n\n([jJ][pP]([eE])?[gG])$\n\n(Match any file ending with at pattern like: \"j\" or \"J\", followed by \"p\" or \"P\", followed by an optional \"e\" or \"E\", followed by \"g\" or \"G\")\n(This could also be done using the \"-i\" option in grep, but I took this as an exercise in REGEX)\n\nFinally, since we (hopefully) start to see a pattern, we can omit the unnecessary parentheses. Since there is only one optional letter (\"E\"), we can omit this one. Also, since the file only has this pattern to end on, we can omit the starting and ending parenthesis. Thus we simply get:\n\n[jJ][pP][eE]?[gG]$\n\nFinally; lets assume you also want to find files with \".gif\"-filetype, we can add this as a second parameter:\n\n ([jJ][pP][eE]?[gG])|([gG][iI][fF])$\n\n(Here I've again added extra parenthesis for readability/grouping. Feel free to remove them if they seem obfuscating.)\n\nFinally, I used ls and a pipeline to send all file names to (e)grep:\n\nls | egrep '([jJ][pP][eE]?[gG])|([gG][iI][fF])$' \n\nResult:\n\ntest.gif\ntest.JPG\ntest.JpEg\ntest.JPEG\ntest.jpg\ntest.JPG\n\nSecond edit:\n   Using the -i option and omitting parenthesis we can shorten it down to only:\n\nls | egrep -i 'jpe?g|gif$'",
    "question_score": 198,
    "answer_score": 49,
    "question_id": 25821,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "grep returns \"Binary file (standard input) matches\" when trying to find a string pattern in file",
    "response": "Presumably the file `.bash_history` starts with non-text data, hence `grep` is treating the file as binary. This is confirmed by the `file .bash_history` output:\n\n.bash_history: data \n\nYou can read a few bytes from start to have a conforming view:\n\nhead -c1K .bash_history \n\nHere I am reading first 1 KiB.\n\nYou can pipe the STDOUT to `hexdump`/`od` or similar.\n\nAs a side note, `grep` takes filename(s) as argument, so `cat` is useless here; try this:\n\ngrep git .bash_history",
    "question_score": 218,
    "answer_score": 29,
    "question_id": 335716,
    "has_full_answer": true,
    "tags": [
      "files",
      "grep",
      "binary"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Match exact string using grep",
    "response": "Try one of:\n\ngrep -w \"deiauk\" textfile\n\ngrep \"\\\" textfile",
    "question_score": 136,
    "answer_score": 234,
    "question_id": 206903,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How to grep lines which does not begin with \"#\" or \";\"?",
    "response": "grep \"^[^#;]\" smb.conf\n\nThe first `^` refers to the beginning of the line, so lines with comments starting after the first character will not be excluded.  `[^#;]` means any character which is not `#` or `;`.\n\nIn other words, it reports lines that start with any character other than `#` and `;`. It's not the same as reporting the lines that don't start with `#` and `;` (for which you'd use `grep -v '^[#;]'`) in that it also excludes empty lines, but that's probably preferable in this case as I doubt you care about empty lines.\n\nIf you wanted to ignore leading blank characters, you could change it to:\n\ngrep '^[[:blank:]]*[^[:blank:]#;]' smb.conf\n\nor\n\ngrep -vxE '[[:blank:]]*([#;].*)?' smb.conf\n\nOr\n\nawk '$1 ~ /^[^;#]/' smb.conf",
    "question_score": 141,
    "answer_score": 230,
    "question_id": 60994,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How do I pass a list of files to grep",
    "response": "Well, the generic case that works with any command that writes to stdout is to use `xargs`, which will let you attach any number of command-line arguments to the end of a command:\n$ find … | xargs grep 'search'\n\nOr to embed the command in your `grep` line with backticks or `$()`, which will run the command and substitute its output:\n$ grep 'search' $(find …)\n\nNote that these commands don't work if the file names contain whitespace, or certain other “weird characters” (`\\'\"` for xargs, `\\[*?` for `$(find …)`).\n\nHowever, in the specific case of `find` the ability to execute a program on the given arguments is built-in:\n$ find … -exec grep 'search' {} \\;\n\nEverything between `-exec` and `;` is the command to execute; `{}` is replaced with the filename found by `find`. That will execute a separate `grep` for each file; since `grep` can take many filenames and search them all, you can change the `;` to `+` to tell find to pass all the matching filenames to `grep` at once:\n$ find … -exec grep 'search' {} \\+",
    "question_score": 148,
    "answer_score": 183,
    "question_id": 20262,
    "has_full_answer": true,
    "tags": [
      "grep",
      "find",
      "pipe"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How to read first and last line from cat output?",
    "response": "sed Solution:\n\nsed -e 1b -e '$!d' file\n\nWhen reading from `stdin` if would look like this (for example `ps -ef`):\n\nps -ef | sed -e 1b -e '$!d'\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      1931  1837  0 20:05 pts/0    00:00:00 sed -e 1b -e $!d\n\nhead & tail Solution:\n\n(head -n1 && tail -n1) <file\n\nWhen data is coming from a command (`ps -ef`):\n\nps -ef 2>&1 | (head -n1 && tail -n1)\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      2068  1837  0 20:13 pts/0    00:00:00 -bash\n\nawk Solution:\n\nawk 'NR==1; END{print}' file\n\nAnd also the piped example with `ps -ef`:\n\nps -ef | awk 'NR==1; END{print}'\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      1935  1837  0 20:07 pts/0    00:00:00 awk NR==1; END{print}",
    "question_score": 96,
    "answer_score": 173,
    "question_id": 139089,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "sed",
      "awk",
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "grep inside less?",
    "response": "`less` has very powerful pattern matching.  From the man page:\n\n  `&pattern`\n  Display only lines which match the `pattern`;\n  lines which do not match the `pattern`\n  are not displayed.  If `pattern` is empty\n  (if you type `&` immediately followed by ENTER),\n  any filtering is turned off, and all lines are displayed. \n  While filtering is in effect,\n  an ampersand is displayed at the beginning of the prompt,\n  as a reminder that some lines in the file may be hidden.\n  \n  Certain characters are special as in the `/` command†:\n  \n  `^N` or `!`\n  Display only lines which do NOT match the `pattern`.\n  `^R`\n      Don't interpret regular expression metacharacters;\n  that is, do a simple textual comparison.\n  ____________\n  † Certain characters are special\n  if entered at the beginning of the `pattern`;\n  they modify the type of search\n  rather than become part of the `pattern`.\n  \n\n   (Of course `^N` and `^R` represent Ctrl+N\nand Ctrl+R, respectively.) \n\nSo, for example, `&dns` will display only lines that match the pattern `dns`,\nand `&!dns` will filter out (exclude) those lines,\ndisplaying only lines that don't match the pattern.\n\nIt is noted in the description of the `/` command that\n\n  The `pattern` is a regular expression,\n  as recognized by the regular expression library supplied by your system.\n\nSo\n\n`&eth[01]`  will display lines containing `eth0` or `eth1`\n`&arp.*eth0` will display lines containing `arp` followed by `eth0`\n`&arp|dns`  will display lines containing `arp` or `dns`\n\nAnd the `!` can invert any of the above. \nSo the command you would want to use for the example in your question is:\n\n&!event text|something else|the other thing|foo|bar\n\nAlso use `/pattern` and `?pattern`\nto search (and `n`/`N` to go to next/previous).",
    "question_score": 114,
    "answer_score": 167,
    "question_id": 179238,
    "has_full_answer": true,
    "tags": [
      "bash",
      "grep",
      "logs",
      "less"
    ],
    "topic": "grep"
  },
  {
    "instruction": "How can I suppress output from grep, so that it only returns the exit status?",
    "response": "Any POSIX compliant version of `grep` has the switch `-q` for quiet:\n\n-q\n     Quiet. Nothing shall be written to the standard output, regardless\n     of matching lines. Exit with zero status if an input line is selected.\n\nIn GNU grep (and possibly others) you can use long-option synonyms as well:\n\n-q, --quiet, --silent     suppress all normal output\n\nExample\n\nString exists:\n\n$ echo \"here\" | grep -q \"here\"\n$ echo $?\n0\n\nString doesn't exist:\n\n$ echo \"here\" | grep -q \"not here\"\n$ echo $?\n1",
    "question_score": 125,
    "answer_score": 153,
    "question_id": 104641,
    "has_full_answer": true,
    "tags": [
      "shell-script",
      "scripting",
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Detecting pattern at the end of a line with grep",
    "response": "The `$` anchor matches the end of a line.\n\nls -R | grep '\\.rar$'\n\nYou can also use `find` for this:\n\nfind . -name '*.rar'",
    "question_score": 118,
    "answer_score": 152,
    "question_id": 124462,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Grep 'OR' regex problem",
    "response": "With normal regex, the characters `(`, `|` and `)` need to be escaped. So you should use\n\n$ grep \"^ID.*\\(ETS\\|FBS\\)\" my_file.txt\n\nYou don't need the escapes when you use the extended regex (`-E`)option. See `man grep`, section \"`Basic vs Extended Regular Expressions`\".",
    "question_score": 87,
    "answer_score": 124,
    "question_id": 21764,
    "has_full_answer": true,
    "tags": [
      "grep",
      "regular-expression"
    ],
    "topic": "grep"
  },
  {
    "instruction": "grep lines starting with \"1\" in Ubuntu",
    "response": "Your regular expression doesn't mean what you think it does. It matches all lines starting (^) with one (1) repeated zero or more (*) times. All strings match that regular expression. `grep '^1'` does what you want.",
    "question_score": 88,
    "answer_score": 119,
    "question_id": 59893,
    "has_full_answer": true,
    "tags": [
      "grep",
      "regular-expression"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Reading grep patterns from a file",
    "response": "The `-f` option specifies a file where grep reads patterns. That's just like passing patterns on the command line (with the `-e` option if there's more than one), except that when you're calling from a shell you may need to quote the pattern to protect special characters in it from being expanded by the shell.\n\nThe argument `-E` or `-F` or `-P`, if any, tells grep which syntax the patterns are written in. With no argument, grep expects basic regular expressions; with `-E`, grep expects extended regular expressions; with `-P` (if supported), grep expects Perl regular expressions; and with `-F`, grep expects literal strings. Whether the patterns come from the command line or from a file doesn't matter.\n\nNote that the strings are substrings: if you pass `a+b` as a pattern then a line containing `a+b+c` is matched. If you want to search for lines containing exactly one of the supplied strings and no more, then pass the `-x` option.",
    "question_score": 100,
    "answer_score": 117,
    "question_id": 83260,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Preventing grep from causing premature termination of \"bash -e\" script",
    "response": "echo \"anything\" | { grep e || true; }\n\nExplanation:\n\nThis will throw an error\n$ echo \"anything\" | grep e\n$ echo $?\n1\n\nThis will not throw an error\n$ echo \"anything\" | { grep e || true; }\n$ echo $?\n0\n\nDopeGhoti's \"no-op\" version (Potentially avoids spawning a process, if `true` is not a builtin), this will not throw an error\n$ echo \"anything\" | { grep e || :; }\n$ echo $?\n0\n\nThe `||` means \"or\". If the first part of the command \"fails\" (meaning `grep e` returns a non-zero exit code) then the part after the `||` is executed, succeeds and returns zero as the exit code  (`true` always returns zero).",
    "question_score": 131,
    "answer_score": 115,
    "question_id": 330660,
    "has_full_answer": true,
    "tags": [
      "bash",
      "grep",
      "exit",
      "error-handling"
    ],
    "topic": "grep"
  },
  {
    "instruction": "grep and tail -f?",
    "response": "Using GNU `tail` and GNU `grep`, I am able to grep a `tail -f` using the straight-forward syntax:\n\ntail -f /var/log/file.log | grep search_term",
    "question_score": 91,
    "answer_score": 114,
    "question_id": 3229,
    "has_full_answer": true,
    "tags": [
      "shell",
      "command-line",
      "grep",
      "tail"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Why doesn't grep ignore binary files by default?",
    "response": "Not everything that grep thinks is a binary file, is actually a binary file. e.g. puppet's logs have ansi color coding in them, which makes grep think they're binary. I'd still want to search them if I'm grepping through /var/log though.",
    "question_score": 118,
    "answer_score": 107,
    "question_id": 70438,
    "has_full_answer": true,
    "tags": [
      "grep"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Is there a basic tutorial for grep, awk and sed?",
    "response": "AWK is particularly well suited for tabular data and has a lower learning curve than some alternatives.\nAWK: A Tutorial and Introduction\nAn AWK Primer (alt link)\nRegularExpressions.info\nsed tutorial\ngrep tutorial\n`info sed`, `info grep` and `info awk` or `info gawk`",
    "question_score": 113,
    "answer_score": 80,
    "question_id": 2434,
    "has_full_answer": true,
    "tags": [
      "grep",
      "sed",
      "awk"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Limit grep context to N characters on line",
    "response": "With GNU `grep`:\n\nN=10; grep -roP \".{0,$N}foo.{0,$N}\" .\n\nExplanation:\n\n`-o` => Print only what you matched\n`-P` => Use Perl-style regular expressions\nThe regex says match 0 to `$N` characters followed by `foo` followed by 0 to `$N` characters.\n\nIf you don't have GNU `grep`:\n\nfind . -type f -exec \\\n```bash\nperl -nle '\n```\n        BEGIN{$N=10}\n        print if s/^.*?(.{0,$N}foo.{0,$N}).*?$/$ARGV:$1/\n```bash\n' {} \\;\n```\n\nExplanation:\n\nSince we can no longer rely on `grep` being GNU `grep`, we make use of `find` to search for files recursively (the `-r` action of GNU `grep`). For each file found, we execute the Perl snippet.\n\nPerl switches:\n\n`-n` Read the file line by line\n`-l` Remove the newline at the end of each line and put it back when printing\n`-e` Treat the following string as code\n\nThe Perl snippet is doing essentially the same thing as `grep`. It starts by setting a variable `$N` to the number of context characters you want. The `BEGIN{}` means this is executed only once at the start of execution not once for every line in every file.\n\nThe statement executed for each line is to print the line if the regex substitution works.\n\nThe regex:\n\nMatch any old thing lazily1 at the start of line (`^.*?`) followed by `.{0,$N}` as in the `grep` case, followed by `foo`followed by another `.{0,$N}` and finally match any old thing lazily till the end of line (`.*?$`).\nWe substitute this with `$ARGV:$1`. `$ARGV` is a magical variable that holds the name of the current file being read. `$1` is what the parens matched: the context in this case.\nThe lazy matches at either end are required because a greedy match would eat all characters before `foo` without failing to match (since `.{0,$N}` is allowed to match zero times).\n\n1That is, prefer not to match anything unless this would cause the overall match to fail. In short, match as few characters as possible.",
    "question_score": 91,
    "answer_score": 41,
    "question_id": 163726,
    "has_full_answer": true,
    "tags": [
      "grep",
      "search"
    ],
    "topic": "grep"
  },
  {
    "instruction": "Problem running find: missing argument to `-exec'",
    "response": "You missed a `;` (escaped here as `\\;` to prevent the shell from interpreting it) or a `+` and a `{}`:\n\nfind . -exec grep chrome {} \\;\n\nor\n\nfind . -exec grep chrome {} +\n\n`find` will execute `grep` and will substitute `{}` with the filename(s) found. The difference between `;` and `+` is that with `;` a single `grep` command for each file is executed whereas with `+` as many files as possible are given as parameters to `grep` at once.",
    "question_score": 788,
    "answer_score": 1191,
    "question_id": 12902,
    "has_full_answer": true,
    "tags": [
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "how can I recursively delete empty directories in my home directory?",
    "response": "The `find` command is the primary tool for recursive file system operations.\nUse the `-type d` expression to tell `find` you're interested in finding directories only (and not plain files). The GNU version of `find` supports the `-empty` test, so\n\n$ find . -type d -empty -print\n\nwill print all empty directories below your current directory.\n\nUse `find ~ -…` or `find \"$HOME\" -…` to base the search on your home directory (if it isn't your current directory).\n\nAfter you've verified that this is selecting the correct directories, use `-delete` to delete all matches:\n\n$ find . -type d -empty -delete",
    "question_score": 312,
    "answer_score": 595,
    "question_id": 46322,
    "has_full_answer": true,
    "tags": [
      "linux",
      "filesystems",
      "find",
      "rm"
    ],
    "topic": "find"
  },
  {
    "instruction": "How can I find broken symlinks",
    "response": "I'd strongly suggest not to use `find -L`  for the task (see below for explanation). Here are some other ways to do this:\n\nIf you want to use a \"pure `find`\" method, and assuming the GNU implementation of `find`, it should rather look like this:\nfind . -xtype l\n\n(`xtype` is a test performed on a dereferenced link)\n\nportably (though less efficiently), you can also exec `test -e` from within the `find` command:\nfind . -type l ! -exec test -e {} \\; -print\n\nEven some `grep` trick could be better (i.e., safer) than `find -L`, but not exactly such as presented in the question (which greps in entire output lines, including filenames):\nfind . -type l -exec sh -c 'file -b \"$1\" | grep -q \"^broken\"' sh {} \\; -print\n\nThe `find -L` trick quoted by solo from commandlinefu looks nice and hacky, but it has one very dangerous pitfall: All the symlinks are followed. Consider directory with the contents presented below:\n$ ls -l\ntotal 0\nlrwxrwxrwx 1 michal users  6 May 15 08:12 link_1 -> nonexistent1\nlrwxrwxrwx 1 michal users  6 May 15 08:13 link_2 -> nonexistent2\nlrwxrwxrwx 1 michal users  6 May 15 08:13 link_3 -> nonexistent3\nlrwxrwxrwx 1 michal users  6 May 15 08:13 link_4 -> nonexistent4\nlrwxrwxrwx 1 michal users 11 May 15 08:20 link_out -> /usr/share/\n\nIf you run `find -L . -type l` in that directory, all `/usr/share/` would be searched as well (and that can take really long)1. For a `find` command that is \"immune to outgoing links\", don't use `-L`.\n\n1 This may look like a minor inconvenience (the command will \"just\" take long to traverse all `/usr/share`) – but can have more severe consequences. For instance, consider chroot environments: They can exist in some subdirectory of the main filesystem and contain symlinks to absolute locations. Those links could seem to be broken for the \"outside\" system, because they only point to proper places once you've entered the chroot. I also recall that some bootloader used symlinks under `/boot` that only made sense in an initial boot phase, when the boot partition was mounted as `/`.\nSo if you use a `find -L` command to find and then delete broken symlinks from some harmless-looking directory, you might even break your system...",
    "question_score": 429,
    "answer_score": 543,
    "question_id": 34248,
    "has_full_answer": true,
    "tags": [
      "shell",
      "find",
      "symlink"
    ],
    "topic": "find"
  },
  {
    "instruction": "Delete files older than X days +",
    "response": "Be careful with special file names (spaces, quotes) when piping to rm.\nThere is a safe alternative - the -delete option:\nfind /path/to/directory/ -mindepth 1 -mtime +5 -delete\n\nThat's it, no separate rm call and you don't need to worry about file names.\nReplace `-delete` with `-depth -print` to test this command before you run it (`-delete` implies `-depth`).\nExplanation:\n\n`-mindepth 1`: without this, `.` (the directory itself) might also\nmatch and therefore get deleted.\n`-mtime +5`: process files whose\ndata was last modified 5*24 hours ago.",
    "question_score": 242,
    "answer_score": 433,
    "question_id": 194863,
    "has_full_answer": true,
    "tags": [
      "files",
      "find",
      "rm",
      "timestamps"
    ],
    "topic": "find"
  },
  {
    "instruction": "Find command: how to ignore case?",
    "response": "Recent versions of GNU `find` have an `-iname` flag, for case-insensitive name searches.\n\nfind . -iname \"WSFY321.c\"",
    "question_score": 292,
    "answer_score": 389,
    "question_id": 32155,
    "has_full_answer": true,
    "tags": [
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to use find command to search for multiple extensions",
    "response": "Use the `-o` flag between different parameters.\n`find ./ -type f \\( -iname \\*.jpg -o -iname \\*.png \\)` works like a charm.\nNOTE There must be a space between the bracket and its contents or it won't work.\nExplanation:\n\n`-type f` - only search for files (not directories)\n`\\(` & `\\)` - are needed for the `-type f` to apply to all arguments\n`-o` - logical OR operator\n`-iname` - like `-name`, but the match is case insensitive",
    "question_score": 297,
    "answer_score": 359,
    "question_id": 15308,
    "has_full_answer": true,
    "tags": [
      "find",
      "regular-expression"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to remove all empty directories in a subtree?",
    "response": "Combining GNU `find` options and predicates, this command should do the job:\nfind . -type d -empty -delete\n\n`-type d` restricts to directories\n`-empty` restricts to empty ones\n`-delete` removes each directory\n\nThe tree is walked from the leaves without the need to specify `-depth` as it is implied by `-delete`.\nIf you don't want to delete the top of the search tree, then add `-mindepth 1` to the start of the list of tests:\nfind /some/path -mindepth 1 -type d -empty -delete",
    "question_score": 221,
    "answer_score": 338,
    "question_id": 8430,
    "has_full_answer": true,
    "tags": [
      "directory",
      "find",
      "rm"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to delete directories based on `find` output?",
    "response": "Find can execute arguments with the `-exec` option for each match it finds. It is a recommended mechanism because you can handle paths with spaces/newlines and other characters in them correctly. You will have to delete the contents of the directory before you can remove the directory itself, so use `-r` with the `rm` command to achieve this.\n\nFor your example you can issue:\n\nfind . -name \".svn\" -exec rm -r \"{}\" \\;\n\nYou can also tell find to just find directories named .svn by adding a `-type d` check:\n\nfind . -name \".svn\" -type d -exec rm -r \"{}\" \\;\n\nWarning Use `rm -r` with caution it deletes the folder and all its contents.\n\nIf you want to delete just empty directories as well as directories that contain only empty directories, find can do that itself with `-delete` and `-empty`:\n\nfind . -name \".svn\" -type d -empty -delete",
    "question_score": 256,
    "answer_score": 300,
    "question_id": 89925,
    "has_full_answer": true,
    "tags": [
      "find",
      "rm",
      "subversion"
    ],
    "topic": "find"
  },
  {
    "instruction": "Finding all large files in the root filesystem",
    "response": "Try:\n\nfind / -xdev -type f -size +100M\n\nIt lists all files that has size bigger than 100M.\n\nIf you want to know about directory, you can try `ncdu`.\n\nIf you aren't running Linux, you may need to use `-size +204800` or `-size +104857600c`, as the `M` suffix to mean megabytes isn't in POSIX.\n\nfind / -xdev -type f -size +102400000c",
    "question_score": 184,
    "answer_score": 291,
    "question_id": 140367,
    "has_full_answer": true,
    "tags": [
      "find",
      "disk-usage"
    ],
    "topic": "find"
  },
  {
    "instruction": "Understanding the -exec option of `find`",
    "response": "This answer comes in the following parts:\n\nBasic usage of `-exec`\nUsing `-exec` in combination with `sh -c`\nUsing `-exec ... {} +`\nUsing `-execdir`\n\nBasic usage of `-exec`\nThe `-exec` option takes an external utility with optional arguments as its argument and executes it.\nIf the string `{}` is present anywhere in the given command, each instance of it will be replaced by the pathname currently being processed (e.g. `./some/path/FILENAME`).  In most shells, the two characters `{}` does not need to be quoted.\nThe command needs to be terminated with a `;` for `find` to know where it ends (as there may be further options afterwards).  To protect the `;` from the shell, it needs to be quoted as `\\;` or `';'`, otherwise the shell will see it as the end of the `find` command.\nExample (the `\\` at the end of the first two lines are just for line continuations):\nfind . -type f -name '*.txt'      \\\n   -exec grep -q 'hello' {} ';'   \\\n   -exec cat {} ';'\n\nThis will find all regular files (`-type f`) whose names matches the pattern `*.txt` in or below the current directory.  It will then test whether the string `hello` occurs in any of the found files using `grep -q` (which does not produce any output, just an exit status).  For those files that contain the string, `cat` will be executed to output the contents of the file to the terminal.\nEach `-exec` also acts like a \"test\" on the pathnames found by `find`, just like `-type` and `-name` does. If the command returns a zero exit status (signifying \"success\"), the next part of the `find` command is considered, otherwise the `find` command continues with the next pathname.  This is used in the example above to find files that contain the string `hello`, but to ignore all other files.\nThe above example illustrates the two most common use cases of `-exec`:\n\nAs a test to further restrict the search.\nTo perform some kind of action on the found pathname (usually, but not necessarily, at the end of the `find` command).\n\nUsing `-exec` in combination with `sh -c`\nThe command that `-exec` can execute is limited to an external utility with optional arguments. To use shell built-ins, functions, conditionals, pipelines, redirections etc. directly with `-exec` is not possible, unless wrapped in something like a `sh -c` child shell.\nIf `bash` features are required, then use `bash -c` in place of `sh -c`.\n`sh -c` runs `/bin/sh` with a script given on the command line, followed by optional command line arguments to that script.\nA simple example of using `sh -c` by itself, without `find`:\nsh -c 'echo  \"You gave me $1, thanks!\"' sh \"apples\"\n\nThis passes two arguments to the child shell script. These will be placed in `$0` and `$1` for the script to use.\n\nThe string `sh`.  This will be available as `$0` inside the script, and if the internal shell outputs an error message, it will prefix it with this string.\n\nThe argument `apples` is available as `$1` in the script, and had there been more arguments, then these would have been available as `$2`, `$3` etc.  They would also be available in the list `\"$@\"` (except for `$0` which would not be part of `\"$@\"`).\n\nThis is useful in combination with `-exec` as it allows us to make arbitrarily complex scripts that acts on the pathnames found by `find`.\nExample: Find all regular files that have a certain filename suffix, and change that filename suffix to some other suffix, where the suffixes are kept in variables:\nfrom=text  #  Find files that have names like something.text\nto=txt     #  Change the .text suffix to .txt\n\nfind . -type f -name \"*.$from\" -exec sh -c 'mv \"$3\" \"${3%.$1}.$2\"' sh \"$from\" \"$to\" {} ';'\n\nInside the internal script, `$1` would be the string `text`, `$2` would be the string `txt` and `$3` would be whatever pathname `find` has found for us.  The parameter expansion `${3%.$1}` would take the pathname and remove the suffix `.text` from it.\nOr, using `dirname`/`basename`:\nfind . -type f -name \"*.$from\" -exec sh -c '\n```bash\nmv \"$3\" \"$(dirname \"$3\")/$(basename \"$3\" \".$1\").$2\"' sh \"$from\" \"$to\" {} ';'\n```\n\nor, with added variables in the internal script:\nfind . -type f -name \"*.$from\" -exec sh -c '\n```bash\nfrom=$1; to=$2; pathname=$3\nmv \"$pathname\" \"$(dirname \"$pathname\")/$(basename \"$pathname\" \".$from\").$to\"' sh \"$from\" \"$to\" {} ';'\n```\n\nNote that in this last variation, the variables `from` and `to` in the child shell are distinct from the variables with the same names in the external script.\nThe above is the correct way of calling an arbitrary complex script from `-exec` with `find`.  Using `find` in a loop like\nfor pathname in $( find ... ); do\n\nis error prone and inelegant (personal opinion). It is splitting filenames on whitespaces, invoking filename globbing, and also forces the shell to expand the complete result of `find` before even running the first iteration of the loop.\nSee also:\n\nWhy is looping over find's output bad practice?\nIs it possible to use `find -exec sh -c` safely?\n\nUsing `-exec ... {} +`\nThe `;` at the end may be replaced by `+`.  This causes `find` to execute the given command with as many arguments (found pathnames) as possible rather than once for each found pathname.  The string `{}`  has to occur just before the `+` for this to work.\nfind . -type f -name '*.txt' \\\n   -exec grep -q 'hello' {} ';' \\\n   -exec cat {} +\n\nHere, `find` will collect the resulting pathnames and execute `cat` on as many of them as possible at once.\nfind . -type f -name \"*.txt\" \\\n   -exec grep -q \"hello\" {} ';' \\\n   -exec mv -t /tmp/files_with_hello/ {} +\n\nLikewise here, `mv` will be executed as few times as possible. This last example requires GNU `mv` from coreutils (which supports the `-t` option).\nUsing `-exec sh -c ... {} +` is also an efficient way to loop over a set of pathnames with an arbitrarily complex script.\nThe basics is the same as when using `-exec sh -c ... {} ';'`, but the script now takes a much longer list of arguments.  These can be looped over by looping over `\"$@\"` inside the script.\nOur example from the last section that changes filename suffixes:\nfrom=text  #  Find files that have names like something.text\nto=txt     #  Change the .text suffix to .txt\n\nfind . -type f -name \"*.$from\" -exec sh -c '\n```bash\nfrom=$1; to=$2\nshift 2  # remove the first two arguments from the list\n```\n             # because in this case these are *not* pathnames\n             # given to us by find\n```bash\nfor pathname do  # or:  for pathname in \"$@\"; do\n```\n        mv \"$pathname\" \"${pathname%.$from}.$to\"\n```bash\ndone' sh \"$from\" \"$to\" {} +\n```\n\nUsing `-execdir`\nThere is also `-execdir` (implemented by most `find` variants, but not a standard option).\nThis works like `-exec` with the difference that the given shell command is executed with the directory of the found pathname as its current working directory and that `{}` will contain the basename of the found pathname without its path (but GNU `find` will still prefix the basename with `./`, while BSD `find` or `sfind` won't).\nExample:\nfind . -type f -name '*.txt' \\\n```bash\n-execdir mv -- {} 'done-texts/{}.done' \\;\n```\n\nThis will move each found `*.txt`-file to a pre-existing `done-texts` subdirectory in the same directory as where the file was found.  The file will also be renamed by adding the suffix `.done` to it. `--`, to mark the end of options is needed here in those `find` implementations that don't prefix the basename with `./`. The quotes around the argument that contains `{}` not as a whole are needed if your shell is `(t)csh`. Also note that not all `find` implementations will expand that `{}` there (`sfind` won't).\nThis would be a bit trickier to do with `-exec` as we would have to get the basename of the found file out of `{}` to form the new name of the file. We also need the directory name from `{}` to locate the `done-texts` directory properly.\nWith `-execdir`, some things like these becomes easier.\nThe corresponding operation using `-exec` instead of `-execdir` would have to employ a child shell:\nfind . -type f -name '*.txt' -exec sh -c '\n```bash\nfor name do\n```\n        mv \"$name\" \"$( dirname \"$name\" )/done-texts/$( basename \"$name\" ).done\"\n```bash\ndone' sh {} +\n```\n\nor,\nfind . -type f -name '*.txt' -exec sh -c '\n```bash\nfor name do\n```\n        mv \"$name\" \"${name%/*}/done-texts/${name##*/}.done\"\n```bash\ndone' sh {} +\n```",
    "question_score": 187,
    "answer_score": 284,
    "question_id": 389705,
    "has_full_answer": true,
    "tags": [
      "shell",
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to skip \"permission denied\" errors when running find in Linux?",
    "response": "you can filter out messages to `stderr`. I prefer to redirect them to `stdout` like this.\n\n find / -name art  2>&1 | grep -v \"Permission denied\"\n\nExplanation:\n\nIn short, all regular output goes to standard output (`stdout`). All error messages to standard error (`stderr`).\n\n`grep` usually finds/prints the specified string, the `-v` inverts this, so it finds/prints every string that doesn't contain \"Permission denied\". All of your output from the find command, including error messages usually sent to `stderr` (file descriptor 2) go now to `stdout`(file descriptor 1) and then get filtered by the `grep` command.\n\nThis assumes you are using the `bash/sh` shell.\n\nUnder `tcsh/csh` you would use  \n\n find / -name art |& grep ....",
    "question_score": 226,
    "answer_score": 281,
    "question_id": 42841,
    "has_full_answer": true,
    "tags": [
      "linux",
      "permissions",
      "files",
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to stop the find command after first match?",
    "response": "With GNU or FreeBSD `find`, you can use the `-quit` predicate:\nfind . ... -print -quit\n\nThe NetBSD `find` equivalent:\nfind . ... -print -exit\n\nIf all you do is printing the name, and assuming the filenames don't contain newline characters, you could do:\nfind . ... -print | head -n 1\n\nThat will not stop `find` after the first match, but possibly, depending on timing and buffering upon the second match or (much) later. Basically, `find` will be terminated with a SIGPIPE when it tries to output something while `head` is already gone because it has already read and displayed the first line of input.\nNote that not all shells will wait for that `find` command after `head` has returned. The Bourne shell and AT&T implementations of `ksh` (when non-interactive) and `yash` (only if that pipeline is the last command in a script) would not, leaving it running in background. If you'd rather see that behaviour in any shell, you could always change the above to:\n(find . ... -print &) | head -n 1\n\nIf you're doing more than printing the paths of the found files, you could try this approach:\nfind . ... -exec sh -c 'printf \"%s\\n\" \"$1\"; kill -s PIPE \"$PPID\"' sh {} \\;\n\n(replace `printf` with whatever you would be doing with that file).\nThat has the side effect of `find` returning an exit status reflecting the fact that it was killed though.\nWe're sending the SIGPIPE signal instead of the default SIGTERM to avoid the message that some shells display when parts of a pipe line are killed with a signal. They generally don't do it for deaths by SIGPIPE, as those are naturally happening (like in `find | head` above...).",
    "question_score": 217,
    "answer_score": 240,
    "question_id": 62880,
    "has_full_answer": true,
    "tags": [
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "Why is looping over find's output bad practice?",
    "response": "The problem\nfor f in $(find .)\n\ncombines two incompatible things.\n`find` prints a list of file paths delimited by newline characters. While the split+glob operator that is invoked when you leave that `$(find .)` unquoted in that list context splits it on the characters of `$IFS` (by default includes newline, but also space and tab (and NUL in `zsh`)) and performs globbing on each resulting word (except in `zsh`) (and even brace expansion in ksh93 (even if the `braceexpand` option is off in older versions) or pdksh derivatives!).\nEven if you make it:\nIFS='\n' # split on newline only\nset -o noglob # disable glob (also disables brace expansion\n              # done upon other expansions in ksh)\nfor f in $(find .) # invoke split+glob\n\nThat's still wrong as the newline character is as valid as any in a file path. The output of `find -print` is simply not post-processable reliably (except by using some convoluted trick, as shown here).\nThat also means the shell needs to store the output of `find` fully, and then split+glob it (which implies storing that output a second time in memory) before starting to loop over the files.\nNote that `find . | xargs cmd` has similar problems (there, blanks, newline, single quote, double quote and backslash (and with some `xarg` implementations bytes not forming part of valid characters) are a problem)\nMore correct alternatives\nThe only way to use a `for` loop on the output of `find` would be to use `zsh` that supports `IFS=$'\\0'` and:\nIFS=$'\\0'\nfor f in $(find . -print0)\n\n(replace `-print0` with `-exec printf '%s\\0' {} +` for `find` implementations that don't support the non-standard (but quite common nowadays) `-print0`).\nHere, the correct and portable way is to use `-exec`:\nfind . -exec something with {} \\;\n\nOr if `something` can take more than one argument:\nfind . -exec something with {} +\n\nIf you do need that list of files to be handled by a shell:\nfind . -exec sh -c '\n  for file do\n```bash\nsomething < \"$file\"\n```\n  done' find-sh {} +\n\n(beware it may start more than one `sh`).\nOn some systems, you can use:\nfind . -print0 | xargs -r0 something with\n\nthough that has little advantage over the standard syntax and means `something`'s `stdin` is either the pipe or `/dev/null`.\nOne reason you may want to use that could be to use the `-P` option of GNU `xargs` for parallel processing. The `stdin` issue can also be worked around with GNU `xargs` with the `-a` option with shells supporting process substitution:\nxargs -r0n 20 -P 4 -a <(find . -print0) something\n\nfor instance, to run up to 4 concurrent invocations of `something` each taking 20 file arguments.\nWith `zsh`, `bash` or recent versions of ksh93u+m, another way to loop over the output of `find -print0` is with:\nwhile IFS= LC_ALL=C read -u3 -rd '' file; do\n  something \"$file\" 3<&-\ndone 3< <(find . -print0)\n\n`read -d ''` reads NUL delimited records instead of newline delimited ones. `LC_ALL=C` is to work around a bug in some versions of bash.\n`bash-4.4` and above can also store files returned by `find -print0` in an array with:\nreadarray -td '' files < <(find . -print0)\n\nThe `zsh` equivalent (which has the advantage of preserving `find`'s exit status):\nfiles=( ${(0)\"$(find . -print0)\"} )\n\nWith `zsh`, you can translate most `find` expressions to a combination of recursive globbing with glob qualifiers. For instance, looping over `find . -name '*.txt' -type f -mtime -1` would be:\nfor file ( ./**/*.txt(ND.m-1) ) cmd $file\n\nOr\nfor file (**/*.txt(ND.m-1)) cmd -- $file\n\n(beware of the need of `--` as with `**/*`, file paths are not starting with `./`, so may start with `-` for instance).\n`ksh93` and `bash` eventually added support for `**/` (though not more advances forms of recursive globbing), but still not the glob qualifiers which makes the use of `**` very limited there. Also beware that `bash` prior to 4.3 follows symlinks when descending the directory tree. While it improved in 4.3, it was not fully fixed until 5.0.\nLike for looping over `$(find .)`, that also means storing the whole list of files in memory¹. That may be desirable though in some cases when you don't want your actions on the files to have an influence on the finding of files (like when you add more files that could end-up being found themselves).\nOther reliability/security considerations\nRace conditions\nNow, if we're talking of reliability, we have to mention the race conditions between the time `find`/`zsh` finds a file and checks that it meets the criteria and the time it is being used (TOCTOU race).\nEven when descending a directory tree, one has to make sure not to follow symlinks and to do that without TOCTOU race. `find` (GNU `find` at least) does that by opening the directories using `openat()` with the right `O_NOFOLLOW` flags (where supported) and keeping a file descriptor open for each directory, `zsh`/`bash`/`ksh` don't do that. So in the face of an attacker being able to replace a directory with a symlink at the right time, you could end up descending the wrong directory.\nEven if `find` does descend the directory properly, with `-exec cmd {} \\;` and even more so with `-exec cmd {} +`, once `cmd` is executed, for instance as `cmd ./foo/bar` or `cmd ./foo/bar ./foo/bar/baz`, by the time `cmd` makes use of `./foo/bar`, the attributes of `bar` may no longer meet the criteria matched by `find`, but even worse, `./foo` may have been replaced by a symlink to some other place (and the race window is made a lot bigger with `-exec {} +` where `find` waits to have enough files to call `cmd`).\nSome `find` implementations have a (non-standard yet) `-execdir` predicate to alleviate the second problem.\nWith:\nfind . -execdir cmd -- {} \\;\n\n`find` `chdir()`s into the parent directory of the file before running `cmd`. Instead of calling `cmd -- ./foo/bar`, it calls `cmd -- ./bar` (`cmd -- bar` with some implementations, hence the `--`), so the problem with `./foo` being changed to a symlink is avoided. That makes using commands like `rm` safer (it could still remove a different file, but not a file in a different directory), but not commands that may modify the files unless they've been designed to not follow symlinks.\n`-execdir cmd -- {} +` sometimes also works but with several implementations including some versions of GNU `find`, it is equivalent to `-execdir cmd -- {} \\;`.\n`-execdir` also has the benefit of working around some of the problems associated with too deep directory trees.\nIn:\nfind . -exec cmd {} \\;\n\nthe size of the path given to `cmd` will grow with the depth of the directory the file is in. If that size gets bigger than `PATH_MAX` (something like 4k on Linux), then any system call that `cmd` does on that path will fail with a `ENAMETOOLONG` error.\nWith `-execdir`, only the file name (possibly prefixed with `./`) is passed to `cmd`. File names themselves on most file systems have a much lower limit (`NAME_MAX`) than `PATH_MAX`, so the `ENAMETOOLONG` error is less likely to be encountered.\nBytes vs characters\nAlso, often overlooked when considering security around `find` and more generally with handling file names in general is the fact that on most Unix-like systems, file names are sequences of bytes (any byte value but 0 in a file path, and on most systems (ASCII based ones, we'll ignore the rare EBCDIC based ones for now) 0x2f is the path delimiter).\nIt's up to the applications to decide if they want to consider those bytes as text. And they generally do, but generally the translation from bytes to characters is done based on the user's locale, based on the environment.\nWhat that means is that a given file name may have different text representation depending on the locale. For instance, the byte sequence `63 f4 74 e9 2e 74 78 74` would be `côté.txt` for an application interpreting that file name in a locale where the character set is ISO-8859-1, and `cєtщ.txt` in a locale where the charset is IS0-8859-5 instead.\nWorse. In a locale where the charset is UTF-8 (the norm nowadays), 63 f4 74 e9 2e 74 78 74 simply couldn't be mapped to characters!\n`find` is one such application that considers file names as text for its `-name`/`-path` predicates (and more, like `-iname` or `-regex` with some implementations).\nWhat that means is that for instance, with several `find` implementations (including GNU `find` on GNU systems²).\nfind . -name '*.txt'\n\nwould not find our `63 f4 74 e9 2e 74 78 74` file above when called in a UTF-8 locale as `*` (which matches 0 or more characters, not bytes) could not match those non-characters.\n`LC_ALL=C find...` would work around the problem as the C locale implies one byte per character and (generally) guarantees that all byte values map to a character (albeit possibly undefined ones for some byte values).\nNow when it comes to looping over those file names from a shell, that byte vs character can also become a problem. We typically see 4 main types of shells in that regard:\n\nThe ones that are still not multi-byte aware like `dash`. For them, a byte maps to a character. For instance, in UTF-8, `côté` is 4 characters, but 6 bytes. In a locale where UTF-8 is the charset, in\n find . -name '????' -exec dash -c '\n   name=${1##*/}; echo \"${#name}\"' sh {} \\;\n\n`find` will successfully find the files whose name consists of 4 characters encoded in UTF-8, but `dash` would report lengths ranging between 4 and 24.\n\n`yash`: the opposite. It only deals with characters. All the input it takes is internally translated to characters. It makes for the most consistent shell, but it also means it cannot cope with arbitrary byte sequences (those that don't translate to valid characters). Even in the C locale, it can't cope with byte values above 0x7f.\n find . -exec yash -c 'echo \"$1\"' sh {} \\;\n\nin a UTF-8 locale will fail on our ISO-8859-1 `côté.txt` from earlier for instance.\n\nThose like `bash` or `zsh` where the multi-byte support has been progressively added. Those will fall back to considering bytes that can't be mapped to characters as if they were characters. They still have a few bugs here and there especially with less common multi-byte charsets like GBK or BIG5-HKSCS (those being quite nasty as many of their multi-byte characters contain bytes in the 0-127 range (like the ASCII characters)).\n\nThose like the `sh` of FreeBSD (11 at least) or `mksh -o utf8-mode` that support multi-bytes but only for UTF-8.\n\nInterrupted output\nAnother problem with parsing the output of `find` or even `find -print0` may arise if `find` is interrupted, for instance because it has triggered some limit or was killed for whatever reason.\nExample:\n$ (ulimit -t 1; find / -type f -print0 2> /dev/null) | xargs -r0 printf 'rm -rf \"%s\"\\n' | tail -n 2\nrm -rf \"/usr/lib/x86_64-linux-gnu/guile/2.2/ccache/language/ecmascript/parse.go\"\nrm -rf \"/usr/\"\nzsh: cpu limit exceeded (core dumped)  ( ulimit -t 1; find / -type f -print0 2> /dev/null; ) |\nzsh: done                              xargs -r0 printf 'rm -rf \"%s\"\\n' | tail -n 2\n\nHere, `find` was interrupted because it reached the CPU time limit. Since the output is buffered (as it goes to a pipe), `find` had output a number of blocks to stdout and the end of the last block it had written at the time it was killed happened to be in the middle of some `/usr/lib/x86_64-linux-gnu/guile...` file path, here unfortunately just after the `/usr/`.\n`xargs`, just saw a non-delimited `/usr/` record followed by EOF and passed that to `printf`. If the command had been `rm -rf` instead, it could have had severe consequences.\n\nNotes\n¹ For completeness, we could mention a hacky way in `zsh` to loop over files using recursive globbing without storing the whole list in memory:\nprocess() {\n  something with $REPLY\n  false\n}\n: **/*(ND.m-1+process)\n\n`+cmd` is a glob qualifier that calls `cmd` (typically a function) with the current file path in `$REPLY`. The function returns true or false to decide if the file should be selected (and may also modify `$REPLY` or return several files in a `$reply` array). Here we do the processing in that function and return false so the file is not selected.\n² GNU `find` uses the system's `fnmatch()` libc function to do the pattern matching, so the behaviour there depends on how that function copes with non-text data.",
    "question_score": 220,
    "answer_score": 125,
    "question_id": 321697,
    "has_full_answer": true,
    "tags": [
      "files",
      "find",
      "filenames",
      "for"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to combine 2 -name conditions in find?",
    "response": "You can do this using a negated `-regex`, too:-\n\n find ./ ! -regex  '.*\\(deb\\|vmdk\\)$'",
    "question_score": 209,
    "answer_score": 76,
    "question_id": 50612,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "How to integrate mv command after find command?",
    "response": "With GNU mv:\n\nfind path_A -name '*AAA*' -exec mv -t path_B {} +\n\nThat will use find's `-exec` option which replaces the `{}` with each find result in turn and runs the command you give it. As explained in `man find`:\n\n   -exec command ;\n          Execute  command;  true  if 0 status is returned.  All following\n          arguments to find are taken to be arguments to the command until\n          an  argument  consisting of `;' is encountered.  \n\nIn this case, we are using the `+` version of `-exec` so that we run as few `mv` operations as possible:\n\n   -exec command {} +\n          This  variant  of the -exec action runs the specified command on\n          the selected files, but the command line is built  by  appending\n          each  selected file name at the end; the total number of invoca‐\n          tions of the command will  be  much  less  than  the  number  of\n          matched  files.   The command line is built in much the same way\n          that xargs builds its command lines.  Only one instance of  `{}'\n          is  allowed  within the command.  The command is executed in the\n          starting directory.",
    "question_score": 124,
    "answer_score": 198,
    "question_id": 154818,
    "has_full_answer": true,
    "tags": [
      "files",
      "find",
      "mv"
    ],
    "topic": "find"
  },
  {
    "instruction": "locate vs find: usage, pros and cons of each other",
    "response": "`locate(1)` has only one big advantage over `find(1)`: speed. \n\n`find(1)`, though, has many advantages over `locate(1)`:\n\n`find(1)` is primordial, going back to the very first version of AT&T Unix. You will even find it in cut-down embedded Linuxes via Busybox. It is all but universal. \n\n`locate(1)` is much younger than `find(1)`. The earliest ancestor of `locate(1)` didn't appear until 1983, and it wasn't widely available as \"`locate`\" until 1994, when it was adopted into GNU findutils and into 4.4BSD.\n`locate(1)` is also nonstandard, thus it is not installed by default everywhere. Some POSIX type OSes don't even offer it as an option, and where it is available, the implementation may be lacking features you want because there is no independent standard specifying the minimum feature set that must be available.\n\nThere is a de facto standard, being BSD `locate(1)`, but that is only because the other two main flavors of `locate` implement all of its options: `-0`, `-c`, `-d`, `-i`, `-l`, `-m`, `-s`, and `-S`. `mlocate` implements 6 additional options not in BSD `locate`: `-b`, `-e`, `-P`, `-q`, `--regex` and `-w`. GNU `locate` implements those six plus another four: `-A`, `-D`, `-E`, and `-p`. (I'm ignoring aliases and minor differences like `-?` vs `-h` vs `--help`.)\n\nThe BSDs and Mac OS X ship BSD `locate`.\n\nMost Linuxes ship GNU `locate`, but Red Hat Linuxes and Arch ship `mlocate` instead. Debian doesn't install either in its base install, but offers both versions in its default package repositories; if both are installed at once, \"`locate`\" runs `mlocate`.\n\nOracle has been shipping `mlocate` in Solaris since 11.2, released in December 2014. Prior to that, `locate` was not installed by default on Solaris. (Presumably, this was done to reduce Solaris' command incompatibility with Oracle Linux, which is based on Red Hat Enterprise Linux, which also uses `mlocate`.)\n\nIBM AIX still doesn't ship any version of `locate`, at least as of AIX 7.2, unless you install GNU `findutils` from the AIX Toolbox for Linux Applications.\n\nHP-UX also appears to lack `locate` in the base system.\n\nOlder \"real\" Unixes generally did not include an implementation of `locate`.\n`find(1)` has a powerful expression syntax, with many functions, Boolean operators, etc.\n`find(1)` can select files by more than just name. It can select by:\n\nage\nsize\nowner\nfile type\ntimestamp\npermissions\ndepth within the subtree...\n\nWhen finding files by name, you can search using file globbing syntax in all versions of `find(1)`, or in GNU or BSD versions, using regular expressions.\n\nCurrent versions of `locate(1)` accept glob patterns as `find` does, but BSD `locate` doesn't do regexes at all. If you're like me and have to use a variety of machine types, you find yourself preferring `grep` filtering to developing a dependence on `-r` or `--regex`.\n\n`locate` needs strong filtering more than `find` does because...\n`find(1)` doesn't necessarily search the entire filesystem. You typically point it at a subdirectory, a parent containing all the files you want it to operate on. The typical behavior for a `locate(1)` implementation is to spew up all files matching your pattern, leaving it to `grep` filtering and such to cut its eruption down to size.\n\n(Evil tip: `locate /` will probably get you a list of all files on the system!)\n\nThere are variants of `locate(1)` like `slocate(1)` which restrict output based on user permissions, but this is not the default version of `locate` in any major operating system.\n`find(1)` can do things to files it finds, in addition to just finding them. The most powerful and widely supported such operator is `-exec`, but there are others. In recent GNU and BSD find implementations, for example, you have the `-delete` and `-execdir` operators.\n`find(1)` runs in real time, so its output is always up to date.\n\nBecause `locate(1)` relies on a database updated hours or days in the past, its output can be outdated. (This is the stale cache problem.) This coin has two sides:\n\n`locate` can name files that no longer exist. \n\nGNU `locate` and `mlocate` have the `-e` flag to make it check for file existence before printing out the name of each file it discovered in the past, but this eats away some of the `locate` speed advantage, and isn't available in BSD `locate` besides.\n`locate` will fail to name files that were created since the last database update.\n\nYou learn to be somewhat distrustful of `locate` output, knowing it may be wrong.\n\nThere are ways to solve this problem, but I am not aware of any implementation in widespread use. For example, there is `rlocate`, but it appears to not work against any modern Linux kernel.\n`find(1)` never has any more privilege than the user running it.\n\nBecause `locate` provides a global service to all users on a system, it wants to have its `updatedb` process run as `root` so it can see the entire filesystem. This leads to a choice of security problems:\n\nRun `updatedb` as root, but make its output file world-readable so `locate` can run without special privileges. This effectively exposes the names of all files in the system to all users. This may be enough of a security breach to cause a real problem.\n\nBSD `locate` is configured this way on Mac OS X and FreeBSD.\nWrite the database as readable only by `root`, and make `locate` `setuid` root so it can read the database. This means `locate` effectively has to reimplement the OS's permission system so it doesn't show you files you can't normally see. It also increases the attack surface of your system, specifically risking a root escalation attack.\nCreate a special \"`locate`\" user or group to own the database file, and mark the `locate` binary as `setuid/setgid` for that user/group so it can read the database. This doesn't prevent privilege escalation attacks by itself, but it greatly mitigates the damage one could cause.\n\n`mlocate` is configured this way on Red Hat Enterprise Linux.\n\nYou still have a problem, though, because if you can use a debugger on `locate` or cause it to dump core you can get at privileged parts of the database.\n\nI don't see a way to create a truly \"secure\" `locate` command, short of running it separately for each user on the system, which negates much of its advantage over `find(1)`.\n\nBottom line, both are very useful. `locate(1)` is better when you're just trying to find a particular file by name, which you know exists, but you just don't remember where it is exactly. `find(1)` is better when you have a focused area to examine, or when you need any of its many advantages.",
    "question_score": 160,
    "answer_score": 196,
    "question_id": 60205,
    "has_full_answer": true,
    "tags": [
      "files",
      "find",
      "search",
      "locate"
    ],
    "topic": "find"
  },
  {
    "instruction": "`find` with multiple `-name` and `-exec` executes only the last matches of `-name`",
    "response": "find . -type f -name \"*.htm*\" -o -name \"*.js*\" -o -name \"*.txt\"\n\nis short for:\n\nfind . '('                                      \\\n           '(' -type f -a -name \"*.htm*\" ')' -o \\\n           '(' -name \"*.js*\" ')' -o             \\\n           '(' -name \"*.txt\" ')'                \\\n       ')' -a -print\n\nThat is, because no action predicate is specified (only conditions), a `-print` action is implicitly added for the files that match the conditions.\n(and, by the way, that would print non-regular `.js` files (the `-type f` only applies to `.htm` files)).\nWhile:\n\nfind . -type f -name \"*.htm*\" -o -name \"*.js*\" -o -name \"*.txt\" \\\n  -exec sh -c 'echo \"$0\"' {} \\;\n\nis short for:\n\nfind . '(' -type f -a -name \"*.htm*\" ')' -o \\\n       '(' -name \"*.js*\" ')' -o \\\n       '(' -name \"*.txt\" -a -exec sh -c 'echo \"$0\"' {} \\; ')'\nFor `find` (like in many languages), AND (`-a`; implicit when omitted) has precedence over OR (`-o`), and adding an explicit action predicate (here `-exec`) cancels the `-print` implicit action seen above.\nHere, you want:\nfind . '(' -name \"*.htm*\" -o -name \"*.js*\" -o -name \"*.txt\" ')' \\\n       -type f \\\n       -exec sh -c 'echo \"$0\"' {} \\;\n\nOr:\nfind . '(' -name \"*.htm*\" -o -name \"*.js*\" -o -name \"*.txt\" ')' \\\n       -type f \\\n       -exec sh -c '\n         for file do\n           echo \"$file\"\n         done' sh {} +\n\nTo avoid running one `sh` per file.\n(`-type f` being more expensive than `-name` as it potentially implies retrieving information from the inode, is best put after though some `find` implementations do reorder the checks internally for optimisation).",
    "question_score": 142,
    "answer_score": 185,
    "question_id": 102191,
    "has_full_answer": true,
    "tags": [
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "How can I grep the results of FIND using -EXEC and still output to a file?",
    "response": "If I understand you correctly this is what you want to do:\n\nfind . -name '*.py' -print0 | xargs -0 grep 'something' > output.txt\n\nFind all files with extension `.py`, `grep` only rows that contain `something` and save the rows in `output.txt`. If the `output.txt` file exists, it will be truncated, otherwise it will be created.\n\nUsing `-exec`:\n\nfind . -name '*.py' -exec grep 'something' {} \\; > output.txt\n\nI'm incorporating Chris Downs comment here: The above command will result in `grep` being executed as many times as `find` finds pathnames that passes the given tests (only the single `-name` test above). However, if you replace the `\\;` with a `+`, `grep` is called with multiple pathnames from `find` (up to a certain limit).\n\nSee question Using semicolon (;) vs plus (+) with exec in find for more on the subject.",
    "question_score": 120,
    "answer_score": 179,
    "question_id": 21033,
    "has_full_answer": true,
    "tags": [
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "How can I use `find` and sort the results by mtime?",
    "response": "Use `find`'s `-printf` command to output both the time (in a sortable way) and the file, then sort. If you use GNU find,\nfind . your-options -printf \"%T+ %p\\n\" | sort\n\nFor convenience here is an explanation of the `-printf \"%T+ %p\\n\"` from `man find`:\n\n`%Tk` File's last modification time in the format specified by `k`, which is the same as for `%A`.\n\nwhere `k` in this case is set to `+`\n`+`      Date and time, separated by `+`, for example `2004-04-28+22:22:05.0'.  This is a GNU extension.  The time is given in the current  timezone  (which may be affected by setting the TZ environment variable).  The seconds field includes a fractional part.\n\n`%p`     File's name.",
    "question_score": 113,
    "answer_score": 172,
    "question_id": 29899,
    "has_full_answer": true,
    "tags": [
      "find",
      "ls"
    ],
    "topic": "find"
  },
  {
    "instruction": "Sorting files according to size recursively",
    "response": "You can also do this with just `du`. Just to be on the safe side I'm using this version of `du`:\n$ du --version\ndu (GNU coreutils) 8.5\n\nThe approach:\n$ du -ah  | grep -v \"/$\" | sort -rh\n\nBreakdown of approach\nThe command `du -ah DIR` will produce a list of all the files and directories in a given directory `DIR`. The `-h` will produce human readable sizes which I prefer. If you don't want them then drop that switch. I'm using the `head -6` just to limit the amount of output!\n$ du -ah ~/Downloads/ | head -6\n4.4M    /home/saml/Downloads/kodak_W820_wireless_frame/W820_W1020_WirelessFrames_exUG_GLB_en.pdf\n624K    /home/saml/Downloads/kodak_W820_wireless_frame/easyshare_w820.pdf\n4.9M    /home/saml/Downloads/kodak_W820_wireless_frame/W820_W1020WirelessFrameExUG_GLB_en.pdf\n9.8M    /home/saml/Downloads/kodak_W820_wireless_frame\n8.0K    /home/saml/Downloads/bugs.xls\n604K    /home/saml/Downloads/netgear_gs724t/GS7xxT_HIG_5Jan10.pdf\n\nEasy enough to sort it smallest to biggest:\n$ du -ah ~/Downloads/ | sort -h | head -6\n0   /home/saml/Downloads/apps_archive/monitoring/nagios/nagios-check_sip-1.3/usr/lib64/nagios/plugins/check_ldaps\n0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/0/index/write.lock\n0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/0/translog/translog-1365292480753\n0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/1/index/write.lock\n0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/1/translog/translog-1365292480946\n0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/2/index/write.lock\n\nReverse it, biggest to smallest:\n$ du -ah ~/Downloads/ | sort -rh | head -6\n10G /home/saml/Downloads/\n3.8G    /home/saml/Downloads/audible/audio_books\n3.8G    /home/saml/Downloads/audible\n2.3G    /home/saml/Downloads/apps_archive\n1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip\n1.5G    /home/saml/Downloads/digital_blasphemy\n\nDon't show me the directory, just the files:\n$ du -ah ~/Downloads/ | grep -v \"/$\" | sort -rh | head -6 \n3.8G    /home/saml/Downloads/audible/audio_books\n3.8G    /home/saml/Downloads/audible\n2.3G    /home/saml/Downloads/apps_archive\n1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip\n1.5G    /home/saml/Downloads/digital_blasphemy\n835M    /home/saml/Downloads/apps_archive/cad_cam_cae/salome/Salome-V6_5_0-LGPL-x86_64.run\n\nIf you want to exclude all directories from the output, you can use a trick with the presence of a dot character. This assumes that your directory names do not contain dots, and that the files you are looking for do. Then you can filter out the directories with `grep -v '\\s/[^.]*$'`:\n$ du -ah ~/Downloads/ | grep -v '\\s/[^.]*$' | sort -rh | head -2\n1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip\n835M    /home/saml/Downloads/apps_archive/cad_cam_cae/salome/Salome-V6_5_0-LGPL-x86_64.run\n\nIf you just want the list of smallest to biggest, but the top 6 offending files you can reverse the sort switch, drop (`-r`), and use `tail -6` instead of the `head -6`.\n$ du -ah ~/Downloads/ | grep -v \"/$\" | sort -h | tail -6\n835M    /home/saml/Downloads/apps_archive/cad_cam_cae/salome/Salome-V6_5_0-LGPL-x86_64.run\n1.5G    /home/saml/Downloads/digital_blasphemy\n1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip\n2.3G    /home/saml/Downloads/apps_archive\n3.8G    /home/saml/Downloads/audible\n3.8G    /home/saml/Downloads/audible/audio_books",
    "question_score": 135,
    "answer_score": 168,
    "question_id": 88065,
    "has_full_answer": true,
    "tags": [
      "command-line",
      "find",
      "ls"
    ],
    "topic": "find"
  },
  {
    "instruction": "Piping commands after a piped xargs",
    "response": "You are almost there. In your last command, you can use `-I` to do the `ls` correctly\n\n  `-I replace-str`\n                Replace  occurrences of replace-str in the initial-arguments with names read from standard input.  Also, unquoted blanks do not terminate input items; instead the separator is the newline character.  Implies `-x` and `-L 1`.\n\nSo, with\n\nfind . -type d -name \"*log*\" | xargs -I {} sh -c \"echo {}; ls -la {} | tail -2\"\n\nyou will `echo` the dir found by `find`, then do the `ls | tail` on it.",
    "question_score": 92,
    "answer_score": 143,
    "question_id": 209249,
    "has_full_answer": true,
    "tags": [
      "shell",
      "find",
      "pipe",
      "xargs"
    ],
    "topic": "find"
  },
  {
    "instruction": "What's the quickest way to find duplicated files?",
    "response": "You can make it shorter:\n\nfind . ! -empty -type f -exec md5sum {} + | sort | uniq -w32 -dD\n\nDo `md5sum` of found files on the `-exec` action of `find` and then `sort` and do `uniq` to get the files having same the `md5sum` separated by newline.",
    "question_score": 100,
    "answer_score": 142,
    "question_id": 277697,
    "has_full_answer": true,
    "tags": [
      "centos",
      "files",
      "find",
      "duplicate-files"
    ],
    "topic": "find"
  },
  {
    "instruction": "Why is my find not recursive?",
    "response": "The problem is, you didn't quote your `-name` parameter. Do this instead:\n\nfind . -name '*.java'\n\nExplanation\n\nWithout the quotes, the shell interprets `*.java` as a glob pattern and expands it to any file names matching the glob before passing it to `find`. This way, if you had, say, `foo.java` in the current directory, `find`'s actual command line would be:\n\nfind . -name foo.java\n\nwhich would obviously list the file in the current directory only (unless you happen to have some similarly-named files further down the tree).\n\nQuoting prevents glob expansion and passes the command line to `find` as-is.\n\nIncidentally, if the glob had failed to match (no `*.java` files in the current directory), you would get one of two behaviors depending on how your shell is set up to handle globs that don't match (this is governed by the `nullglob` option in Bash, for example):\n\nIf a glob that doesn't match is not expanded by the shell, `find` will (accidentally, mind you) exhibit correct behavior.\nIf a glob that doesn't match is expanded into an empty string by the shell, `find` will complain that it is missing an argument to `-name`.",
    "question_score": 91,
    "answer_score": 120,
    "question_id": 123440,
    "has_full_answer": true,
    "tags": [
      "shell",
      "find",
      "wildcards"
    ],
    "topic": "find"
  },
  {
    "instruction": "Bash: How to read one line at a time from output of a command?",
    "response": "There's a mistake, you need `\n\n`Process Substitution, `$()` is a command substitution and `here-string.",
    "question_score": 91,
    "answer_score": 111,
    "question_id": 52026,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell",
      "find",
      "pipe"
    ],
    "topic": "find"
  },
  {
    "instruction": "Sorting the output of \"find -print0\" by piping to the \"sort\" command",
    "response": "Use `find` as usual and delimit your lines with NUL. GNU `sort` can handle these with the -z switch:\n\nfind . -print0 | sort -z | xargs -r0 yourcommand",
    "question_score": 141,
    "answer_score": 107,
    "question_id": 34325,
    "has_full_answer": true,
    "tags": [
      "shell",
      "find",
      "sort",
      "xargs"
    ],
    "topic": "find"
  },
  {
    "instruction": "Why does find -mtime +1 only return files older than 2 days?",
    "response": "Well, the simple answer is, I guess, that your find implementation is following the POSIX/SuS standard, which says it must behave this way. Quoting from SUSv4/IEEE Std 1003.1, 2013 Edition, \"find\":\n\n  -mtime  n\n       The primary shall evaluate as true if the file modification time subtracted\n       from the initialization time, divided by 86400 (with any remainder discarded), is n.\n\n(Elsewhere in that document it explains that `n` can actually be `+n`, and the meaning of that as \"greater than\").\n\nAs to why the standard says it shall behave that way—well, I'd guess long in the past a programmer was lazy or not thinking about it, and just wrote the C code `(current_time - file_time) / 86400`. C integer arithmetic discards the remainder. Scripts started depending on that behavior, and thus it was standardized.\n\nThe spec'd behavior would also be portable to a hypothetical system that only stored a modification date (not time). I don't know if such a system has existed.",
    "question_score": 168,
    "answer_score": 106,
    "question_id": 92346,
    "has_full_answer": true,
    "tags": [
      "find",
      "timestamps"
    ],
    "topic": "find"
  },
  {
    "instruction": "Get list of subdirectories which contain a file whose name contains a string",
    "response": "find . -type f -name '*f*' | sed -r 's|/[^/]+$||' |sort |uniq\n\nThe above finds all files below the current directory (`.`) that are regular files (`-type f`) and have `f` somewhere in their name (`-name '*f*'`).  Next, `sed` removes the file name, leaving just the directory name.  Then, the list of directories is sorted (`sort`) and duplicates removed (`uniq`).\nThe `sed` command consists of a single substitute.  It looks for matches to the regular expression `/[^/]+$` and replaces anything matching that with nothing.  The dollar sign means the end of the line.  `[^/]+'` means one or more characters that are not slashes.  Thus, `/[^/]+$` means all characters from the final slash to the end of the line.  In other words, this matches the file name at the end of the full path.  Thus, the sed command removes the file name, leaving unchanged the name of directory that the file was in.\nSimplifications\nMany modern `sort` commands support a `-u` flag which makes `uniq` unnecessary.   For GNU sed:\nfind . -type f -name '*f*' | sed -r 's|/[^/]+$||' |sort -u\n\nAnd, for MacOS sed:\nfind . -type f -name '*f*' | sed -E 's|/[^/]+$||' |sort -u\n\nAlso, if your `find` command supports it, it is possible to have `find` print the directory names directly.  This avoids the need for `sed`:\nfind . -type f -name '*f*' -printf '%h\\n' | sort -u\n\nMore robust version (Requires GNU tools)\nThe above versions will be confused by file names that include newlines.  A more robust solution is to do the sorting on NUL-terminated strings:\nfind . -type f -name '*f*' -printf '%h\\0' | sort -zu | sed -z 's/$/\\n/'\n\nSimplified using dirname\nImagine needing the command in a script where command will be in single quotes, escaping sed command is painful and less than ideal, so replace with dirname.\nIssues regard special chars and newline are also mute if you did not need to sort or directories names are not affected.\nfind . -type f -name \"*f*\" -exec dirname \"{}\" \\; |sort -u\n\ntake care of newline issue:\nfind . -type f -name \"*f*\" -exec dirname -z \"{}\" \\; |sort -zu |sed -z 's/$/\\n/'",
    "question_score": 97,
    "answer_score": 95,
    "question_id": 111949,
    "has_full_answer": true,
    "tags": [
      "command-line",
      "find"
    ],
    "topic": "find"
  },
  {
    "instruction": "Getting size with du of files only",
    "response": "I usually use the `-exec` utility. Like this:\n\nfind . -type f -exec du -a {} +\n\nI tried it both on bash and ksh with GNU find. I never tried AIX, but I'm sure your version of find has some `-exec` syntax.\n\nThe following snippet sorts the list, largest first:\n\nfind . -type f -exec du -a {} + | sort -n -r | less",
    "question_score": 88,
    "answer_score": 94,
    "question_id": 22432,
    "has_full_answer": true,
    "tags": [
      "files",
      "find",
      "disk-usage",
      "aix",
      "ksh"
    ],
    "topic": "find"
  },
  {
    "instruction": "Shell command to tar directory excluding certain files/folders",
    "response": "You can have multiple exclude options for tar so\n\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\netc will work. Make sure to put `--exclude` before the source and destination items.",
    "question_score": 1139,
    "answer_score": 1476,
    "question_id": 984204,
    "has_full_answer": true,
    "tags": [
      "linux",
      "shell",
      "archive",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Tar a directory, but don't store full absolute paths in the archive",
    "response": "tar -cjf site1.tar.bz2 -C /var/www/site1 .\n\nIn the above example, tar will change to directory `/var/www/site1` before doing its thing because the option `-C /var/www/site1` was given.\n\nFrom `man tar`:\n\nOTHER OPTIONS\n\n  -C, --directory DIR\n       change to directory DIR",
    "question_score": 501,
    "answer_score": 624,
    "question_id": 18681595,
    "has_full_answer": true,
    "tags": [
      "linux",
      "bash",
      "backup",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "What is the difference between tar and zip?",
    "response": "`tar` in itself just bundles files together (the result is called a tarball), while `zip` applies compression as well.\nUsually you use `gzip` along with `tar` to compress the resulting tarball, thus achieving similar results as with `zip`.\nFor reasonably large archives there are important differences though.  A `zip` archive is a collection of compressed files.  A gzipped tar is a compressed collection (of uncompressed files).  Thus a zip archive is a randomly accessible list of concatenated compressed items, and a `.tar.gz` is an archive that must be fully expanded before the catalog is accessible.\n\nThe caveat of a `zip` is that you don't get compression across files (because each file is compressed independent of the others in the archive, the compression cannot take advantage of similarities among the contents of different files); the advantage is that you can access any of the files contained within by looking at only a specific (target file dependent) section of the archive (as the \"catalog\" of the collection is separate from the collection itself).\nThe caveat of a `.tar.gz` is that you must decompress the whole archive to access files contained therein (as the files are within the tarball); the advantage is that the compression can take advantage of similarities among the files (as it compresses the whole tarball).",
    "question_score": 289,
    "answer_score": 504,
    "question_id": 10540935,
    "has_full_answer": true,
    "tags": [
      "unix",
      "compression",
      "zip",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Utilizing multi core for tar+gzip/bzip compression/decompression",
    "response": "You can use pigz instead of gzip, which does gzip compression on multiple cores.  Instead of using the -z option, you would pipe it through pigz:\n\ntar cf - paths-to-archive | pigz > archive.tar.gz\n\nBy default, pigz uses the number of available cores, or eight if it could not query that.  You can ask for more with -p n, e.g. -p 32.  pigz has the same options as gzip, so you can request better compression with -9.  E.g.\n\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz",
    "question_score": 323,
    "answer_score": 425,
    "question_id": 12313242,
    "has_full_answer": true,
    "tags": [
      "gzip",
      "tar",
      "bzip2",
      "bzip"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How do I turn off the output from tar commands on Unix?",
    "response": "Just drop the option `v`.\n\n`-v` is for verbose. If you don't use it then it won't display:\n\ntar -zxf tmp.tar.gz -C ~/tmp1",
    "question_score": 224,
    "answer_score": 416,
    "question_id": 13341702,
    "has_full_answer": true,
    "tags": [
      "unix",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Create a tar.xz in one command",
    "response": "Use the `-J` compression option for `xz`. And remember to `man tar` :)\n\ntar cfJ  \n\nEdit 2015-08-10:\n\nIf you're passing the arguments to `tar` with dashes (ex: `tar -cf` as opposed to `tar cf`), then the `-f` option must come last, since it specifies the filename (thanks to @A-B-B for pointing that out!). In that case, the command looks like:\n\ntar -cJf",
    "question_score": 251,
    "answer_score": 393,
    "question_id": 18855850,
    "has_full_answer": true,
    "tags": [
      "compression",
      "archive",
      "tar",
      "xz"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Tar archiving that takes input from a list of files",
    "response": "Yes:\n\ntar -cvf allfiles.tar -T mylist.txt",
    "question_score": 253,
    "answer_score": 382,
    "question_id": 8033857,
    "has_full_answer": true,
    "tags": [
      "linux",
      "unix",
      "archive",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How do I tar a directory of files and folders without including the directory itself?",
    "response": "cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd - \n\nshould do the job in one line. It works well for hidden files as well. \"*\" doesn't expand hidden files by path name expansion at least in bash. Below is my experiment:\n\n$ mkdir my_directory\n$ touch my_directory/file1\n$ touch my_directory/file2\n$ touch my_directory/.hiddenfile1\n$ touch my_directory/.hiddenfile2\n$ cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd ..\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n$ tar ztf my_dir.tgz\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2",
    "question_score": 560,
    "answer_score": 276,
    "question_id": 939982,
    "has_full_answer": true,
    "tags": [
      "archive",
      "tar",
      "gzip"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Are tar.gz and tgz the same thing?",
    "response": "I think in the old package repo days, `.tgz` was used because files on DOS floppies could only have three letter extensions. When this limitation was removed, `.tar.gz` was used to be more verbose by showing both the archive type (`tar`) and zipper (`gzip`).\nThey are identical.",
    "question_score": 173,
    "answer_score": 273,
    "question_id": 11534918,
    "has_full_answer": true,
    "tags": [
      "linux",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Excluding directory when creating a .tar.gz file",
    "response": "Try removing the last / at the end of the directory path to exclude\ntar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\"",
    "question_score": 152,
    "answer_score": 240,
    "question_id": 4290174,
    "has_full_answer": true,
    "tags": [
      "linux",
      "gzip",
      "tar",
      "compression"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Find files and tar them (with spaces)",
    "response": "Use this:\n\nfind . -type f -print0 | tar -czvf backup.tar.gz --null -T -\n\nIt will:\n\ndeal with files with spaces, newlines, leading dashes, and other funniness\nhandle an unlimited number of files\nwon't repeatedly overwrite your backup.tar.gz like using `tar -c` with `xargs` will do when you have a large number of files\n\nAlso see:\n\nGNU tar manual\nHow can I build a tar from stdin?, search for null",
    "question_score": 122,
    "answer_score": 233,
    "question_id": 5891866,
    "has_full_answer": true,
    "tags": [
      "linux",
      "find",
      "backup",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to extract filename.tar.gz file",
    "response": "If `file filename.tar.gz` gives this message: POSIX tar archive, \nthe archive is a tar, not a GZip archive.\n\nUnpack a tar without the `z`, it is for gzipped (compressed), only:\n\nmv filename.tar.gz filename.tar # optional\ntar xvf filename.tar\n\nOr try a generic Unpacker like `unp` (https://packages.qa.debian.org/u/unp.html), a script for unpacking a wide variety of archive formats.\n\ndetermine the file type: \n\n$ file ~/Downloads/filename.tbz2\n/User/Name/Downloads/filename.tbz2: bzip2 compressed data, block size = 400k",
    "question_score": 133,
    "answer_score": 199,
    "question_id": 15744023,
    "has_full_answer": true,
    "tags": [
      "linux",
      "file",
      "gzip",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to check if a Unix .tar.gz file is a valid file without uncompressing?",
    "response": "What about just getting a listing of the tarball and throw away the output, rather than decompressing the file?\ntar -tzf my_tar.tar.gz >/dev/null\n\nEdit as per comment. Thanks Frozen Flame! This test in no way implies integrity of the data. Because it was designed as a tape archival utility most implementations of tar will allow multiple copies of the same file!",
    "question_score": 148,
    "answer_score": 167,
    "question_id": 2001709,
    "has_full_answer": true,
    "tags": [
      "gzip",
      "validation",
      "tar",
      "gunzip"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to uncompress a tar.gz in another directory",
    "response": "gzip -dc archive.tar.gz | tar -xf - -C /destination\n\nor, with GNU tar\n\ntar xzf archive.tar.gz -C /destination",
    "question_score": 305,
    "answer_score": 123,
    "question_id": 18402395,
    "has_full_answer": true,
    "tags": [
      "unix",
      "tar",
      "gzip"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How do I tar a directory without retaining the directory structure?",
    "response": "cd /home/username/drupal/sites/default/files\ntar czf ~/backup.tgz *",
    "question_score": 124,
    "answer_score": 46,
    "question_id": 5695881,
    "has_full_answer": true,
    "tags": [
      "linux",
      "unix",
      "gnu",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Create a .tar.bz2 file Linux",
    "response": "You are not indicating what to include in the archive.\nGo one level outside your folder and try:\nsudo tar -cvjSf folder.tar.bz2 folder\n\nOr from the same folder try\nsudo tar -cvjSf folder.tar.bz2 *",
    "question_score": 113,
    "answer_score": 215,
    "question_id": 23246946,
    "has_full_answer": true,
    "tags": [
      "linux",
      "compression",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to tar certain file types in all subdirectories?",
    "response": "`find ./someDir -name \"*.php\" -o -name \"*.html\" | tar -cf my_archive -T -`",
    "question_score": 122,
    "answer_score": 207,
    "question_id": 18731603,
    "has_full_answer": true,
    "tags": [
      "linux",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How do I extract files without folder structure using tar",
    "response": "You can use the --strip-components option of tar.\n\n --strip-components count\n         (x mode only) Remove the specified number of leading path ele-\n         ments.  Pathnames with fewer elements will be silently skipped.\n         Note that the pathname is edited after checking inclusion/exclu-\n         sion patterns but before security checks.\n\nI create a tar file with a similar structure to yours:\n\n$tar -tf tarfolder.tar\ntarfolder/\ntarfolder/file.a\ntarfolder/file.b\n\n$ls -la file.*\nls: file.*: No such file or directory\n\nThen extracted by doing:\n\n$tar -xf tarfolder.tar --strip-components 1\n$ls -la file.*\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.a\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.b",
    "question_score": 116,
    "answer_score": 191,
    "question_id": 14295771,
    "has_full_answer": true,
    "tags": [
      "linux",
      "unix",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "reading tar file contents without untarring it, in python script",
    "response": "you can use `getmembers()`\n\n>>> import  tarfile\n>>> tar = tarfile.open(\"test.tar\")\n>>> tar.getmembers()\n\nAfter that, you can use `extractfile()` to extract the members as file object. Just an example\n\nimport tarfile,os\nimport sys\nos.chdir(\"/tmp/foo\")\ntar = tarfile.open(\"test.tar\")\nfor member in tar.getmembers():\n```bash\nf=tar.extractfile(member)\ncontent=f.read()\nprint \"%s has %d newlines\" %(member, content.count(\"\\n\"))\nprint \"%s has %d spaces\" % (member,content.count(\" \"))\nprint \"%s has %d characters\" % (member, len(content))\nsys.exit()\n```\ntar.close()\n\nWith the file object `f` in the above example, you can use `read()`, `readlines()` etc.",
    "question_score": 119,
    "answer_score": 159,
    "question_id": 2018512,
    "has_full_answer": true,
    "tags": [
      "python",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Uncompress tar.gz file",
    "response": "Use `-C` option of tar:\n\ntar zxvf .tar.gz -C /usr/src/\n\nand then, the content of the tar should be in:\n\n/usr/src/",
    "question_score": 90,
    "answer_score": 152,
    "question_id": 16356460,
    "has_full_answer": true,
    "tags": [
      "linux",
      "ubuntu",
      "debian",
      "gzip",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How can I build a tar from stdin?",
    "response": "Something like:\ntar cfz foo.tgz --files-from=-\n\nBut keep in mind that this won't work for all possible filenames; you should consider the `--null` option and feed `tar` from `find -print0`.  (The `xargs` example won't quite work for large file lists because it will spawn multiple `tar` commands.)",
    "question_score": 83,
    "answer_score": 119,
    "question_id": 2597875,
    "has_full_answer": true,
    "tags": [
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to extract tar archive from stdin?",
    "response": "Use `-` as the input file:\n\ncat largefile.tgz.aa largefile.tgz.ab | tar zxf -\n\nMake sure you cat them in the same order they were split.\n\nIf you're using zsh you can use the multios feature and avoid invoking cat:\n\n< largefile.tgz.aa < largefile.tgz.ab tar zxf -\n\nOr if they are in alphabetical order:\n\n<largefile.tgz.* | tar zxf -",
    "question_score": 73,
    "answer_score": 119,
    "question_id": 11524602,
    "has_full_answer": true,
    "tags": [
      "pipe",
      "stdin",
      "tar",
      "pipeline"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to extract a single file from tar to a different directory?",
    "response": "The problem is that your arguments are in incorrect order. The single file argument must be last.\n\nE.g. \n\n$ tar xvf test.tar -C anotherDirectory/ testfile1\n\nshould do the trick.\n\nPS: You should have asked this question on superuser instead of SO",
    "question_score": 81,
    "answer_score": 116,
    "question_id": 9249603,
    "has_full_answer": true,
    "tags": [
      "unix",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "Deleting files after adding to tar archive",
    "response": "With GNU tar, use the option `--remove-files`.",
    "question_score": 78,
    "answer_score": 111,
    "question_id": 10781609,
    "has_full_answer": true,
    "tags": [
      "bash",
      "unix",
      "compression",
      "tar"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to create tar.gz archive file in Windows?",
    "response": "tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows.\n\nIf you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip.",
    "question_score": 78,
    "answer_score": 97,
    "question_id": 10773880,
    "has_full_answer": true,
    "tags": [
      "windows",
      "gzip",
      "cpanel",
      "tar",
      "7zip"
    ],
    "topic": "tar"
  },
  {
    "instruction": "How to force ssh client to use only password auth?",
    "response": "Disable PubkeyAuthentication and also set PreferredAuthentications to `password` so that alternative methods like gssapi-with-mic aren't used:\nssh -o PubkeyAuthentication=no -o PreferredAuthentications=password  example.com\n\nYou need to make sure that the client isn't configured to disallow password authentication.",
    "question_score": 699,
    "answer_score": 1092,
    "question_id": 15138,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "password",
      "key-authentication"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Why am I still getting a password prompt with ssh with public key authentication?",
    "response": "Make sure the permissions on the `~/.ssh` directory and its contents are proper. When I first set up my ssh key auth, I didn't have the `~/.ssh` folder properly set up, and it yelled at me.\n\nYour home directory `~`, your `~/.ssh` directory and the `~/.ssh/authorized_keys` file on the remote machine must be writable only by you: `rwx------` and `rwxr-xr-x` are fine, but `rwxrwx---` is no good¹, even if you are the only user in your group (if you prefer numeric modes: `700` or `755`, not `775`).\nIf `~/.ssh` or `authorized_keys` is a symbolic link, the canonical path (with symbolic links expanded) is checked.\nYour `~/.ssh/authorized_keys` file (on the remote machine) must be readable (at least 400), but you'll need it to be also writable (600) if you will add any more keys to it.\nYour private key file (on the local machine) must be readable and writable only by you: `rw-------`, i.e. `600`.\nAlso, if SELinux is set to enforcing, you may need to run `restorecon -R -v ~/.ssh` (see e.g. Ubuntu bug 965663 and Debian bug report #658675; this is patched in CentOS 6).\n\n¹  Except on some distributions (Debian and derivatives) which have patched the code to allow group writability if you are the only user in your group.",
    "question_score": 739,
    "answer_score": 872,
    "question_id": 36540,
    "has_full_answer": true,
    "tags": [
      "ubuntu",
      "centos",
      "ssh",
      "sshd",
      "key-authentication"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Scroll inside Screen, or Pause Output",
    "response": "Screen has its own scroll buffer, as it is a terminal multiplexer and has to deal with several buffers.\nMaybe there's a better way, but I'm used to scrolling using the \"copy mode\" (which you can use to copy text using screen itself, although that requires the paste command too):\n\nHit your screen prefix combination (`C-a` / control+A by default), then hit Escape or [.\n\nMove up and down with the arrow keys (↑ and ↓).\n\nWhen you're done, hit any key except arrow keys, numbers, and certain letters to get back to the end of the scroll buffer. Most people use q or Escape\n\n(If instead of exiting you press Enter or Space and then move the cursor, you will start selecting text to copy, and pressing Enter or Space a second time will copy it. Then you can paste with C-a followed by ].)\nOf course, you can always use `more` and `less`, two commonly used pagers, which may be enough for some commands.",
    "question_score": 594,
    "answer_score": 814,
    "question_id": 40242,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "terminal",
      "gnu-screen",
      "putty",
      "scrolling"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "What's ssh port forwarding and what's the difference between ssh local and remote port forwarding",
    "response": "I have drawn some sketches\n\nIntroduction\n\nlocal: `-L Specifies that the given port on the local (client) host is to be forwarded to the given host and port on the remote side.`\n`ssh -L sourcePort:forwardToHost:onPort connectToHost` means: connect with ssh to `connectToHost`, and forward all connection attempts to the local `sourcePort` to port `onPort` on the machine called `forwardToHost`, which can be reached from the `connectToHost` machine.\n\nremote: `-R Specifies that the given port on the remote (server) host is to be forwarded to the given host and port on the local side.`\n`ssh -R sourcePort:forwardToHost:onPort connectToHost` means: connect with ssh to `connectToHost`, and forward all connection attempts to the remote `sourcePort` to port `onPort` on the machine called `forwardToHost`, which can be reached from your local machine.\n\nExamples\nExample for 1\nssh -L 80:localhost:80 SUPERSERVER\n\nYou specify that a connection made to the local port 80 is to be forwarded to port 80 on SUPERSERVER. That means if someone connects to your computer with a webbrowser, he gets the response of the webserver running on SUPERSERVER. You, on your local machine, have no webserver running.\nExample for 2\nssh -R 80:localhost:80 tinyserver\n\nYou specify, that a connection made to the port 80 of tinyserver is to be forwarded to port 80 on your local machine. That means if someone connects to the small and slow server with a webbrowser, he gets the response of the webserver running on your local machine. The tinyserver, which has not enough diskspace for the big website, has no webserver running. But people connecting to tinyserver think so.\nMore examples\nOther things could be: The powerful machine has five webservers running on five different ports. If a user connects to one of the five tinyservers at port 80 with his webbrowser, the request is redirected to the corresponding webserver running on the powerful machine. That would be\nssh -R 80:localhost:30180 tinyserver1\nssh -R 80:localhost:30280 tinyserver2\netc.\n\nOr maybe your machine is only the connection between the powerful and the small servers. Then it would be (for one of the tinyservers that play to have their own webservers):\nssh -R 80:SUPERSERVER:30180 tinyserver1\nssh -R 80:SUPERSERVER:30280 tinyserver2\netc",
    "question_score": 310,
    "answer_score": 777,
    "question_id": 115897,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "forwarding"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Specify identity file (id_rsa) with rsync",
    "response": "You can specify the exact ssh command via the '-e' option:\n\nrsync -Pav -e \"ssh -i $HOME/.ssh/somekey\" username@hostname:/from/dir/ /to/dir/\n\nMany ssh users are unfamiliar with their ~/.ssh/config file. You can specify default settings per host via the config file.\n\nHost hostname\n```bash\nUser username\nIdentityFile ~/.ssh/somekey\n```\n\nIn the long run it is best to learn the ~/.ssh/config file.",
    "question_score": 429,
    "answer_score": 712,
    "question_id": 127352,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "rsync"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How to forward X over SSH to run graphics applications remotely?",
    "response": "X11 forwarding needs to be enabled on both the client side and the server side.\nOn the client side, the `-X` (capital X) option to `ssh` enables X11 forwarding, and you can make this the default (for all connections or for a specific connection) with `ForwardX11 yes` in `~/.ssh/config`.\nOn the server side, `X11Forwarding yes` must be specified in `/etc/ssh/sshd_config`. Note that the default is no forwarding (some distributions turn it on in their default `/etc/ssh/sshd_config`), and that the user cannot override this setting. If you change the configuration, remember to tell the server to reload its configuration, e.g. `service ssh reload` if your system uses systemd.\nThe `xauth` program must be installed on the server side. If there are any X11 programs there, it's very likely that `xauth` will be there. In the unlikely case `xauth` was installed in a nonstandard location, it can be called through `~/.ssh/rc` (on the server!).\nNote that you do not need to set any environment variables on the server. `DISPLAY` and `XAUTHORITY` will automatically be set to their proper values. If you run ssh and `DISPLAY` is not set, it means ssh is not forwarding the X11 connection.\nTo confirm that ssh is forwarding X11, check for a line containing `Requesting X11 forwarding` in the output of `ssh -v -X`. Note that the server won't reply either way, a security precaution of hiding details from potential attackers.",
    "question_score": 585,
    "answer_score": 682,
    "question_id": 12755,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "xorg",
      "xforwarding"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How does reverse SSH tunneling work?",
    "response": "I love explaining this kind of thing through visualization.  :-)\nThink of your SSH connections as tubes.  Big tubes.  Normally, you'll reach through these tubes to run a shell on a remote computer.  The shell runs in a virtual terminal (tty) through that tube.  But you know this part already.\nThink of your tunnel as another tube within a tube.  You still have the big SSH connection, but the -L or -R option lets you set up a smaller tube inside it.\nYour ssh remote shell actually communicates with you using one of these smaller, embedded tubes attached to stdio.\nEvery tube has a beginning and an end.  The big tube, your SSH connection, started with your SSH client and ends up at the SSH server you connected to.  All the smaller tubes have the same endpoints, except that the role of \"start\" or \"end\" is determined by whether you used `-L` or `-R` (respectively) to create them.\n(You haven't said, but I'm going to assume that the \"remote\" machine you've mentioned, the one behind the firewall, can access the Internet using Network Address Translation (NAT).  This is kind of important, so please correct this assumption if it is false.)\nWhen you create a tunnel, you specify an address and port on which it will answer (or \"bind\"), and an address and port to which it will be delivered.  The `-L` option tells the tunnel to bind on the local side of the tunnel (the host running your client).  The `-R` option tells the tunnel to bind on the remote side (the SSH server).\n\nSo...  To be able to SSH from the Internet into a host behind a firewall, you need the target host to open an SSH connection to a host on the outside and include a `-R` tunnel whose \"entry\" point is the \"remote\" side of its connection.\nOf the two models shown above, you want the one on the right.\nFrom the firewalled host:\nssh -f -N -T -R22222:localhost:22 yourpublichost.example.com\n\nThis tells the client on your target host to establish a tunnel with a `-R`emote entry point.  Anything that attaches to port 22222 on the far end of the tunnel will actually reach \"localhost port 22\", where \"localhost\" is from the perspective of the exit point of the tunnel (i.e. your ssh client in this case, on the target host).\nThe other options are:\n\n`-f` tells ssh to background itself after it authenticates, so you don't have to sit around running something like `sleep` on the remote server for the tunnel to remain alive.\n`-N` says that you want an SSH connection, but you don't actually want to run any remote commands. If all you're creating is a tunnel, then including this option saves resources.\n`-T` disables pseudo-tty allocation, which is appropriate because you're not trying to create an interactive shell.\n\nThere will be a password challenge unless you have set up a key for a passwordless login.\n(Note that if you intend to leave a connection open long term, unattended, possibly having it automatically refresh the connection when it goes down (by parsing `ssh -O check `), I recommend using a separate, unique SSH key for it that you set up for just this tunnel/customer/server, especially if you are using RemoteForward. Trust no one.)\nNow that the -R service tunnel is active, you can connect to it from yourpublichost, establish a connection to the firewalled host through the tunnel:\nssh -p 22222 username@localhost\n\nYou'll get a host key challenge, as you've probably never hit this host before.  Then you'll get a password challenge for the `username` account (unless you've set up keys for passwordless login).\nIf you're going to be accessing this host on a regular basis, you can also simplify access by adding a few lines to your `~/.ssh/config` file on `yourpublichost`:\nhost firewalledhost\n```bash\nUser firewalleduser\nHostname localhost\nPort 22222\n```\n\nAdjust `firewalledhost` and `firewalleduser` to suit.  The `firewalleduser` field must match your username on the remote server, but `firewalledhost` can be any name that suits you, the name doesn't have to match anything resolvable, since your connection is governed by `Hostname` and `Port`.\nAlternately, if you want to reach this from elsewhere on the Internet, you might add the following to your `~/.ssh/config`:\nhost firewalledhost\n```bash\nProxyCommand ssh -fWlocalhost:22222 yourpublichost        \n```\n\nThe `-W` option is used to open a connection to a remote host in order to continue the SSH conversation. It implies `-N` and `-T`.\nSee also:\n\nExpose the reverse endpoint on a non-localhost IP\nTips on using `ControlMaster` to maintain your tunnel",
    "question_score": 497,
    "answer_score": 566,
    "question_id": 46235,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "networking",
      "firewall",
      "ssh-tunneling",
      "remote-management"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How to list keys added to ssh-agent with ssh-add?",
    "response": "Use `ssh-add -l` to list them by their fingerprints.\n$ ssh-add -l\n2048 72:...:eb /home/gert/.ssh/mykey (RSA)\n\nOr `ssh-add -L` to get the full key in OpenSSH format.\n$ ssh-add -L\nssh-rsa AAAAB3NzaC1yc[...]B63SQ== /home/gert/.ssh/id_rsa\n\nThe latter format is the same as you would put them in a `~/.ssh/authorized_keys` file.",
    "question_score": 458,
    "answer_score": 544,
    "question_id": 58969,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "key-authentication",
      "ssh-agent"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How to avoid being asked passphrase each time I push to Bitbucket",
    "response": "You need to use an ssh agent.  Short answer: try \n\n$ ssh-add\n\nbefore pushing.  Supply your passphrase when asked.\n\nIf you aren't already running an ssh agent you will get the following message:\n\nCould not open a connection to your authentication agent.\n\nIn that situation, you can start one and set your environment up thusly\n\neval $(ssh-agent)\n\nThen repeat the `ssh-add` command.\n\nIt's worth taking a look at the ssh agent manpage.",
    "question_score": 303,
    "answer_score": 460,
    "question_id": 12195,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "key-authentication"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How to enable diffie-hellman-group1-sha1 key exchange on Debian 8.0?",
    "response": "The OpenSSH website has a page dedicated to legacy issues such as this one. It suggests the following approach, on the client:\n\nssh -oKexAlgorithms=+diffie-hellman-group1-sha1 123.123.123.123\n\nor more permanently, adding\n\nHost 123.123.123.123\n```bash\nKexAlgorithms +diffie-hellman-group1-sha1\n```\n\nto `~/.ssh/config`.\n\nThis will enable the old algorithms on the client, allowing it to connect to the server.",
    "question_score": 282,
    "answer_score": 451,
    "question_id": 340844,
    "has_full_answer": true,
    "tags": [
      "debian",
      "ssh",
      "openssh",
      "key-authentication",
      "ssh-agent"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "What do options `ServerAliveInterval` and `ClientAliveInterval` in sshd_config do exactly?",
    "response": "ServerAliveInterval: number of seconds that the client will wait before sending a null packet to the server (to keep the connection alive).\n\nClientAliveInterval: number of seconds that the server will wait before sending a null packet to the client (to keep the connection alive).\n\nSetting a value of 0 (the default) will disable these features so your connection could drop if it is idle for too long.\n\nServerAliveInterval seems to be the most common strategy to keep a connection alive. To prevent the broken pipe problem, here is the ssh config I use in my .ssh/config file:\n\nHost myhostshortcut\n     HostName myhost.com\n     User barthelemy\n     ServerAliveInterval 60\n     ServerAliveCountMax 10\n\nThe above setting will work in the following way,\n\nThe client will wait idle for 60 seconds (ServerAliveInterval time) and, send a \"no-op null packet\" to the server and expect a response. If no response comes, then it will keep trying the above process till 10 (ServerAliveCountMax) times (600 seconds). If the server still doesn't respond, then the client disconnects the ssh connection.\n\nClientAliveCountMax on the server side might also help. This is the limit of how long a client are allowed to stay unresponsive before being disconnected. The default value is 3, as in three ClientAliveInterval.",
    "question_score": 303,
    "answer_score": 375,
    "question_id": 3026,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "configuration"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Multiple similar entries in ssh config",
    "response": "From the `ssh_config(5)` man page:\n\n Host    Restricts the following declarations (up to the next Host key‐\n         word) to be only for those hosts that match one of the patterns\n         given after the keyword.  If more than one pattern is provided,\n         they should be separated by whitespace.\n\n  \n  ...\n\n HostName\n         Specifies the real host name to log into.  This can be used to\n         specify nicknames or abbreviations for hosts.  If the hostname\n         contains the character sequence ‘%h’, then this will be replaced\n         with the host name specified on the commandline (this is useful\n         for manipulating unqualified names).\n\nSo:\n\nHost XXX1 XXX2 XXX3\n  HostName %h.YYY.com",
    "question_score": 330,
    "answer_score": 368,
    "question_id": 61655,
    "has_full_answer": true,
    "tags": [
      "ssh"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Keep processes running after SSH session disconnects",
    "response": "Use `nohup` to make your process ignore the hangup signal:\n$ nohup long-running-process &\n$ exit\n\nTo monitor progress, check nohup's output:\n$ tail -f nohup.out",
    "question_score": 306,
    "answer_score": 347,
    "question_id": 479,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "command-line",
      "background-process"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Why do I need a tty to run sudo if I can sudo without a password?",
    "response": "That's probably because your `/etc/sudoers` file (or any file it includes) has: \n\nDefaults requiretty\n\n...which makes `sudo` require a TTY. Red Hat systems (RHEL, Fedora...) have been known to require a TTY in default `sudoers` file. That provides no real security benefit and can be safely removed.\n\nRed Hat have acknowledged the problem and it will be removed in future releases.\n\nIf changing the configuration of the server is not an option, as a work-around for that mis-configuration, you could use the `-t` or `-tt` options to `ssh` which spawns a pseudo-terminal on the remote side, but beware that it has a number of side effects.\n\n`-tt` is meant for interactive use. It puts the local terminal in `raw` mode so that you interact with the remote terminal. That means that if `ssh` I/O is not from/to a terminal, that will have side effects. For instance, all the input will be echoed back, special terminal characters (`^?`, `^C`, `^U`) will cause special processing; on output, `LF`s will be converted to `CRLF`s... (see this answer to Why is this binary file being changed? for more details.\n\nTo minimise the impact, you could invoke it as:\n\nssh -tt host 'stty raw -echo; sudo ...' < <(cat)\n\nThe `\n\nNote that since the output of the remote command will go to a terminal, that will still affect its buffering (which will be line-based for many applications) and bandwidth efficiency since `TCP_NODELAY` is on. Also with `-tt`, `ssh` sets the IPQoS to `lowdelay` as opposed to `throughput`. You could work around both with:\n\nssh -o IPQoS=throughput -tt host 'stty raw -echo; sudo cmd | cat' < <(cat)\n\nAlso, note that it means  the remote command cannot detect end-of-file  on its stdin and the stdout and stderr of the remote command are merged into a single stream.\n\nSo, not so good a work around after all.\n\nIf you've a got a way to spawn a pseudo-terminal on the remote host (like with `expect`, `zsh`, `socat`, `perl`'s `IO::Pty`...), then it would be better to use that to create the pseudo-terminal to attach `sudo` to (but not for I/O), and use `ssh` without `-t`.\n\nFor example, with `expect`:\n\nssh host 'expect -c \"spawn -noecho sh -c {\n     exec sudo cmd >&4 2>&5 &- 5>&- 6<&-}\n exit [lindex [wait] 3]\" 4>&1 5>&2 6<&0'\n\nOr with `script` (here assuming the implementation from `util-linux`):\n\nssh host 'SHELL=/bin/sh script -qec \"\n              sudo cmd &4 2>&5 3&- 5>&-\n            \" /dev/null 3&1 5>&2'\n\n(assuming (for both) that the login shell of the remote user is Bourne-like).",
    "question_score": 243,
    "answer_score": 311,
    "question_id": 122616,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "sudo",
      "tty"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Copy a file back to local system with ssh",
    "response": "Master connection\n\nIt's easiest if you plan in advance.\n\nOpen a master connection the first time. For subsequent connections, route slave connections through the existing master connection. In your `~/.ssh/config`, set up connection sharing to happen automatically:\n\nControlMaster auto\nControlPath ~/.ssh/control:%h:%p:%r\n\nIf you start an ssh session to the same (user, port, machine) as an existing connection, the second session will be tunneled over the first. Establishing the second connection requires no new authentication and is very fast.\n\nSo while you have your active connection, you can quickly:\n\ncopy a file with `scp` or `rsync`;\nmount a remote filesystem with sshfs.\n\nForwarding\n\nOn an existing connection, you can establish a reverse ssh tunnel. On the ssh command line, create a remote forwarding by passing `-R 22042:localhost:22` where 22042 is a randomly chosen number that's different from any other port number on the remote machine. Then `ssh -p 22042 localhost` on the remote machine connects you back to the source machine; you can use `scp -P 22042 foo localhost:` to copy files.\n\nYou can automate this further with `RemoteForward 22042 localhost:22`. The problem with this is that if you connect to the same computer with multiple instances of ssh, or if someone else is using the port, you don't get the forwarding.\n\nIf you haven't enabled a remote forwarding from the start, you can do it on an existing ssh session. Type Enter `~C` Enter `-R 22042:localhost:22` Enter.\nSee “Escape characters” in the manual for more information.\n\nThere is also some interesting information in this Server Fault thread.\n\nCopy-paste\n\nIf the file is small, you can type it out and copy-paste from the terminal output. If the file contains non-printable characters, use an encoding such as base64.\n\nremote.example.net$ base64 <myfile\n(copy the output)\n\nlocal.example.net$ base64 -d >myfile\n(paste the clipboard contents)\nCtrl+D\n\nMore conveniently, if you have X forwarding active, copy the file on the remote machine and paste it locally. You can pipe data in and out of `xclip` or `xsel`. If you want to preserve the file name and metadata, copy-paste an archive.\n\nremote.example.net$ tar -czf - myfile | xsel\n\nlocal.example.net$ xsel | tar -xzf -",
    "question_score": 334,
    "answer_score": 176,
    "question_id": 2857,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "file-copy"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "ssh-add complains: Could not open a connection to your authentication agent",
    "response": "Your shell is meant to evaluate that shell code output by `ssh-agent`. Run this instead:\n\neval \"$(ssh-agent)\"\n\nOr if you've started ssh-agent already, copy paste it to your shell prompt (assuming you're running a Bourne-like shell).\n\n`ssh` commands need to know how to talk to the `ssh-agent`, they know that from the `SSH_AUTH_SOCK` environment variable.",
    "question_score": 229,
    "answer_score": 333,
    "question_id": 48863,
    "has_full_answer": true,
    "tags": [
      "command-line",
      "ssh",
      "ssh-agent"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Is it possible to find out the hosts in the known_hosts file?",
    "response": "You've got `HashKnownHosts` set to \"`yes`\" in your `ssh_config` file, so the hostnames aren't available in plaintext.\nIf you know the hostname you're looking for ahead of time, you can search for it with:\nssh-keygen -H -F hostname\n# Or, if SSH runs on port other than 22. Use literal brackets [].\nssh-keygen -H -F '[hostname]:2222'\n\nHere's the relevant section from the `ssh-keygen(1)` man page:\n\n`-F` hostname\nSearch for the specified hostname in a `known_hosts` file, listing  any\noccurrences found.  This option is useful to find hashed host  names\nor addresses and may also be used in conjunction with the  `-H` option\nto print found keys in a hashed format.",
    "question_score": 210,
    "answer_score": 242,
    "question_id": 31549,
    "has_full_answer": true,
    "tags": [
      "ssh"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Is your SSH password revealed when you attempt to connect to the wrong server?",
    "response": "Simple put: yes\n\nMore detail...\n\nIf you connect to my machine then you don't know if I'm running a normal `ssh` server, or one that has been modified to write out the password being passed.\n\nFurther, I wouldn't necessarily need to modify `sshd`, but could write a PAM module (eg using `pam_script`), which will be passed your password.\n\nSo, yes.  NEVER send your password to an untrusted server.  The owner of the machine could easily have configured it to log all attempted passwords.\n\n(In fact this isn't uncommon in the infosec world; set up a honeypot server to log the passwords attempted)",
    "question_score": 194,
    "answer_score": 215,
    "question_id": 309938,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "password"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Kill an unresponsive ssh session without closing the terminal",
    "response": "One way is to use the ssh escape character.  By default this is \"~\", but it can be set manually with `-e` option when invoking ssh or via `EscapeChar` in your ssh config.  To kill the hung session this will often work:\n\n`~.`\n\nAs pointed out by Gilles this is only recognized immediately after hitting Enter.",
    "question_score": 145,
    "answer_score": 203,
    "question_id": 2919,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "terminal"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Get SSH server key fingerprint",
    "response": "You could do this by combining `ssh-keyscan` and `ssh-keygen`:\n$ file=$(mktemp)\n$ ssh-keyscan host > $file 2> /dev/null\n$ ssh-keygen -l -f $file\n521 de:ad:be:ef:de:ad:be:ef:de:ad:be:ef:de:ad:be:ef host (ECDSA)\n4096 8b:ad:f0:0d:8b:ad:f0:0d:8b:ad:f0:0d:8b:ad:f0:0d host (RSA)\n$ rm $file\n\nEdit: since OpenSSH 7.2 this oneliner works:\n`ssh-keyscan host | ssh-keygen -lf -`\n(credits to @mykhal)",
    "question_score": 198,
    "answer_score": 182,
    "question_id": 126908,
    "has_full_answer": true,
    "tags": [
      "ssh"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How can I keep my SSH sessions from freezing?",
    "response": "The changes you've made in `/etc/ssh/ssh_config` and `/etc/ssh/sshd_config` are correct but will still not have any effect.\nTo get your configuration working, make these configuration changes on the client:\n`/etc/ssh/ssh_config`\nHost *\nServerAliveInterval 100\n\nServerAliveInterval  The client will send a null packet to the server every 100 seconds to keep the connection alive\nNULL packet  Is sent by the server to the client. The same packet is sent by the client to the server. A TCP NULL packet does not contain any controlling flag like SYN, ACK, FIN etc. because the server does not require a reply from the client. The NULL packet is described here: https://www.rfc-editor.org/rfc/rfc6592\nThen configuring the sshd part on the server.\n`/etc/ssh/sshd_config`\nClientAliveInterval 60\nTCPKeepAlive yes\nClientAliveCountMax 10000\n\nClientAliveInterval The server will wait 60 seconds before sending a null packet to the client to keep the connection alive\nTCPKeepAlive Is there to ensure that certain firewalls don't drop idle connections.\nClientAliveCountMax Server will send alive messages to the client even though it has not received any message back from the client.\nFinally restart the `ssh server`\n`service ssh restart`  or  `service sshd restart` depending on what system you are on.",
    "question_score": 143,
    "answer_score": 174,
    "question_id": 200239,
    "has_full_answer": true,
    "tags": [
      "ssh"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "Why eval the output of ssh-agent?",
    "response": "`ssh-agent` outputs the environment variables you need to have to connect to it:\n\nshadur@proteus:~$ ssh-agent\nSSH_AUTH_SOCK=/tmp/ssh-492P67qzMeGA/agent.7948; export SSH_AUTH_SOCK;\nSSH_AGENT_PID=7949; export SSH_AGENT_PID;\necho Agent pid 7949;\nshadur@proteus:~$ \n\nBy calling `eval` you immediately load those variables into your environment. \n\nAs to why `ssh-agent` can't do that itself... Note the word choice. Not \"won't\", \"can't\". In Unix, a process can only modify its own environment variables, and pass them on to children. It can not modify its parent process' environment because the system won't allow it. This is pretty basic security design. \n\nYou could get around the `eval` by using `ssh-agent utility` where `utility` is your login shell, your window manager or whatever other thing needs to have the SSH environment variables set. This is also mentioned in the manual.",
    "question_score": 148,
    "answer_score": 170,
    "question_id": 351725,
    "has_full_answer": true,
    "tags": [
      "shell",
      "ssh",
      "ssh-agent",
      "eval"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "What does the Broken pipe message mean in an SSH session?",
    "response": "It's possible that your server closes connections that are idle for too long.\nYou can update either your client (`ServerAliveInterval`) or your server (`ClientAliveInterval`)\n\n ServerAliveInterval\n         Sets a timeout interval in seconds after which if no data has\n         been received from the server, ssh(1) will send a message through\n         the encrypted channel to request a response from the server.  The\n         default is 0, indicating that these messages will not be sent to\n         the server.  This option applies to protocol version 2 only.\n\n ClientAliveInterval\n         Sets a timeout interval in seconds after which if no data has\n         been received from the client, sshd(8) will send a message\n         through the encrypted channel to request a response from the\n         client.  The default is 0, indicating that these messages will\n         not be sent to the client.  This option applies to protocol\n         version 2 only.\n\nTo update your server (and restart your `sshd`)\n\necho \"ClientAliveInterval 60\" | sudo tee -a /etc/ssh/sshd_config\n\nOr client-side:\n\necho \"ServerAliveInterval 60\" >> ~/.ssh/config",
    "question_score": 187,
    "answer_score": 156,
    "question_id": 2010,
    "has_full_answer": true,
    "tags": [
      "ssh"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "SSH: tunnel without shell on ssh server",
    "response": "As said in other posts, if you don't want a prompt on the remote host, you must use the `-N` option of SSH. But this just keeps SSH running without having a prompt, and the shell busy.\n\nYou just need to put the SSH'ing as a background task with the `&` sign :\n\nssh -N -L 8080:ww.xx.yy.zz:80 user@server &\n\nThis will launch the ssh tunnelling in the background. \nBut some messages may appear, especially when you try to connect to a non-listening port (if you server apache is not launched). To avoid these messages to spawn in your shell while doing other stuff, you may redirect STDOUT/STDERR to the big void : \n\nssh -N -L 8080:ww.xx.yy.zz:80 user@server >/dev/null 2>&1 & \n\nHave fun with SSH.",
    "question_score": 137,
    "answer_score": 138,
    "question_id": 100859,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "ssh-tunneling"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "How to ssh to remote server using a private key?",
    "response": "TL;DR: To use an existing private key inline you need to select the identity file path with the parameter `-i` as follows:\nssh -i '/path/to/keyfile' username@server\n\nYou need your SSH public key and you will need your ssh private key. Keys can be generated with `ssh-keygen`.\nThe private key must be kept on Server 1 and the public key must be stored on Server 2.\nThis is completly described in the manpage of openssh, so I will quote a lot of it. You should read the section 'Authentication'. Also the openSSH manual should be really helpful: http://www.openssh.org/manual.html\nPlease be careful with ssh because this affects the security of your server.\nFrom `man ssh`:\n ~/.ssh/identity\n ~/.ssh/id_dsa\n ~/.ssh/id_rsa\n     Contains the private key for authentication.  These files contain\n     sensitive data and should be readable by the user but not acces-\n     sible by others (read/write/execute).  ssh will simply ignore a\n     private key file if it is accessible by others.  It is possible\n     to specify a passphrase when generating the key which will be\n     used to encrypt the sensitive part of this file using 3DES.\n\n ~/.ssh/identity.pub\n ~/.ssh/id_dsa.pub\n ~/.ssh/id_rsa.pub\n     Contains the public key for authentication.  These files are not\n     sensitive and can (but need not) be readable by anyone.\n\nThis means you can store your private key in your home directory in .ssh. Another possibility is to tell ssh via the `-i` parameter switch to use a special identity file.\nAlso from `man ssh`:\n -i identity_file\n     Selects a file from which the identity (private key) for RSA or\n     DSA authentication is read.  The default is ~/.ssh/identity for\n     protocol version 1, and ~/.ssh/id_rsa and ~/.ssh/id_dsa for pro-\n     tocol version 2.  Identity files may also be specified on a per-\n     host basis in the configuration file.  It is possible to have\n     multiple -i options (and multiple identities specified in config-\n     uration files).\n\nThis is for the private key. Now you need to introduce your public key on Server 2. Again a quote from `man ssh`:\n  ~/.ssh/authorized_keys\n         Lists the public keys (RSA/DSA) that can be used for logging in\n         as this user.  The format of this file is described in the\n         sshd(8) manual page.  This file is not highly sensitive, but the\n         recommended permissions are read/write for the user, and not\n         accessible by others.\n\nThe easiest way to achive that is to copy the file to Server 2 and append it to the authorized_keys file:\nscp -p your_pub_key.pub user@host:\nssh user@host\nhost$ cat id_dsa.pub >> ~/.ssh/authorized_keys\n\nAuthorisation via public key must be allowed for the ssh daemon, see `man ssh_config`. Usually this can be done by adding the following statement to the config file:\nPubkeyAuthentication yes",
    "question_score": 162,
    "answer_score": 136,
    "question_id": 23291,
    "has_full_answer": true,
    "tags": [
      "ssh",
      "openssh"
    ],
    "topic": "ssh"
  },
  {
    "instruction": "No module named pip in venv but pip installed",
    "response": "I ran into the exact same problem after installing Python 3.13 on WSL. Suddenly, all my existing virtual environments (created with Python 3.12) broke in VSCode. I was getting the \"Invalid Python interpreter\" error, Pylance couldn't resolve any imports, and pip appeared to be missing—even though I could see it in the `venv/bin` folder.\nHere’s what fixed it for me:\nFirst, check what your system `python3` now points to:\npython3 --version\nwhich python3\n\nIn my case, it was now Python 3.13, which explains why stuff started breaking. Your virtual environment still points to the Python 3.12 binary internally, but VSCode (and maybe even pip) is trying to use 3.13 instead.\nYou can confirm that by looking at the `pyvenv.cfg` file inside your venv:\ncat venv/pyvenv.cfg\n\nYou should see something like:\nhome = /usr/bin/python3.12\n\nIf that's the case, then you just need to tell VSCode to use that exact interpreter. Open the command palette (`Ctrl+Shift+P`) in VSCode, choose “Python: Select Interpreter”, and manually select the path to your virtualenv’s Python binary:\n/path/to/your/venv/bin/python\n\nAlso, double-check the shebang in your pip script:\nhead -n 1 venv/bin/pip\n\nIf it says `#!/usr/bin/python3`, that might now point to Python 3.13, which breaks the venv. You can fix this by rebuilding the venv with the correct Python version:\npython3.12 -m venv --upgrade-deps venv\n\nOr, if that doesn’t work cleanly:\nrm -rf venv\npython3.12 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\nAnd yeah, `ensurepip` being disabled for system Python is normal on Ubuntu. Just make sure you have the necessary packages installed:\nsudo apt install python3.12-venv python3.12-distutils\n\nOnce I manually selected the right interpreter in VSCode and fixed the pip shebang, everything worked again—IntelliSense, linting, imports, etc. Hope that helps.",
    "question_score": 4,
    "answer_score": 3,
    "question_id": 79605465,
    "has_full_answer": true,
    "tags": [
      "python",
      "ubuntu",
      "pip",
      "windows-subsystem-for-linux",
      "venv"
    ],
    "topic": "venv"
  },
  {
    "instruction": "abjad.show() issues \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" in Python",
    "response": "Looking at the source of \"...\\abjad\\configuration.py\", line 388 (as pointed to by the error message), we have the following lines:\ncommand = [\"lilypond\", \"--version\"]\nproc = subprocess.run(command, stdout=subprocess.PIPE)\n\nSo the process is trying to run the command \"`lilypond --version`\", and failing because Windows cannot find an executable file or command script with that name. You should check where that command is installed and ensure that its full path is added to your `PATH` environment variable.",
    "question_score": 0,
    "answer_score": 2,
    "question_id": 79414767,
    "has_full_answer": true,
    "tags": [
      "python",
      "venv",
      "abjad"
    ],
    "topic": "venv"
  },
  {
    "instruction": "python venv install skips component file \"pointer.png\"",
    "response": "It's because it's not present in `tool.setuptools.package-data` in pyproject.toml file.\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\"]\n\nWith the previous configuration, you add all this extensions in your package as you can see in the next screenshot (content of the package uploaded on pypi).\n\nSo adding the png extension should work:\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\", \"*.png\"]",
    "question_score": 1,
    "answer_score": 1,
    "question_id": 79441934,
    "has_full_answer": true,
    "tags": [
      "python",
      "pip",
      "venv"
    ],
    "topic": "venv"
  },
  {
    "instruction": "Can 2 VS Code windows for 2 git clones each work with their own venv?",
    "response": "OK.  It was pretty simple.  I just had to:\n\nOn the command line: activate my venv: `source .venv/bin/activate`\n`code .`\ncommand-shift-p\nClick \"Python: select interpreter\"\nClick \"Enter Interpreter path\"\nPaste and hit Enter\n\nI was mistakenly guessing that just activating the environment (either in the terminal from which I launch vscode or in vscode's terminal) was enough.  I probably did this for the primary clone long ago and just forgot.",
    "question_score": 0,
    "answer_score": 0,
    "question_id": 79513411,
    "has_full_answer": true,
    "tags": [
      "visual-studio-code",
      "venv"
    ],
    "topic": "venv"
  },
  {
    "instruction": "Check if a python module exists with specific venv path",
    "response": "import sys\nimport importlib.util\nimport os\nfrom pathlib import Path\n\ndef is_module_installed_in_venv(module_name, venv_path):\n```bash\nvenv_python_lib_path = Path(venv_path) / 'lib'\nfor python_dir in venv_python_lib_path.iterdir():\n```\n        if python_dir.name.startswith('python'):\n            site_packages_path = python_dir / 'site-packages'\n            break\n    \n```bash\nif not site_packages_path.exists():\n```\n        return False\n    \n```bash\nsys.path.insert(0, str(site_packages_path))\n```\n\n```bash\nmodule_spec = importlib.util.find_spec(module_name)\n```\n    \n```bash\nsys.path.pop(0)\n```\n    \n```bash\nreturn module_spec is not None\n```\n\nvenv_path = '/home/user/anaconda3/envs/env_name/' \nmodule_name = 'numpy'\n\nif is_module_installed_in_venv(module_name, venv_path):\n```bash\nprint(\"do something\")\n```\n\nthis works , make sure to include the full path",
    "question_score": 0,
    "answer_score": 0,
    "question_id": 79551674,
    "has_full_answer": true,
    "tags": [
      "python",
      "python-importlib",
      "venv"
    ],
    "topic": "venv"
  },
  {
    "instruction": "Cannot import: `from serpapi import GoogleSearch`",
    "response": "The solution was to remove all system Python installations from my Mac (including the official Python binary and Brew python). Then create a run shell script configuration in PyCharm like this:\n`source venv/bin/activate && streamlit run app/web/app.py`",
    "question_score": -1,
    "answer_score": 0,
    "question_id": 79628986,
    "has_full_answer": true,
    "tags": [
      "python",
      "pycharm",
      "streamlit",
      "venv"
    ],
    "topic": "venv"
  },
  {
    "instruction": "Get the chmod numerical value for a file",
    "response": "You can get the value directly using a stat output format, e.g.\nLinux:\nstat --format '%a' \n\nBSD/OS X:\nstat -f \"%OLp\" \n\nBusybox:\n stat -c '%a'",
    "question_score": 170,
    "answer_score": 272,
    "question_id": 46915,
    "has_full_answer": true,
    "tags": [
      "linux",
      "freebsd",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "What are correct permissions for /tmp ? I unintentionally set it all public recursively",
    "response": "The normal settings for `/tmp` are 1777 aka `a=rwx,o+t`, which `ls` shows as `drwxrwxrwt`. That is: wide open, except that only the owner of a file (or of `/tmp`, but in that case that's `root` which has every right anyway) can remove or rename it (that's what this extra `t` bit means for a directory).\nThe problem with a `/tmp` with mode 777 is that another user could remove a file that you've created and substitute the content of their choice.\nIf your `/tmp` is a tmpfs filesystem, a reboot will restore everything. Otherwise, run `chmod 1777 /tmp`.\nAdditionally, a lot of files in `/tmp` need to be private. However, at least one directory critically needs to be world-readable: `/tmp/.X11-unix`, and possibly some other similar directories (`/tmp/.XIM-unix`, etc.). The following command should mostly set things right:\nchmod 1777 /tmp\nfind /tmp \\\n     -mindepth 1 \\\n     -name '.*-unix' -exec chmod 1777 {} + -prune -o \\\n     -exec chmod go-rwx {} +\n\nI.e. make all files and directories private (remove all permissions for group and other), but make the X11 sockets accessible to all. Access control on these sockets is enforced by the server, not by the file permissions. There may be other sockets that need to be publicly available. Run `find /tmp -type s -user 0` to discover root-owned sockets which you may need to make world-accessible. There may be sockets owned by other system users as well (e.g. to communicate with a system bus); explore with `find /tmp -type s ! -user $UID` (where `$UID` is your user ID).",
    "question_score": 119,
    "answer_score": 178,
    "question_id": 71622,
    "has_full_answer": true,
    "tags": [
      "permissions",
      "chmod",
      "tmp"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "Understanding UNIX permissions and file types",
    "response": "I’ll answer your questions in three parts: file types, permissions, and use cases for the various forms of `chmod`.\n\nFile types\n\nThe first character in `ls -l` output represents the file type; `d` means it’s a directory. It can’t be set or unset, it depends on how the file was created. You can find the complete list of file types in the ls documentation; those you’re likely to come across are\n\n`-`: “regular” file, created with any program which can write a file\n`b`: block special file, typically disk or partition devices, can be created with `mknod`\n`c`: character special file, can also be created with `mknod` (see `/dev` for examples)\n`d`: directory, can be created with `mkdir`\n`l`: symbolic link, can be created with `ln -s`\n`p`: named pipe, can be created with `mkfifo`\n`s`: socket, can be created with `nc -U`\n`D`: door, created by some server processes on Solaris/openindiana.\n\nPermissions\n\n`chmod 0777` is used to set all the permissions in one `chmod` execution, rather than combining changes with `u+` etc. Each of the four digits is an octal value representing a set of permissions:\n\n`suid`, `sgid` and “sticky” (see below)\nuser permissions\ngroup permissions\n“other” permissions\n\nThe octal value is calculated as the sum of the permissions:\n\n“read” is 4\n“write” is 2\n“execute” is 1\n\nFor the first digit:\n\n`suid` is 4; binaries with this bit set run as their owner user (commonly `root`)\n`sgid` is 2; binaries with this bit set run as their owner group (this was used for games so high scores could be shared, but it’s often a security risk when combined with vulnerabilities in the games), and files created in directories with this bit set belong to the directory’s owner group by default (this is handy for creating shared folders)\n“sticky” (or “restricted deletion”) is 1; files in directories with this bit set can only be deleted by their owner, the directory’s owner, or `root` (see `/tmp` for a common example of this).\n\nSee the `chmod` manpage for details. Note that in all this I’m ignoring other security features which can alter users’ permissions on files (SELinux, file ACLs...).\n\nSpecial bits are handled differently depending on the type of file (regular file or directory) and the underlying system. (This is mentioned in the `chmod` manpage.) On the system I used to test this (with `coreutils` 8.23 on an `ext4` filesystem, running Linux kernel 3.16.7-ckt2), the behaviour is as follows. For a file, the special bits are always cleared unless explicitly set, so `chmod 0777` is equivalent to `chmod 777`, and both commands clear the special bits and give everyone full permissions on the file. For a directory, the special bits are never fully cleared using the four-digit numeric form, so in effect `chmod 0777` is also equivalent to `chmod 777` but it’s misleading since some of the special bits will remain as-is. (A previous version of this answer got this wrong.) To clear special bits on directories you need to use `u-s`, `g-s` and/or `o-t` explicitly or specify a negative numeric value, so `chmod -7000` will clear all the special bits on a directory.\n\nIn `ls -l` output, `suid`, `sgid` and “sticky” appear in place of the `x` entry: `suid` is `s` or `S` instead of the user’s `x`, `sgid` is `s` or `S` instead of the group’s `x`, and “sticky” is `t` or `T` instead of others’ `x`. A lower-case letter indicates that both the special bit and the executable bit are set; an upper-case letter indicates that only the special bit is set.\n\nThe various forms of chmod\n\nBecause of the behaviour described above, using the full four digits in `chmod` can be confusing (at least it turns out I was confused). It’s useful when you want to set special bits as well as permission bits; otherwise the bits are cleared if you’re manipulating a file, preserved if you’re manipulating a directory. So `chmod 2750` ensures you’ll get at least `sgid` and exactly `u=rwx,g=rx,o=`; but `chmod 0750` won’t necessarily clear the special bits.\n\nUsing numeric modes instead of text commands (`[ugo][=+-][rwxXst]`) is probably more a case of habit and the aim of the command. Once you’re used to using numeric modes, it’s often easier to just specify the full mode that way; and it’s useful to be able to think of permissions using numeric modes, since many other commands can use them (`install`, `mknod`...).\n\nSome text variants can come in handy: if you simply want to ensure a file can be executed by anyone, `chmod a+x` will do that, regardless of what the other permissions are. Likewise, `+X` adds the execute permission only if one of the execute permissions is already set or the file is a directory; this can be handy for restoring permissions globally without having to special-case files v. directories. Thus, `chmod -R ug=rX,u+w,o=` is equivalent to applying `chmod -R 750` to all directories and executable files and `chmod -R 640` to all other files.",
    "question_score": 103,
    "answer_score": 164,
    "question_id": 183994,
    "has_full_answer": true,
    "tags": [
      "linux",
      "permissions",
      "ls",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "How to create a Samba share that is writable from Windows without 777 permissions?",
    "response": "I recommend to create a dedicated user for that share and specify it in `force user`(see docs).\n\nCreate a user (`shareuser` for example) and set the owner of everything in the share folder to that user:\n\nadduser --system shareuser\nchown -R shareuser /path/to/share\n\nThen add `force user` and permission mask settings in `smb.conf`:\n\n[myshare]\npath = /path/to/share\nwriteable = yes\nbrowseable = yes\npublic = yes\ncreate mask = 0644\ndirectory mask = 0755\nforce user = shareuser\n\nNote that `guest ok` is a synonym for `public`.",
    "question_score": 74,
    "answer_score": 102,
    "question_id": 206309,
    "has_full_answer": true,
    "tags": [
      "permissions",
      "windows",
      "samba",
      "chmod",
      "shared-folders"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "Change permissions for a symbolic link",
    "response": "You can make a new symlink and move it to the location of the old link.\n\nln -s  npm2\nmv -f npm2 npm\n\nThat will preserve the link ownership. Alternatively, you can use `chown` to set the link's ownership manually.\n\nchown -h myuser:myuser npm\n\nOn most systems, symlink permissions don't matter. When using the symlink, the permissions of the components of symlink's target will be checked.  However, on some systems they do matter.  MacOS requires read permission on the link for `readlink`, and NetBSD's `symperm` mount option forces link permissions checks on read and traversal.  On those systems (and their relatives, including FreeBSD and OpenBSD) there is a equivalent `-h` option to `chmod`.\n\nchmod -h 777 npm",
    "question_score": 65,
    "answer_score": 95,
    "question_id": 87200,
    "has_full_answer": true,
    "tags": [
      "symlink",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "Create file in folder: permission denied",
    "response": "First of all you have to know that the default permission of directories in Ubuntu is 644 which means you can't create a file in a directory you are not the owner.\n\nyou are trying as `user:francisco-vergara` to create a file in a directory `/home/sixven/camp_sms/inputs` which is owned by `user:sixven`.\n\nSo how to solve this:\n\nYou can either change the permission of the directory and enable others to create files inside.\n\nsudo chmod -R 777 /home/sixven/camp_sms/inputs\n\nThis command will change the permission of the directory recursively and enable all other users to create/modify and delete files and directories inside.\nYou can change the owner ship of this directory and make `user:francisco-vergara` as the owner\n\nsudo chown -R francisco-vergara:francisco-vergara /home/sixven/camp_sms/inputs\n\nBut like this the `user:sixven` can't write in this folder again and thus you  may moving in a circular infinite loop.\n\nSo i advise you to use Option 1.\n\nOr if this directory will be accessed by both users you can do the following trick:\n\nchange ownership of the directory to `user:francisco-vergara` and keep the group owner `group:sixven`.\n\nsudo chown -R francisco-vergara /home/sixven/camp_sms/inputs\n\nLike that both users can still use the directory.\n\nBut as I said you before It's easiest and more efficient to use option 1.",
    "question_score": 66,
    "answer_score": 92,
    "question_id": 119358,
    "has_full_answer": true,
    "tags": [
      "chmod",
      "chown"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "Make group permissions same as user permissions",
    "response": "This is what you want:\n\nchmod -R g=u directory",
    "question_score": 48,
    "answer_score": 81,
    "question_id": 31978,
    "has_full_answer": true,
    "tags": [
      "permissions",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "How to get permission number by string : -rw-r--r--",
    "response": "Please check `stat` output:\n\n# stat .xsession-errors \n  File: ‘.xsession-errors’\n  Size: 839123          Blocks: 1648       IO Block: 4096   regular file\nDevice: 816h/2070d      Inode: 3539028     Links: 1\nAccess: (0600/-rw-------)  Uid: ( 1000/     lik)   Gid: ( 1000/     lik)\nAccess: 2012-05-30 23:11:48.053999289 +0300\nModify: 2012-05-31 07:53:26.912690288 +0300\nChange: 2012-05-31 07:53:26.912690288 +0300\n Birth: -",
    "question_score": 82,
    "answer_score": 67,
    "question_id": 39710,
    "has_full_answer": true,
    "tags": [
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "What is the first chmod octal digit in a four-digit value for?",
    "response": "Please note that `chmod 777 filename` is the equivalent of `chmod 0777 filename` in this example.\n\nThe first octal digit sets the setuid, setgid and sticky bits (see this article for more details on setuid/setgid).  octal 2 means to set group ID on the file.  So, the equivalent would be to do a `chmod a+rwx filename`, then `chmod g+s filename`.  The `chmod` info page does explain this in more detail.",
    "question_score": 67,
    "answer_score": 66,
    "question_id": 6905,
    "has_full_answer": true,
    "tags": [
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "chmod + silent mode + how force exit code 0 in spite of error",
    "response": "Please advice how to force the chmod command to give exit code 0 in\n  spite of error\n\nchmod -f 777 file.txt || :\n\nThis would execute `:`, i.e. the null command, if `chmod` fails.  Since the null command does nothing but always succeeds, you would see an exit code of 0.",
    "question_score": 66,
    "answer_score": 65,
    "question_id": 118217,
    "has_full_answer": true,
    "tags": [
      "shell",
      "shell-script",
      "chmod",
      "error-handling"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "How to set the permission drwxr-xr-x to other folders?",
    "response": "To apply those permissions to a directory:\n\n`chmod 755 directory_name`\n\nTo apply to all directories inside the current directory:\n\n`chmod 755 */`\n\nIf you want to modify all directories and subdirectories, you'll need to combine find with chmod:\n\n`find . -type d -exec chmod 755 {} +`",
    "question_score": 46,
    "answer_score": 60,
    "question_id": 184413,
    "has_full_answer": true,
    "tags": [
      "linux",
      "permissions",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "Why use `chmod 644` instead of `chmod u=rw,go=r,...`?",
    "response": "Using the octal codes has two advantages I can think of, neither of which is that huge:\n\nThey're shorter, easier to type.\nA few things only understand them, and if you routinely use them you'll not be scratching your head (or running to documentation) when you run into one. E.g., you have to use octal for `chmod` in Perl or C.\n\nSometimes really simple utilities won't handle the \"friendly\" versions; especially in non-GNU userlands.\n\nFurther, some utilities spit out octal. For example, if you run `umask` to see what your current umask is, it'll spit it out in octal (though in bash, `umask -S` does symbolic).\n\nSo, in short, I'd say the only reason to prefer them is to type fewer characters, but that even if you elect not to use them, you should know how they map so that you can figure out an octal code if you run into one of the things that only does octal. But you don't need to immediately know that 5 maps to `rx`, you only need to be able to figure that out.",
    "question_score": 38,
    "answer_score": 40,
    "question_id": 26645,
    "has_full_answer": true,
    "tags": [
      "permissions",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "What is a capital X in posix / chmod?",
    "response": "The manpage says:\n\n  execute/search only if the file is a directory or already has execute\n         permission for some user (`X`)\n\nPOSIX says:\n\n  The perm symbol X shall represent the execute/search portion of the file mode bits if the file is a directory or if the current (unmodified) file mode bits have at least one of the execute bits (S_IXUSR, S_IXGRP, or S_IXOTH) set. It shall be ignored if the file is not a directory and none of the execute bits are set in the current file mode bits.\n\nThis is a conditional permission flag: `chmod` looks at whatever it is currently processing, and if it’s a directory, or if it has any execute bit set in its current permissions (owner, group or other), it acts as if the requested permission was `x`, otherwise it ignores it. The condition is verified at the time `chmod` applies the specific `X` instruction, so you can clear execute bits in the same run with `a-x,a=rwX` to only set the executable bit on directories.\n\nYou can see whether a file has an execute bit set by looking at the “access” part of `stat`’s output, or the first column of `ls -l`. Execute bits are represented by `x`. `-rwxr-xr-x` is common for executables and indicates that the executable bit is set for the owner, group and other users; `-rw-r--r--` is common for other files and indicates that the executable bit is not set (but the read bit is set for everyone, and the write bit for the owner). See Understanding UNIX permissions and their attributes which has much more detail.\n\nThus in your example, `u=rwX` sets the owner permissions to read and write in all cases, and for directories and executable files, execute; likewise for group (`g=rX`) and other (`o=rX`), read, and execute for directories and executable files.\n\nThe intent of this operator is to allow the user to give `chmod` a variety of files and directories, and get the correct execute permissions (assuming none of the files had an invalid execute bit set). It avoids having to distinguish between files and directories (as in the traditional `find . -type f -exec chmod 644 {} +` and `find . -type d -exec chmod 755 {} +` commands), and attempts to deal with executables in a sensible way.\n\n(Note that macOS `chmod` apparently only supports `X` for `+` operations.)",
    "question_score": 35,
    "answer_score": 40,
    "question_id": 416877,
    "has_full_answer": true,
    "tags": [
      "permissions",
      "chmod",
      "posix"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "Why does chmod +w not give write permission to other(o)",
    "response": "Your specific situation\n\nIn your specific situation, we can guess that your current `umask` is `002` (this is a common default value) and this explains your surprise.\n\nIn that specific situation where `umask` value is 002 (all numbers octal).\n\n`+r` means `ugo+r` because `002 & 444` is `000`, which lets all bits to be set\n`+x` means `ugo+x` because `002 & 111` is `000`, which lets all bits to be set\nbut `+w` means `ug+w` because `002 & 222` is `002`, which prevents the \"o\" bit to be set.\n\nOther examples\n\nWith `umask 022` `+w` would mean `u+w`.\nWith `umask 007` `+rwx` would mean `ug+rwx`.\nWith `umask 077` `+rwx` would mean `u+rwx`.\n\nWhat would have matched your expectations\n\nWhen you change `umask` to `000`, by executing\n\numask 000\n\nin your terminal, then\n\nchmod +w file\n\nwill set permissions to ugo+w.\n\nSide note\n\nAs suggested by ilkkachu, note that `umask 000` doesn't mean that everybody can read and write all your files. \n\nBut `umask 000` means everyone that has some kind of access to any user account on your machine (which may include programs running server services ofc) can read and write all the files you make with that mask active and don't change (if the containing chain of directories up to the root also allows them).",
    "question_score": 32,
    "answer_score": 33,
    "question_id": 429421,
    "has_full_answer": true,
    "tags": [
      "linux",
      "command-line",
      "permissions",
      "chmod"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "How to recover from a chmod -R 000 /bin?",
    "response": "Boot another clean OS, mount the file system and fix permissions.\n\nAs your broken file system lives in a VM, you should have your host system available and working. Mount your broken file system there and fix it.\n\nIn case of QEMU/KVM you can for example mount the file system using nbd.",
    "question_score": 40,
    "answer_score": 30,
    "question_id": 77852,
    "has_full_answer": true,
    "tags": [
      "permissions",
      "executable",
      "chmod",
      "system-recovery"
    ],
    "topic": "chmod"
  },
  {
    "instruction": "How can I replace a string in a file(s)?",
    "response": "1. Replacing all occurrences of one string with another in all files in the current directory:\nThese are for cases where you know that the directory contains only regular files and that you want to process all non-hidden files. If that is not the case, use the approaches in 2.\nAll `sed` solutions in this answer assume the `sed` implementation from GNU, NetBSD, OpenBSD, ast-open which process the `-i` option the same way `perl` does. If using FreeBSD's `sed` implementation (as also found on macOS or DragonFly BSD), replace `-i` with `-i ''`. Also note that the use of the `-i` switch with any version of `sed` has certain filesystem security implications and is inadvisable in any script which you plan to distribute in any way.\n\nNon-recursive, non-hidden files in this directory only:\nperl -i -pe 's/foo/bar/g' ./* \nsed -i 's/foo/bar/g' ./*\n\nRecursive, regular files (including hidden ones) in this and all subdirectories\nfind . -type f -exec perl -i -pe 's/foo/bar/g' {} +\nfind . -type f -exec sed -i 's/foo/bar/g' {} +\n\nIf you are using zsh:\nperl -i -pe 's/foo/bar/g' ./**/*(D.)\nsed -i 's/foo/bar/g' ./**/*(D.)\n\n(may fail if the list is too big, see `zargs` to work around).\nBash did add support for zsh's recursive globbing (though only working properly with regards to symlinks in 5.0 or newer) but can't check directly for regular files, a loop is needed (parenthesis avoid setting the options globally):\n( shopt -s globstar dotglob\n```bash\nfor file in **; do\n```\n        if [[ -f $file && ! -L $file ]]; then\n            sed -i 's/foo/bar/g' \"./$file\"\n        fi\n```bash\ndone\n```\n)\n\nThe files are selected when they are regular files (`-f`) excluding symlinks (`-L`) resolving to regular files like the other approaches above do (`sed`/`perl` would break symlinks).\n\n2. Replace only if the file name matches another string / has a specific extension / is of a certain type etc:\n\nNon-recursive, files in this directory only:\nsed -i -- 's/foo/bar/g' *baz*    ## all files whose name contains baz\nsed -i -- 's/foo/bar/g' *.baz    ## files ending in .baz\n\n(using the `--` option delimiter rather than `./` prefix works here as the filenames are guaranteed not to be `-` and makes the argument list shorter, less likely to break the system limit on the size of the arguments)\n\nRecursive, regular files in this and all subdirectories\nfind . -name '*baz*' -type f -exec sed -i 's/foo/bar/g' {} +\n\nIf you are using `zsh`:\nsed -i -- 's/foo/bar/g' **/*baz*(D.)\nsed -i -- 's/foo/bar/g' **/*.baz(D.)\n\nIf you are using bash 5.0+, adapt the loop above and replace `**` with `**baz*` or `**.baz`.\nAgain `--` can be used here in place of prefixing each argument with `./`.\n\nIf a file is of a certain type, for example, executable (see `man find` for more options):\nfind . -type f -executable -exec sed -i 's/foo/bar/g' {} +\n\nWith `zsh`:\nsed -i 's/foo/bar/g' ./**/*(D*)\n\n3. Replace only if the string is found in a certain context\n\nReplace `foo` with `bar` only if it's followed by `baz`:\nsed -i 's/foo\\(.*baz\\)/bar\\1/' file\n\nIn `sed`, using `\\( \\)` saves whatever is in the parentheses and you can then access it with `\\1`.  There are many variations of this theme, to learn more about such regular expressions, see here.\n\nReplace `foo` with `bar` only if `foo` is found on the 3d column (field) of the input file (assuming whitespace-separated fields):\ngawk -i /usr/share/awk/inplace.awk '{gsub(/foo/,\"baz\",$3); print}' file\n(needs `gawk` 4.1.0 or newer and the path to the `inplace` extension may be adapted. Avoid the unsafe `-i inplace`).\n\nFor a different field just use `$N` where `N` is the number of the field of interest. For a different field separator (`:` in this example) use:\ngawk -i /usr/share/awk/inplace.awk -F: '{gsub(/foo/,\"baz\",$3);print}' file\n\nAnother solution using `perl`:\nperl -i -ane '$F[2]=~s/foo/baz/g; $\" = \" \"; print \"@F\\n\"' file \n\nNote: both the `awk` and `perl` solutions will affect spacing in the file (remove the leading and trailing blanks, and convert sequences of blanks to one space character in those lines that match). For a different field, use `$F[N-1]` where `N` is the field number you want and for a different field separator use (the `$\"=\":\"` sets the output field separator to `:`):\nperl -i -F: -ane '$F[2]=~s/foo/baz/g; $\"=\":\";print \"@F\"' foo \n\nReplace `foo` with `bar` only on the 4th line:\nsed -i '4s/foo/bar/g' file\ngawk -i /usr/share/awk/inplace.awk 'NR==4{gsub(/foo/,\"baz\")};1' file\nperl -i -pe 's/foo/bar/g if $.==4' file\n\n4. Multiple replace operations: replace with different strings\n\nYou can combine `sed` commands:\nsed -i 's/foo/bar/g; s/baz/zab/g; s/Alice/Joan/g' file\n\nBe aware that order matters (`sed 's/foo/bar/g; s/bar/baz/g'` will substitute `foo` with `baz`).\n\nor Perl commands\nperl -i -pe 's/foo/bar/g; s/baz/zab/g; s/Alice/Joan/g' file\n\nIf you have a large number of patterns, it is easier to save your patterns and their replacements in a `sed` script file:\n#!/usr/bin/sed -f\ns/foo/bar/g\ns/baz/zab/g\n\nOr, if you have too many pattern pairs for the above to be feasible, you can read pattern pairs from a file (two space separated patterns, `$pattern` and `$replacement`, per line):\nwhile IFS=' ' read -r pattern replacement; do   \n```bash\nsed -i \"s/$pattern/$replacement/\" file\n```\ndone < patterns.txt\n\nThat will be quite slow for long lists of patterns and large data files so you might want to read the patterns and create a `sed` script from them instead. The following assumes a space> delimiter separates a list of MATCHspace>REPLACE pairs occurring one-per-line in the file `patterns.txt` :\nsed 's| *\\([^ ]*\\) *\\([^ ]*\\).*|s/\\1/\\2/g|' <patterns.txt |\nsed -f- ./editfile >outfile\n\nThe above format is largely arbitrary and, for example, doesn't allow for a space> in either of MATCH or REPLACE. The method is very general though: basically, if you can create an output stream which looks like a `sed` script, then you can source that stream as a `sed` script by specifying `sed`'s script file as `-`stdin.\n\nYou can combine and concatenate multiple scripts in similar fashion:\nSOME_PIPELINE |\nsed -e'#some expression script'  \\\n```bash\n-f./script_file -f-          \\\n-e'#more inline expressions' \\\n```\n./actual_edit_file >./outfile\n\nA POSIX `sed` will concatenate all scripts into one in the order they appear on the command-line. None of these need end in a `\\n`ewline.\n\n`grep` can work the same way:\nsed -e'#generate a pattern list' <in |\ngrep -f- ./grepped_file\n\nWhen working with fixed-strings as patterns, it is good practice to escape regular expression metacharacters. You can do this rather easily:\nsed 's/[]$&^*\\./[]/\\\\&/g\n     s| *\\([^ ]*\\) *\\([^ ]*\\).*|s/\\1/\\2/g|\n' <patterns.txt |\nsed -f- ./editfile >outfile\n\n5. Multiple replace operations: replace multiple patterns with the same string\n\nReplace any of `foo`, `bar` or `baz` with `foobar`\nsed -Ei 's/foo|bar|baz/foobar/g' file\n\nor\nperl -i -pe 's/foo|bar|baz/foobar/g' file\n\n6. Replace File paths in multiple files\n\nAnother use case of using different delimiter:\nsed -i 's|path/to/foo|path/to/bar|g' ./*\n\nor\nperl -i -pe 's|path/to/foo|path/to/bar|g' ./*",
    "question_score": 976,
    "answer_score": 1296,
    "question_id": 112023,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "awk",
      "sed",
      "perl"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Replacing string based on line number",
    "response": "You can specify line number in sed or NR (number of record) in awk.\n\nawk 'NR==34 { sub(\"AAA\", \"BBB\") }'\n\nor use FNR (file number record) if you want to specify more than one file on the command line.\n\nawk 'FNR==34 { sub(\"AAA\", \"BBB\") }'\n\nor\n\nsed '34s/AAA/BBB/'\n\nto do in-place replacement with sed\n\nsed -i '34s/AAA/BBB/' file_name",
    "question_score": 89,
    "answer_score": 154,
    "question_id": 70878,
    "has_full_answer": true,
    "tags": [
      "sed",
      "awk"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How do you list number of lines of every file in a directory in human readable format.",
    "response": "How many lines are in each file.\n\nUse `wc`, originally for word count, I believe, but it can do lines, words, characters, bytes, and the longest line length. The `-l` option tells it to count lines.\n\nwc -l \n\nThis will output the number of lines in :\n\n$ wc -l /dir/file.txt\n32724 /dir/file.txt\n\nYou can also pipe data to `wc` as well:\n\n$ cat /dir/file.txt | wc -l\n32724\n$ curl google.com --silent | wc -l\n63\n\n  How many lines are in directory.\n\nTry:\n\nfind . -name '*.pl' | xargs wc -l\n\nanother one-liner:\n\n( find ./ -name '*.pl' -print0 | xargs -0 cat ) | wc -l\n\nBTW, `wc` command counts new lines codes, not lines. When last line in the file does not end with new line code, this will not counted.\n\nYou may use grep -c ^ , full example:\n\n#this example prints line count for all found files\ntotal=0\nfind /path -type f -name \"*.php\" | while read FILE; do\n     #you see use grep instead wc ! for properly counting\n     count=$(grep -c ^ < \"$FILE\")\n     echo \"$FILE has $count lines\"\n     let total=total+count #in bash, you can convert this for another shell\ndone\necho TOTAL LINES COUNTED:  $total\n\n  How many lines in total\n\nNot sure that I understood you request correctly. e.g. this will output results in the following format, showing the number of lines for each file:\n\n# wc -l `find /path/to/directory/ -type f`\n 103 /dir/a.php\n 378 /dir/b/c.xml\n 132 /dir/d/e.xml\n 613 total\n\nAlternatively, to output just the total number of new line characters without the file by file counts to following command can prove useful:\n\n# find /path/to/directory/ -type f -exec wc -l {} \\; | awk '{total += $1} END{print total}'\n 613\n\n  Most importantly, I need this in 'human readable format' eg.\n  12,345,678 rather than 12345678\n\nBash has a printf function built in:\n\nprintf \"%0.2f\\n\" $T\n\nAs always, there are many different methods that could be used to achieve the same results mentioned here.",
    "question_score": 108,
    "answer_score": 151,
    "question_id": 260630,
    "has_full_answer": true,
    "tags": [
      "bash",
      "awk",
      "python",
      "perl"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Using awk to sum the values of a column, based on the values of another column",
    "response": "awk -F '|' '$1 ~ /smiths/ {sum += $3} END {print sum}' inputfilename\n\nThe `-F` flag sets the field separator; I put it in single quotes because it is a special shell character.  \nThen `$1 ~ /smiths/` applies the following {code block} only to lines where the first field matches the regex `/smiths/`.  \nThe rest is the same as your code.\n\nNote that since you're not really using a regex here, just a specific value, you could just as easily use:\n\nawk -F '|' '$1 == \"smiths\" {sum += $3} END {print sum}' inputfilename\n\nWhich checks string equality.  This is equivalent to using the regex `/^smiths$/`, as mentioned in another answer, which includes the `^` anchor to only match the start of the string (the start of field 1) and the `$` anchor to only match the end of the string.  Not sure how familiar you are with regexes.  They are very powerful, but for this case you could use a string equality check just as easily.",
    "question_score": 108,
    "answer_score": 130,
    "question_id": 242946,
    "has_full_answer": true,
    "tags": [
      "awk"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Remove line containing certain string and the following line",
    "response": "If you have GNU sed (so non-embedded Linux or Cygwin):\n\nsed '/bar/,+1 d'\n\nIf you have `bar` on two consecutive lines, this will delete the second line without analyzing it. For example, if you have a 3-line file `bar`/`bar`/`foo`, the `foo` line will stay.",
    "question_score": 103,
    "answer_score": 111,
    "question_id": 56123,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "sed",
      "awk",
      "replace"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How to view all the content in an awk array?",
    "response": "You can loop over the array's keys and extract the corresponding values:\n\nawk '{my_dict[$1] = $2} END { for (key in my_dict) { print my_dict[key] } }' zen\n\nTo get output similar to that you'd get with a Python dictionary, you can print the key as well:\n\nawk '{my_dict[$1] = $2} END { for (key in my_dict) { print key \": \" my_dict[key] } }' zen\n\nThis works regardless of the key type.",
    "question_score": 70,
    "answer_score": 95,
    "question_id": 183279,
    "has_full_answer": true,
    "tags": [
      "awk"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How to print the longest line in a file?",
    "response": "cat ./text | awk ' { if ( length > x ) { x = length; y = $0 } }END{ print y }'\n\nUPD: summarizing all the advices in the comments\n\nawk 'length > max_length { max_length = length; longest_line = $0 } END { print longest_line }' ./text",
    "question_score": 75,
    "answer_score": 87,
    "question_id": 24509,
    "has_full_answer": true,
    "tags": [
      "bash",
      "awk",
      "filter"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Difference between gawk vs. awk",
    "response": "AWK is a programming language. There are several implementations of AWK (mostly in the form of interpreters). AWK has been codified in POSIX.\nThe main implementations in use today are:\n\n`nawk` (“new awk”, an evolution of `oawk`, the original UNIX implementation), used on *BSD and widely available on Linux;\n`mawk`, a fast implementation that mostly sticks to standard features;\n`gawk`, the GNU implementation, with many extensions;\nthe\nBusybox (small, intended for embedded systems, not many features).\n\nIf you only care about standard features, call `awk`, which may be Gawk or nawk or mawk or some other implementation. If you want the features in GNU awk, use `gawk` or Perl or Python.",
    "question_score": 76,
    "answer_score": 74,
    "question_id": 29576,
    "has_full_answer": true,
    "tags": [
      "awk",
      "gawk"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How to add line numbers in every line using shell command?",
    "response": "If you want the same format that you have specified \n\nawk '{print NR  \"> \" $s}' inputfile > outputfile\n\notherwise, though not standard, most implementations of the `cat` command can print line numbers for you (numbers padded to width 6 and followed by TAB in at least the GNU, busybox, Solaris and FreeBSD implementations). \n\ncat -n inputfile > outputfile\n\nOr you can use `grep -n` (numbers followed by `:`) with a regexp like `^` that matches any line:\n\ngrep -n '^' inputfile > outputfile",
    "question_score": 83,
    "answer_score": 67,
    "question_id": 222218,
    "has_full_answer": true,
    "tags": [
      "shell-script",
      "shell",
      "text-processing",
      "awk",
      "sed"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How does awk '!a[$0]++' work?",
    "response": "Here is a \"intuitive\" answer, for a more in depth explanation of awk's mechanism see either @Cuonglm's\nIn this case, `!a[$0]++`, the post-increment `++` can be set aside for a moment, it does not change the value of the expression. So, look at only `!a[$0]`. Here:\na[$0]\n\nuses the current line `$0` as key to the array `a`, taking the value stored there. If this particular key was never referenced before, `a[$0]` evaluates to the empty string.\n!a[$0]\n\nThe `!` negates the value from before. If it was empty or zero (false), we now have a true result. If it was non-zero (true), we have a false result. If the whole expression evaluated to true, meaning that `a[$0]` was not set to begin with, the whole line is printed as the default action.\nAlso, regardless of the old value, the post-increment operator adds one to `a[$0]`, so the next time the same value in the array is accessed, it will be positive and the whole condition will fail.",
    "question_score": 75,
    "answer_score": 64,
    "question_id": 159695,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "awk"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Pass shell variable as a /pattern/ to awk",
    "response": "Use awk's `~` operator, and you don't need to provide a literal regex on the right-hand side:\nfunction _process () {\n```bash\nawk -v l=\"$line\" -v pattern=\"$1\" '\n```\n        $0 ~ pattern {p=1; exit} \n        END {if(p) print l >> \"outfile.txt\"}\n```bash\n'  \n```\n}\n\nHere calling `exit` upon the first match as we don't need to read the rest. You don't even need `awk`, `grep` would be enough and likely more efficient and avoid the problem of `awk`'s `-v var='value'` doing backslash processing:\nfunction _process () {\n```bash\ngrep -qe \"$1\" && printf '%s\\n' \"$line\"\n```\n}\n\nDepending on the pattern, you may want `grep -Eqe \"$1\"`",
    "question_score": 79,
    "answer_score": 57,
    "question_id": 120788,
    "has_full_answer": true,
    "tags": [
      "shell",
      "awk",
      "quoting",
      "variable"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Add thousands separator in a number",
    "response": "With `sed`:\n\n$ echo \"123456789\" | sed 's/\\([[:digit:]]\\{3\\}\\)\\([[:digit:]]\\{3\\}\\)\\([[:digit:]]\\{3\\}\\)/\\1,\\2,\\3/g'\n123,456,789\n\n(Note that this only works for exactly 9 digits!)\n\nor this with `sed`:\n\n$ echo \"123456789\" | sed ':a;s/\\B[0-9]\\{3\\}\\>/,&/;ta'\n123,456,789\n\nWith `printf`:\n\n$ LC_NUMERIC=en_US printf \"%'.f\\n\" 123456789\n123,456,789",
    "question_score": 70,
    "answer_score": 48,
    "question_id": 113795,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "awk",
      "string"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Show sum of file sizes in directory listing",
    "response": "The following function does most of what you're asking for:\n\ndir () { ls -FaGl \"${@}\" | awk '{ total += $4; print }; END { print total }'; }\n\n... but it won't give you what you're asking for from `dir -R *.jpg *.tif`, because that's not how `ls -R` works. You might want to play around with the `find` utility for that.",
    "question_score": 134,
    "answer_score": 35,
    "question_id": 72661,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "awk",
      "ls"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Removing leading zeros from date output",
    "response": "As per the GNU `date` manpage:\n\n   By default, date  pads  numeric  fields  with  zeroes.   The  following\n   optional flags may follow '%':\n\n   -      (hyphen) do not pad the field\n\nTherefore you can do\n\nalias date=\"date '+%Y.%-m.%-d.%-H.%-M.%-S'\"\n\nand receive\n\n2013.6.14.3.19.31",
    "question_score": 69,
    "answer_score": 123,
    "question_id": 79371,
    "has_full_answer": true,
    "tags": [
      "sed",
      "awk",
      "date"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Common lines between two files",
    "response": "Use `comm -12 file1 file2` to get common lines in both files.\nYou may also needs your file to be sorted to `comm` to work as expected.\ncomm -12 <(sort file1) <(sort file2)\n\nFrom `man comm`:\n-1     suppress column 1 (lines unique to FILE1)\n-2     suppress column 2 (lines unique to FILE2)\n\nOr using `grep` command you need to add `-x` option to match the whole line as a matching pattern. The `F` option is telling `grep` that match pattern as a string not a regex match.\ngrep -Fxf file1 file2\n\nOr using `awk`.\nawk 'NR==FNR{seen[$0]=1; next} seen[$0]' file1 file2\n\nThis is reading the whole line of file1 into an array called `seen` where the key is a whole line (in `awk` the `$0` represents the whole current line).\nWe used `NR==FNR` as a condition to run the following block only for the first input file1 and not file2 (`NR` is referring to the number of records across all inputs, and `FNR` is the file number of records for each individual input. So, `FNR` is unique for each input file whereas `NR` is unique for all inputs files.)\nThe `next` statement telling `awk` to not continue the rest of the code and rather start over again until `NR` is not equal to `FNR`, which means all lines of file1 are read by `awk`.\nThen next condition `seen[$0]` will apply only for the second input file2. For each line in file2 it will print every line that was marked as present `=1` in file1 in the array.\nAnother simple option is using `sort` and `uniq`:\nsort file1 file2|uniq -d\n\nThis will print both files sorted then `uniq -d` will print only duplicated lines. BUT this is granted when there is NO duplicated lines in both files themselves, else below is always granted even if there is a lines duplicated within both files.\nuniq -d <(sort <(sort -u file1) <(sort -u file2))",
    "question_score": 52,
    "answer_score": 106,
    "question_id": 398142,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "awk",
      "diff",
      "uniq",
      "comm"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Remove last character from string captured with awk",
    "response": "Yes, with `substr()` you can do string slicing:\n\n... | awk '{if (NR!=1) {print substr($2, 1, length($2)-1)}}'\n\n`length($2)` will get us the length of the second field, deducting 1 from that to strip off the last character.\n\nExample:\n\n$ echo spamegg foobar | awk '{print substr($2, 1, length($2)-1)}'\nfooba",
    "question_score": 61,
    "answer_score": 97,
    "question_id": 305190,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "awk",
      "string"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How to grep lines between start and end pattern?",
    "response": "`grep` won't help you here. This is a job better accomplished with `sed` using range expressions:\n\n$ sed -n '/aaa/,/cdn/p' file\naaa\nb12\ncdn\n$ sed -n '/zdk/,/dke/p' file\nzdk\naaa\nb12\ncdn\ndke\n\n`sed -n` suppresses the automatic printing, so that lines are printed just if explicitly asked to. And this happens when the range `/aaa/,/cdn/` happens.\n\nThese range expressions are also available in `awk`, where you can say:\n\nawk '/zdk/,/dke/' file\n\nOf course, all these conditions can be expanded to a more strict regex like `sed -n '/^aaa$/,/^cdn$/p' file` to check that the lines consist on exactly `aaa` and `cdn`, nothing else.",
    "question_score": 63,
    "answer_score": 90,
    "question_id": 236751,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "awk",
      "sed",
      "grep"
    ],
    "topic": "awk"
  },
  {
    "instruction": "How to get last part of http link in Bash?",
    "response": "Using `awk` for this would work, but it's kind of deer hunting with a howitzer.  If you already have your URL bare, it's pretty simple to do what you want if you put it into a shell variable and use `bash`'s built-in parameter substitution:\n\n$ myurl='http://www.example.com/long/path/to/example/file.ext'\n$ echo ${myurl##*/}\nfile.ext\n\nThe way this works is by removing a prefix that greedily matches '*/', which is what the `##` operator does:\n\n${haystack##needle} # removes any matching 'needle' from the\n                    # beginning of the variable 'haystack'",
    "question_score": 46,
    "answer_score": 80,
    "question_id": 325490,
    "has_full_answer": true,
    "tags": [
      "bash",
      "shell-script",
      "awk",
      "cut"
    ],
    "topic": "awk"
  },
  {
    "instruction": "Using 'sed' to find and replace",
    "response": "`sed` is the stream editor, in that you can use `|` (pipe) to send standard streams (STDIN and STDOUT specifically) through `sed` and alter them programmatically on the fly, making it a handy tool in the Unix philosophy tradition; but can edit files directly, too, using the `-i` parameter mentioned below.\nConsider the following:\n\nsed -i -e 's/few/asd/g' hello.txt\n\n`s/` is used to substitute the found expression `few` with `asd`:\n\n  The few, the brave.\n  \n  \n  \n  The asd, the brave.\n\n`/g` stands for \"global\", meaning to do this for the whole line. If you leave off the `/g` (with `s/few/asd/`, there always needs to be three slashes no matter what) and `few` appears twice on the same line, only the first `few` is changed to `asd`:\n\n  The few men, the few women, the brave.\n  \n  \n  \n  The asd men, the few women, the brave.\n\nThis is useful in some circumstances, like altering special characters at the beginnings of lines (for instance, replacing the greater-than symbols some people use to quote previous material in email threads with a horizontal tab while leaving a quoted algebraic inequality later in the line untouched), but in your example where you specify that anywhere `few` occurs it should be replaced, make sure you have that `/g`.\n\nThe following two options (flags) are combined into one, `-ie`:\n\n`-i` option is used to edit in place on the file `hello.txt`.\n\n`-e` option indicates the expression/command to run, in this case `s/`.\n\nNote: It's important that you use `-i -e` to search/replace. If you do `-ie`, you create a backup of every file with the letter 'e' appended.",
    "question_score": 488,
    "answer_score": 873,
    "question_id": 159367,
    "has_full_answer": true,
    "tags": [
      "shell-script",
      "text-processing",
      "sed"
    ],
    "topic": "sed"
  },
  {
    "instruction": "What characters do I need to escape when using sed in a sh script?",
    "response": "There are two levels of interpretation here: the shell, and sed.\n\nIn the shell, everything between single quotes is interpreted literally, except for single quotes themselves. You can effectively have a single quote between single quotes by writing `'\\''` (close single quote, one literal single quote, open single quote).\n\nSed uses basic regular expressions. In a BRE, in order to have them treated literally, the characters `$.*[\\^` need to be quoted by preceding them by a backslash, except inside character sets (`[…]`). Letters, digits and `(){}+?|` must not be quoted (you can get away with quoting some of these in some implementations). The sequences `\\(`, `\\)`, `\\n`, and in some implementations `\\{`, `\\}`, `\\+`, `\\?`, `\\|` and other backslash+alphanumerics have special meanings. You can get away with not quoting `$^` in some positions in some implementations.\n\nFurthermore, you need a backslash before `/` if it is to appear in the regex outside of bracket expressions. You can choose an alternative character as the delimiter by writing, e.g., `s~/dir~/replacement~` or `\\~/dir~p`; you'll need a backslash before the delimiter if you want to include it in the BRE. If you choose a character that has a special meaning in a BRE and you want to include it literally, you'll need three backslashes; I do not recommend this, as it may behave differently in some implementations.\n\nIn a nutshell, for `sed 's/…/…/'`:\n\nWrite the regex between single quotes.\nUse `'\\''` to end up with a single quote in the regex.\nPut a backslash before `$.*/[\\]^` and only those characters (but not inside bracket expressions). (Technically you shouldn't put a backslash before `]` but I don't know of an implementation that treats `]` and `\\]` differently outside of bracket expressions.)\nInside a bracket expression, for `-` to be treated literally, make sure it is first or last (`[abc-]` or `[-abc]`, not `[a-bc]`).\nInside a bracket expression, for `^` to be treated literally, make sure it is not first (use `[abc^]`, not `[^abc]`).\nTo include `]` in the list of characters matched by a bracket expression, make it the first character  (or first after `^` for a negated set): `[]abc]` or `[^]abc]` (not `[abc]]` nor `[abc\\]]`).\n\nIn the replacement text:\n\n`&` and `\\` need to be quoted by preceding them by a backslash,\nas do the delimiter (usually `/`) and newlines.\n`\\` followed by a digit has a special meaning. `\\` followed by a letter has a special meaning (special characters) in some implementations, and `\\` followed by some other character means `\\c` or `c` depending on the implementation.\nWith single quotes around the argument (`sed 's/…/…/'`), use `'\\''` to put a single quote in the replacement text.\n\nIf the regex or replacement text comes from a shell variable, remember that\n\nThe regex is a BRE, not a literal string.\nIn the regex, a newline needs to be expressed as `\\n` (which will never match unless you have other `sed` code adding newline characters to the pattern space). But note that it won't work inside bracket expressions with some `sed` implementations.\nIn the replacement text, `&`, `\\` and newlines need to be quoted.\nThe delimiter needs to be quoted (but not inside bracket expressions).\nUse double quotes for interpolation: `sed -e \"s/$BRE/$REPL/\"`.",
    "question_score": 429,
    "answer_score": 478,
    "question_id": 32907,
    "has_full_answer": true,
    "tags": [
      "shell-script",
      "sed",
      "quoting"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Using sed to find and replace complex string (preferrably with regex)",
    "response": "sed -i -E \"s/(.+)name(.+)/\\1something\\2/\" file.xml\n\nThis is, I think, what you're looking for.\nExplanation:\n\nparentheses in the first part define groups (strings in fact) that can be reused in the second part\n`\\1`, `\\2`, etc. in the second part are references to the i-th group captured in the first part (the numbering starts with 1)\n`-E` enables extended regular expressions (needed for `+` and grouping).\n`-i` enables \"in-place\" file edit mode",
    "question_score": 241,
    "answer_score": 403,
    "question_id": 78625,
    "has_full_answer": true,
    "tags": [
      "sed",
      "regular-expression",
      "quoting"
    ],
    "topic": "sed"
  },
  {
    "instruction": "How can I use sed to replace a multi-line string?",
    "response": "In the simplest calling of sed, it has one line of text in the pattern space, ie. 1 line of `\\n` delimited text from the input. The single line in the pattern space has no `\\n`. That's why your regex is not finding anything.\nYou can read multiple lines into the pattern-space and manipulate things surprisingly well, but with a more than normal effort.  Sed has a set of commands which allow this type of thing.  Here is a link to a Command Summary for sed. It is the best one I've found, and got me rolling.\nHowever forget the \"one-liner\" idea once you start using sed's micro-commands. It is useful to lay it out like a structured program until you get the feel of it. It is surprisingly simple, and equally unusual. You could think of it as the \"assembler language\" of text editing.\nSummary: Use sed for simple things, and maybe a bit more, but in general, when it gets beyond working with a single line, most people prefer something else.\nI'll let someone else suggest something else, as I'm really not sure what the best choice would be (I'd use sed, but that's because I don't know perl well enough.)\n\nsed '/^a test$/{\n       $!{ N        # append the next line when not on the last line\n         s/^a test\\nPlease do not$/not a test\\nBe/\n                    # now test for a successful substitution, otherwise\n                    #+  unpaired \"a test\" lines would be mis-handled\n         t sub-yes  # branch_on_substitute (goto label :sub-yes)\n         :sub-not   # a label (not essential; here to self document)\n                    # if no substituion, print only the first line\n         P          # pattern_first_line_print\n         D          # pattern_ltrunc(line+nl)_top/cycle\n         :sub-yes   # a label (the goto target of the 't' branch)\n                    # fall through to final auto-pattern_print (2 lines)\n       }    \n     }' alpha.txt  \n\nHere it is the same script, condensed into what is obviously harder to read and work with, but some would dubiously call a one-liner\nsed '/^a test$/{$!{N;s/^a test\\nPlease do not$/not a test\\nBe/;ty;P;D;:y}}' alpha.txt\n\nHere is my command \"cheat-sheet\"\n:  # label\n=  # line_number\na  # append_text_to_stdout_after_flush\nb  # branch_unconditional             \nc  # range_change                     \nd  # pattern_delete_top/cycle          \nD  # pattern_ltrunc(line+nl)_top/cycle \ng  # pattern=hold                      \nG  # pattern+=nl+hold                  \nh  # hold=pattern                      \nH  # hold+=nl+pattern                  \ni  # insert_text_to_stdout_now         \nl  # pattern_list                       \nn  # pattern_flush=nextline_continue   \nN  # pattern+=nl+nextline              \np  # pattern_print                     \nP  # pattern_first_line_print          \nq  # flush_quit                        \nr  # append_file_to_stdout_after_flush \ns  # substitute                                          \nt  # branch_on_substitute              \nw  # append_pattern_to_file_now         \nx  # swap_pattern_and_hold             \ny  # transform_chars",
    "question_score": 460,
    "answer_score": 366,
    "question_id": 26284,
    "has_full_answer": true,
    "tags": [
      "sed",
      "regular-expression",
      "utilities"
    ],
    "topic": "sed"
  },
  {
    "instruction": "How to insert text before the first line of a file?",
    "response": "Use `sed`'s insert (`i`) option which will insert the text in the preceding line.\nsed '1 i\\\n\nQuestion author's update:\nTo make it edit the file in place - with GNU `sed` - I had to add the\n`-i` option:\nsed -i '1 i\\anything' file\n\nAlso syntax\nsed  -i '1i text' filename\n\nFor non-GNU sed\nYou need to hit the return key immediately after the backslash `1i\\` and after `first_line_text`:\nsed -i '1i\\\nfirst_line_text\n'\n\nAlso note that some non-GNU `sed` implementations (for example the one on macOS) require an argument for the `-i` flag (use `-i ''` to get the same effect as with GNU `sed`).\nFor `sed` implementations that does not support `-i` at all, run without this option but redirect the output to a new file.  Then replace the old file with the newly created file.",
    "question_score": 176,
    "answer_score": 231,
    "question_id": 99350,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "sed"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Decoding URL encoding (percent encoding)",
    "response": "Found these Python one liners that do what you want:\n\nPython2\n\n$ alias urldecode='python -c \"import sys, urllib as ul; \\\n```bash\nprint ul.unquote_plus(sys.argv[1])\"'\n```\n\n$ alias urlencode='python -c \"import sys, urllib as ul; \\\n```bash\nprint ul.quote_plus(sys.argv[1])\"'\n```\n\nPython3\n\n$ alias urldecode='python3 -c \"import sys, urllib.parse as ul; \\\n```bash\nprint(ul.unquote_plus(sys.argv[1]))\"'\n```\n\n$ alias urlencode='python3 -c \"import sys, urllib.parse as ul; \\\n```bash\nprint (ul.quote_plus(sys.argv[1]))\"'\n```\n\nExample\n\n$ urldecode 'q+werty%3D%2F%3B'\nq werty=/;\n\n$ urlencode 'q werty=/;'\nq+werty%3D%2F%3B\n\nReferences\n\nUrlencode and urldecode from a command line",
    "question_score": 157,
    "answer_score": 183,
    "question_id": 159253,
    "has_full_answer": true,
    "tags": [
      "shell-script",
      "text-processing",
      "sed",
      "url"
    ],
    "topic": "sed"
  },
  {
    "instruction": "How to insert text after a certain string in a file?",
    "response": "Append line after match\n\n`sed  '/\\[option\\]/a Hello World' input`\n\nInsert line before match\n\n`sed  '/\\[option\\]/i Hello World' input`\n\nAdditionally you can take backup and edit input file in-place using `-i.bkp` option to sed",
    "question_score": 144,
    "answer_score": 177,
    "question_id": 121161,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "sed"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Convert file contents to lower case",
    "response": "If your input only contains ASCII characters, you could use `tr` like:\n\ntr A-Z a-z < input \n\nor (less easy to remember and type IMO; but not limited to ASCII latin letters, though in some implementations including GNU `tr`, still limited to single-byte characters, so in UTF-8 locales, still limited to ASCII letters):\n\ntr '[:upper:]' '[:lower:]' < input\n\nif you have to use `sed`:\n\nsed 's/.*/\\L&/g' < input\n\n(here assuming the GNU implementation).\n\nWith POSIX `sed`, you'd need to specify all the transliterations and then you can choose which letters you want to convert:\n\nsed 'y/AǼBCΓDEFGH.../aǽbcγdefgh.../' < input\n\nWith `awk`:\n\nawk '{print tolower($0)}' < input",
    "question_score": 123,
    "answer_score": 177,
    "question_id": 171603,
    "has_full_answer": true,
    "tags": [
      "shell",
      "text-processing",
      "sed",
      "tcsh",
      "tr"
    ],
    "topic": "sed"
  },
  {
    "instruction": "SED: insert text after the last line?",
    "response": "The simplest way with GNU sed:\n\nsed -i -e '$aTEXTTOEND' filename\n \nHow it works\n`$` matches the last line (it's a normal `sed` address; `4aTEXTTOEND` would insert after the fourth line), `a` is the append command, and `TEXTTOEND` is what to append, `filename` is the file where the `TEXTTOEND` will be inserted\nsed on macOS\nThis is a GNU extension and does not work in the same way in the macOS version of `sed`. You can install `gsed` with Homebrew.",
    "question_score": 117,
    "answer_score": 165,
    "question_id": 20573,
    "has_full_answer": true,
    "tags": [
      "sed"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Delete First line of a file",
    "response": "The reason file.txt is empty after that command is the order in which the shell does things. The first thing that happens with that line is the redirection. The file \"file.txt\" is opened and truncated to 0 bytes. After that the sed command runs, but at the point the file is already empty. \n\nThere are a few options, most involve writing to a temporary file.\n\nsed '1d' file.txt > tmpfile; mv tmpfile file.txt # POSIX\nsed -i '1d' file.txt # GNU sed only, creates a temporary file\n\nperl -ip -e '$_ = undef if $. == 1' file.txt # also creates a temporary file",
    "question_score": 176,
    "answer_score": 112,
    "question_id": 96226,
    "has_full_answer": true,
    "tags": [
      "shell-script",
      "sed",
      "ksh"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Replace string in a huge (70GB), one line, text file",
    "response": "The usual text processing tools are not designed to handle lines that don't fit in RAM. They tend to work by reading one record (one line), manipulating it, and outputting the result, then proceeding to the next record (line).\n\nIf there's an ASCII character that appears frequently in the file and doesn't appear in `` or ``, then you can use that as the record separator. Since most tools don't allow custom record separators, swap between that character and newlines. `tr` processes bytes, not lines, so it doesn't care about any record size. Supposing that `;` works:\n\n<corpus.txt tr '\\n;' ';\\n' |\nsed 's///g' |\ntr '\\n;' ';\\n' >corpus.txt.new\n\nYou could also anchor on the first character of the text you're searching for, assuming that it isn't repeated in the search text and it appears frequently enough. If the file may start with `unk>`, change the sed command to `sed '2,$ s/…` to avoid a spurious match.\n\n<corpus.txt tr '\\n<' '<\\n' |\nsed 's/^unk>/raw_unk>/g' |\ntr '\\ncorpus.txt.new\n\nAlternatively, use the last character.\n\n' '>\\n' |\nsed 's/<unk$/<raw_unk/g' |\ntr '\\n>' '>\\n' >corpus.txt.new\n\nNote that this technique assumes that sed operates seamlessly on a file that doesn't end with a newline, i.e. that it processes the last partial line without truncating it and without appending a final newline. It works with GNU sed. If you can pick the last character of the file as the record separator, you'll avoid any portability trouble.",
    "question_score": 135,
    "answer_score": 111,
    "question_id": 413664,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "sed",
      "large-files"
    ],
    "topic": "sed"
  },
  {
    "instruction": "how to rename multiple files by replacing string in file name? this string contains a \"#\"",
    "response": "This is not hard, simply make sure to escape the octothorpe (#) in the name by prepending a reverse-slash (\\).\n\nfind . -type f -name 'Lucky-*' | while read FILE ; do\n```bash\nnewfile=\"$(echo ${FILE} |sed -e 's/\\\\#U00a9/safe/')\" ;\nmv \"${FILE}\" \"${newfile}\" ;\n```\ndone",
    "question_score": 137,
    "answer_score": 63,
    "question_id": 175135,
    "has_full_answer": true,
    "tags": [
      "sed",
      "rename",
      "mv"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Delete last line from the file",
    "response": "in sed `$` is the last line so to delete the last line:\nsed '$d' \n\nUpdated to include a way to delete multiple lines from the bottom of the file:\ntac  | tail -n +3 | tac > \n\n`tac` reads the file backwards (`cat` backwards)\n`tail -n +3` reads the input starting at the nth line\n`tac` reads the input and reverses the order back to the original",
    "question_score": 93,
    "answer_score": 162,
    "question_id": 52779,
    "has_full_answer": true,
    "tags": [
      "text-processing",
      "sed"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Slash and backslash in sed",
    "response": "Use single quotes for the expression you used:\n\nsed 's/\\//\\\\\\//g'\n\nIn double quotes, `\\` has a special meaning, so you have to backslash it:\n\nsed \"s/\\//\\\\\\\\\\//g\"\n\nBut it's cleaner to change the delimiter:\n\nsed 's=/=\\\\/=g'\nsed \"s=/=\\\\\\/=g\"",
    "question_score": 82,
    "answer_score": 159,
    "question_id": 211834,
    "has_full_answer": true,
    "tags": [
      "bash",
      "sed",
      "quoting"
    ],
    "topic": "sed"
  },
  {
    "instruction": "Can sed save its output to a file?",
    "response": "tee and > can be used for data redirection because these are meant to be used for data redirection in linux. \n\nsed on the other hand is a stream editor. sed is not meant for data redirection as tee and > meant to be. However you can use conjunction of commands to do that.\n\nuse tee or > with sed\n\nsed 's/Hello/Hi/g' file-name | tee file\n\nor\n\nsed 's/Hello/Hi/g' file-name > file\n\nuse sed with -i option\n\nsed -i 's/Hello/Hi/g' file-name\n\nthe last one does not redirect, instead it will make changes in the file itself.",
    "question_score": 101,
    "answer_score": 139,
    "question_id": 283407,
    "has_full_answer": true,
    "tags": [
      "sed",
      "io-redirection"
    ],
    "topic": "sed"
  },
  {
    "instruction": "What does the \"yield\" keyword do in Python?",
    "response": "To understand what `yield` does, you must understand what generators are. And before you can understand generators, you must understand iterables.\nIterables\nWhen you create a list, you can read its items one by one. Reading its items one by one is called iteration:\n>>> mylist = [1, 2, 3]\n>>> for i in mylist:\n...    print(i)\n1\n2\n3\n\n`mylist` is an iterable. When you use a list comprehension, you create a list, and so an iterable:\n>>> mylist = [x*x for x in range(3)]\n>>> for i in mylist:\n...    print(i)\n0\n1\n4\n\nEverything you can use \"`for... in...`\" on is an iterable; `lists`, `strings`, files...\nThese iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.\nGenerators\nGenerators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:\n>>> mygenerator = (x*x for x in range(3))\n>>> for i in mygenerator:\n...    print(i)\n0\n1\n4\n\nIt is just the same except you used `()` instead of `[]`. BUT, you cannot perform `for i in mygenerator` a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end after calculating 4, one by one.\nYield\n`yield` is a keyword that is used like `return`, except the function will return a generator.\n>>> def create_generator():\n...    mylist = range(3)\n...    for i in mylist:\n...        yield i*i\n...\n>>> mygenerator = create_generator() # create a generator\n>>> print(mygenerator) # mygenerator is an object!\n\n>>> for i in mygenerator:\n...     print(i)\n0\n1\n4\n\nHere it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.\nTo master `yield`, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky.\nThen, your code will continue from where it left off each time `for` uses the generator.\nNow the hard part:\nThe first time the `for` calls the generator object created from your function, it will run the code in your function from the beginning until it hits `yield`, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting `yield`. That can be because the loop has come to an end, or because you no longer satisfy an `\"if/else\"`.\n\nYour code explained\nGenerator:\n# Here you create the method of the node object that will return the generator\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n\n```bash\n# Here is the code that will be called each time you use the generator object:\n```\n\n```bash\n# If there is still a child of the node object on its left\n# AND if the distance is ok, return the next child\nif self._leftchild and distance - max_dist < self._median:\n```\n        yield self._leftchild\n\n```bash\n# If there is still a child of the node object on its right\n# AND if the distance is ok, return the next child\nif self._rightchild and distance + max_dist >= self._median:\n```\n        yield self._rightchild\n\n```bash\n# If the function arrives here, the generator will be considered empty\n# There are no more than two values: the left and the right children\n```\n\nCaller:\n# Create an empty list and a list with the current object reference\nresult, candidates = list(), [self]\n\n# Loop on candidates (they contain only one element at the beginning)\nwhile candidates:\n\n```bash\n# Get the last candidate and remove it from the list\nnode = candidates.pop()\n```\n\n```bash\n# Get the distance between obj and the candidate\ndistance = node._get_dist(obj)\n```\n\n```bash\n# If the distance is ok, then you can fill in the result\nif distance <= max_dist and distance >= min_dist:\n```\n        result.extend(node._values)\n\n```bash\n# Add the children of the candidate to the candidate's list\n# so the loop will keep running until it has looked\n# at all the children of the children of the children, etc. of the candidate\ncandidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\n```\n\nreturn result\n\nThis code contains several smart parts:\n\nThe loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, `candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))` exhausts all the values of the generator, but `while` keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.\n\nThe `extend()` method is a list object method that expects an iterable and adds its values to the list.\n\nUsually, we pass a list to it:\n>>> a = [1, 2]\n>>> b = [3, 4]\n>>> a.extend(b)\n>>> print(a)\n[1, 2, 3, 4]\n\nBut in your code, it gets a generator, which is good because:\n\nYou don't need to read the values twice.\nYou may have a lot of children and you don't want them all stored in memory.\n\nAnd it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...\nYou can stop here, or read a little bit to see an advanced use of a generator:\nControlling a generator exhaustion\n>>> class Bank(): # Let's create a bank, building ATMs\n...    crisis = False\n...    def create_atm(self):\n...        while not self.crisis:\n...            yield \"$100\"\n>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want\n>>> corner_street_atm = hsbc.create_atm()\n>>> print(corner_street_atm.next())\n$100\n>>> print(corner_street_atm.next())\n$100\n>>> print([corner_street_atm.next() for cash in range(5)])\n['$100', '$100', '$100', '$100', '$100']\n>>> hsbc.crisis = True # Crisis is coming, no more money!\n>>> print(corner_street_atm.next())\n\n>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs\n>>> print(wall_street_atm.next())\n\n>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty\n>>> print(corner_street_atm.next())\n\n>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business\n>>> for cash in brand_new_atm:\n...    print cash\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n...\n\nNote: For Python 3, use`print(corner_street_atm.__next__())` or `print(next(corner_street_atm))`\nIt can be useful for various things like controlling access to a resource.\nItertools, your best friend\nThe `itertools` module contains special functions to manipulate iterables. Ever wish to duplicate a generator?\nChain two generators? Group values in a nested list with a one-liner? `Map / Zip` without creating another list?\nThen just `import itertools`.\nAn example? Let's see the possible orders of arrival for a four-horse race:\n>>> horses = [1, 2, 3, 4]\n>>> races = itertools.permutations(horses)\n>>> print(races)\n\n>>> print(list(itertools.permutations(horses)))\n[(1, 2, 3, 4),\n (1, 2, 4, 3),\n (1, 3, 2, 4),\n (1, 3, 4, 2),\n (1, 4, 2, 3),\n (1, 4, 3, 2),\n (2, 1, 3, 4),\n (2, 1, 4, 3),\n (2, 3, 1, 4),\n (2, 3, 4, 1),\n (2, 4, 1, 3),\n (2, 4, 3, 1),\n (3, 1, 2, 4),\n (3, 1, 4, 2),\n (3, 2, 1, 4),\n (3, 2, 4, 1),\n (3, 4, 1, 2),\n (3, 4, 2, 1),\n (4, 1, 2, 3),\n (4, 1, 3, 2),\n (4, 2, 1, 3),\n (4, 2, 3, 1),\n (4, 3, 1, 2),\n (4, 3, 2, 1)]\n\nUnderstanding the inner mechanisms of iteration\nIteration is a process implying iterables (implementing the `__iter__()` method) and iterators (implementing the `__next__()` method).\nIterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.\nThere is more about it in this article about how `for` loops work.",
    "question_score": 13085,
    "answer_score": 18226,
    "question_id": 231767,
    "has_full_answer": true,
    "tags": [
      "python",
      "iterator",
      "generator",
      "yield"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I merge two dictionaries in a single expression in Python?",
    "response": "How can I merge two Python dictionaries in a single expression?\nFor dictionaries `x` and `y`, their shallowly-merged dictionary `z` takes values from `y`, replacing those from `x`.\n\nIn Python 3.9.0 or greater (released 17 October 2020, `PEP-584`, discussed here):\nz = x | y\n\nIn Python 3.5 or greater:\nz = {**x, **y}\n\nIn Python 2, (or 3.4 or lower) write a function:\ndef merge_two_dicts(x, y):\n```bash\nz = x.copy()   # start with keys and values of x\nz.update(y)    # modifies z with keys and values of y\nreturn z\n```\n\nand now:\nz = merge_two_dicts(x, y)\n\nExplanation\nSay you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\n\nThe desired result is to get a new dictionary (`z`) with the values merged, and the second dictionary's values overwriting those from the first.\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\n\nA new syntax for this, proposed in PEP 448 and available as of Python 3.5, is\nz = {**x, **y}\n\nAnd it is indeed a single expression.\nNote that we can merge in with literal notation as well:\nz = {**x, 'foo': 1, 'bar': 2, **y}\n\nand now:\n>>> z\n{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}\n\nIt is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into the What's New in Python 3.5 document.\nHowever, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:\nz = x.copy()\nz.update(y) # which returns None since it mutates z\n\nIn both approaches, `y` will come second and its values will replace `x`'s values, thus `b` will point to `3` in our final result.\nNot yet on Python 3.5, but want a single expression\nIf you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a single expression, the most performant while the correct approach is to put it in a function:\ndef merge_two_dicts(x, y):\n```bash\n\"\"\"Given two dictionaries, merge them into a new dict as a shallow copy.\"\"\"\nz = x.copy()\nz.update(y)\nreturn z\n```\n\nand then you have a single expression:\nz = merge_two_dicts(x, y)\n\nYou can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:\ndef merge_dicts(*dict_args):\n```bash\n\"\"\"\nGiven any number of dictionaries, shallow copy and merge into a new dict,\nprecedence goes to key-value pairs in latter dictionaries.\n\"\"\"\nresult = {}\nfor dictionary in dict_args:\n```\n        result.update(dictionary)\n```bash\nreturn result\n```\n\nThis function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries `a` to `g`:\nz = merge_dicts(a, b, c, d, e, f, g) \n\nand key-value pairs in `g` will take precedence over dictionaries `a` to `f`, and so on.\nCritiques of Other Answers\nDon't use what you see in the formerly accepted answer:\nz = dict(x.items() + y.items())\n\nIn Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two `dict_items` objects together, not two lists -\n>>> c = dict(a.items() + b.items())\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\n\nand you would have to explicitly create them as lists, e.g. `z = dict(list(x.items()) + list(y.items()))`. This is a waste of resources and computation power.\nSimilarly, taking the union of `items()` in Python 3 (`viewitems()` in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:\n>>> c = dict(a.items() | b.items())\n\nThis example demonstrates what happens when values are unhashable:\n>>> x = {'a': []}\n>>> y = {'b': []}\n>>> dict(x.items() | y.items())\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: unhashable type: 'list'\n\nHere's an example where `y` should have precedence, but instead the value from `x` is retained due to the arbitrary order of sets:\n>>> x = {'a': 2}\n>>> y = {'a': 1}\n>>> dict(x.items() | y.items())\n{'a': 2}\n\nAnother hack you should not use:\nz = dict(x, **y)\n\nThis uses the `dict` constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic.\nHere's an example of the usage being remediated in django.\nDictionaries are intended to take hashable keys (e.g. `frozenset`s or tuples), but this method fails in Python 3 when keys are not strings.\n>>> c = dict(a, **b)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: keyword arguments must be strings\n\nFrom the mailing list, Guido van Rossum, the creator of the language, wrote:\n\nI am fine with\ndeclaring dict({}, **{1:3}) illegal, since after all it is abuse of\nthe ** mechanism.\n\nand\n\nApparently dict(x, **y) is going around as \"cool hack\" for \"call\nx.update(y) and return x\". Personally, I find it more despicable than\ncool.\n\nIt is my understanding (as well as the understanding of the creator of the language) that the intended usage for `dict(**y)` is for creating dictionaries for readability purposes, e.g.:\ndict(a=1, b=10, c=11)\n\ninstead of\n{'a': 1, 'b': 10, 'c': 11}\n\nResponse to comments\n\nDespite what Guido says, `dict(x, **y)` is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.\n\nAgain, it doesn't work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. `dict` broke this consistency in Python 2:\n>>> foo(**{('a', 'b'): None})\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: foo() keywords must be strings\n>>> dict(**{('a', 'b'): None})\n{('a', 'b'): None}\n\nThis inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.\nI submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.\nMore comments:\n\n`dict(x.items() + y.items())` is still the most readable solution for Python 2. Readability counts.\n\nMy response: `merge_two_dicts(x, y)` actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.\n\n`{**x, **y}` does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.\n\nYes. I must refer you back to the question, which is asking for a shallow merge of two dictionaries, with the first's values being overwritten by the second's - in a single expression.\nAssuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:\nfrom copy import deepcopy\n\ndef dict_of_dicts_merge(x, y):\n```bash\nz = {}\noverlapping_keys = x.keys() & y.keys()\nfor key in overlapping_keys:\n```\n        z[key] = dict_of_dicts_merge(x[key], y[key])\n```bash\nfor key in x.keys() - overlapping_keys:\n```\n        z[key] = deepcopy(x[key])\n```bash\nfor key in y.keys() - overlapping_keys:\n```\n        z[key] = deepcopy(y[key])\n```bash\nreturn z\n```\n\nUsage:\n>>> x = {'a':{1:{}}, 'b': {2:{}}}\n>>> y = {'b':{10:{}}, 'c': {11:{}}}\n>>> dict_of_dicts_merge(x, y)\n{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}\n\nComing up with contingencies for other value types is far beyond the scope of this question, so I will point you at my answer to the canonical question on a \"Dictionaries of dictionaries merge\".\nLess Performant But Correct Ad-hocs\nThese approaches are less performant, but they will provide correct behavior.\nThey will be much less performant than `copy` and `update` or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dictionaries have precedence)\nYou can also chain the dictionaries manually inside a dict comprehension:\n{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7\n\nor in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):\ndict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2\n\n`itertools.chain` will chain the iterators over the key-value pairs in the correct order:\nfrom itertools import chain\nz = dict(chain(x.items(), y.items())) # iteritems in Python 2\n\nPerformance Analysis\nI'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)\nfrom timeit import repeat\nfrom itertools import chain\n\nx = dict.fromkeys('abcdefg')\ny = dict.fromkeys('efghijk')\n\ndef merge_two_dicts(x, y):\n```bash\nz = x.copy()\nz.update(y)\nreturn z\n```\n\nmin(repeat(lambda: {**x, **y}))\nmin(repeat(lambda: merge_two_dicts(x, y)))\nmin(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\nmin(repeat(lambda: dict(chain(x.items(), y.items()))))\nmin(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n\nIn Python 3.8.1, NixOS:\n>>> min(repeat(lambda: {**x, **y}))\n1.0804965235292912\n>>> min(repeat(lambda: merge_two_dicts(x, y)))\n1.636518670246005\n>>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\n3.1779992282390594\n>>> min(repeat(lambda: dict(chain(x.items(), y.items()))))\n2.740647904574871\n>>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n4.266070580109954\n\n$ uname -a\nLinux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux\n\nResources on Dictionaries\n\nMy explanation of Python's dictionary implementation, updated for 3.6.\nAnswer on how to add new keys to a dictionary\nMapping two lists into a dictionary\nThe official Python docs on dictionaries\nThe Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017\nModern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017",
    "question_score": 7097,
    "answer_score": 9329,
    "question_id": 38987,
    "has_full_answer": true,
    "tags": [
      "python",
      "dictionary",
      "merge"
    ],
    "topic": "python"
  },
  {
    "instruction": "Does Python have a ternary conditional operator?",
    "response": "Yes, it was added in version 2.5. The expression syntax is:\na if condition else b\n\nFirst `condition` is evaluated, then exactly one of either `a` or `b` is evaluated and returned based on the Boolean value of `condition`. If `condition` evaluates to `True`, then `a` is evaluated and returned but `b` is ignored, or else when `b` is evaluated and returned but `a` is ignored.\nThis allows short-circuiting because when `condition` is true only `a` is evaluated and `b` is not evaluated at all, but when `condition` is false only `b` is evaluated and `a` is not evaluated at all.\nFor example:\n>>> 'true' if True else 'false'\n'true'\n>>> 'true' if False else 'false'\n'false'\n\nNote that conditionals are an expression, not a statement. This means you can't use statements such as `pass`, or assignments with `=` (or \"augmented\" assignments like `+=`), within a conditional expression:\n>>> pass if False else pass\n  File \"\", line 1\n```bash\npass if False else pass\n```\n         ^\nSyntaxError: invalid syntax\n\n>>> # Python parses this as `x = (1 if False else y) = 2`\n>>> # The `(1 if False else x)` part is actually valid, but\n>>> # it can't be on the left-hand side of `=`.\n>>> x = 1 if False else y = 2\n  File \"\", line 1\nSyntaxError: cannot assign to conditional expression\n\n>>> # If we parenthesize it instead...\n>>> (x = 1) if False else (y = 2)\n  File \"\", line 1\n```bash\n(x = 1) if False else (y = 2)\n```\n       ^\nSyntaxError: invalid syntax\n\n(In 3.8 and above, the `:=` \"walrus\" operator allows simple assignment of values as an expression, which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.)\nSimilarly, because it is an expression, the `else` part is mandatory:\n# Invalid syntax: we didn't specify what the value should be if the \n# condition isn't met. It doesn't matter if we can verify that\n# ahead of time.\na if True\n\nYou can, however, use conditional expressions to assign a variable like so:\nx = a if True else b\n\nOr for example to return a value:\n# Of course we should just use the standard library `max`;\n# this is just for demonstration purposes.\ndef my_max(a, b):\n```bash\nreturn a if a > b else b\n```\n\nThink of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will do the same thing with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to do something different depending on the condition, then use a normal `if` statement instead.\n\nKeep in mind that it's frowned upon by some Pythonistas for several reasons:\n\nThe order of the arguments is different from those of the classic `condition ? a : b` ternary operator from many other languages (such as C, C++, Go, Perl, Ruby, Java, JavaScript, etc.), which may lead to bugs when people unfamiliar with Python's \"surprising\" behaviour use it (they may reverse the argument order).\nSome find it \"unwieldy\", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).\nStylistic reasons. (Although the 'inline `if`' can be really useful, and make your script more concise, it really does complicate your code)\n\nIf you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, `x = 4 if b > 8 else 9` is read aloud as `x will be 4 if b is greater than 8 otherwise 9`.\nOfficial documentation:\n\nConditional expressions\nIs there an equivalent of C’s ”?:” ternary operator?",
    "question_score": 8089,
    "answer_score": 9311,
    "question_id": 394809,
    "has_full_answer": true,
    "tags": [
      "python",
      "operators",
      "conditional-operator"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I access the index value in a 'for' loop?",
    "response": "Use the built-in function `enumerate()`:\nfor idx, x in enumerate(xs):\n```bash\nprint(idx, x)\n```\n\nIt is non-Pythonic to manually index via `for i in range(len(xs)): x = xs[i]` or manually manage an additional state variable.\nCheck out PEP 279 for more.",
    "question_score": 5637,
    "answer_score": 9190,
    "question_id": 522563,
    "has_full_answer": true,
    "tags": [
      "python",
      "loops",
      "list"
    ],
    "topic": "python"
  },
  {
    "instruction": "What does if __name__ == \"__main__\": do?",
    "response": "Short Answer\nIt's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:\n\nIf you import the guardless script in another script (e.g. `import my_script_without_a_name_eq_main_guard`), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.\n\nIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.\n\nLong Answer\nTo better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.\nWhenever the Python interpreter reads a source file, it does two things:\n\nit sets a few special variables like `__name__`, and then\n\nit executes all of the code found in the file.\n\nLet's see how this works and how it relates to your question about the `__name__` checks we always see in Python scripts.\nCode Sample\nLet's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called `foo.py`.\n# Suppose this is foo.py.\n\nprint(\"before import\")\nimport math\n\nprint(\"before function_a\")\ndef function_a():\n```bash\nprint(\"Function A\")\n```\n\nprint(\"before function_b\")\ndef function_b():\n```bash\nprint(\"Function B {}\".format(math.sqrt(100)))\n```\n\nprint(\"before __name__ guard\")\nif __name__ == '__main__':\n```bash\nfunction_a()\nfunction_b()\n```\nprint(\"after __name__ guard\")\n\nSpecial Variables\nWhen the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the `__name__` variable.\nWhen Your Module Is the Main Program\nIf you are running your module (the source file) as the main program, e.g.\npython foo.py\n\nthe interpreter will assign the hard-coded string `\"__main__\"` to the `__name__` variable, i.e.\n# It's as if the interpreter inserts this at the top\n# of your module when run as the main program.\n__name__ = \"__main__\" \n\nWhen Your Module Is Imported By Another\nOn the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:\n# Suppose this is in some other main program.\nimport foo\n\nThe interpreter will search for your `foo.py` file (along with searching for a few other variants), and prior to executing that module, it will assign the name `\"foo\"` from the import statement to the `__name__` variable, i.e.\n# It's as if the interpreter inserts this at the top\n# of your module when it's imported from another module.\n__name__ = \"foo\"\n\nExecuting the Module's Code\nAfter the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.\nAlways\n\nIt prints the string `\"before import\"` (without quotes).\n\nIt loads the `math` module and assigns it to a variable called `math`. This is equivalent to replacing `import math` with the following (note that `__import__` is a low-level function in Python that takes a string and triggers the actual import):\n\n# Find and load a module given its string name, \"math\",\n# then assign it to a local variable called math.\nmath = __import__(\"math\")\n\nIt prints the string `\"before function_a\"`.\n\nIt executes the `def` block, creating a function object, then assigning that function object to a variable called `function_a`.\n\nIt prints the string `\"before function_b\"`.\n\nIt executes the second `def` block, creating another function object, then assigning it to a variable called `function_b`.\n\nIt prints the string `\"before __name__ guard\"`.\n\nOnly When Your Module Is the Main Program\n\nIf your module is the main program, then it will see that `__name__` was indeed set to `\"__main__\"` and it calls the two functions, printing the strings `\"Function A\"` and `\"Function B 10.0\"`.\n\nOnly When Your Module Is Imported by Another\n\n(instead) If your module is not the main program but was imported by another one, then `__name__` will be `\"foo\"`, not `\"__main__\"`, and it'll skip the body of the `if` statement.\n\nAlways\n\nIt will print the string `\"after __name__ guard\"` in both situations.\n\nSummary\nIn summary, here's what'd be printed in the two cases:\n# What gets printed if foo is the main program\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nFunction A\nFunction B 10.0\nafter __name__ guard\n\n# What gets printed if foo is imported as a regular module\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nafter __name__ guard\n\nWhy Does It Work This Way?\nYou might naturally wonder why anybody would want this.  Well, sometimes you want to write a `.py` file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:\n\nYour module is a library, but you want to have a script mode where it runs some unit tests or a demo.\n\nYour module is only used as a main program, but it has some unit tests, and the testing framework works by importing `.py` files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.\n\nYour module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.\n\nBeyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script's module.\nFood for Thought\n\nQuestion: Can I have multiple `__name__` checking blocks?  Answer: it's strange to do so, but the language won't stop you.\n\nSuppose the following is in `foo2.py`.  What happens if you say `python foo2.py` on the command-line? Why?\n\n# Suppose this is foo2.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n```bash\nprint(\"a1\")\nfrom foo2 import function_b\nprint(\"a2\")\nfunction_b()\nprint(\"a3\")\n```\n\ndef function_b():\n```bash\nprint(\"b\")\n```\n\nprint(\"t1\")\nif __name__ == \"__main__\":\n```bash\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\n```\nprint(\"t2\")\n      \n\nNow, figure out what will happen in `foo3.py` (having removed the `__name__` check):\n\n# Suppose this is foo3.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n```bash\nprint(\"a1\")\nfrom foo3 import function_b\nprint(\"a2\")\nfunction_b()\nprint(\"a3\")\n```\n\ndef function_b():\n```bash\nprint(\"b\")\n```\n\nprint(\"t1\")\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\nprint(\"t2\")\n\nWhat will this do when used as a script?  When imported as a module?\n\n# Suppose this is in foo4.py\n__name__ = \"__main__\"\n\ndef bar():\n```bash\nprint(\"bar\")\n```\n    \nprint(\"before __name__ guard\")\nif __name__ == \"__main__\":\n```bash\nbar()\n```\nprint(\"after __name__ guard\")",
    "question_score": 8386,
    "answer_score": 8984,
    "question_id": 419163,
    "has_full_answer": true,
    "tags": [
      "python",
      "namespaces",
      "program-entry-point",
      "python-module",
      "idioms"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I make a flat list out of a list of lists?",
    "response": "A list of lists named `xss` can be flattened using a nested list comprehension:\nflat_list = [\n```bash\nx\nfor xs in xss\nfor x in xs\n```\n]\n\nThe above is equivalent to:\nflat_list = []\n\nfor xs in xss:\n```bash\nfor x in xs:\n```\n        flat_list.append(x)\n\nHere is the corresponding function:\ndef flatten(xss):\n```bash\nreturn [x for xs in xss for x in xs]\n```\n\nThis is the fastest method.\nAs evidence, using the `timeit` module in the standard library, we see:\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' '[x for xs in xss for x in xs]'\n10000 loops, best of 3: 143 usec per loop\n\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'sum(xss, [])'\n1000 loops, best of 3: 969 usec per loop\n\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'reduce(lambda xs, ys: xs + ys, xss)'\n1000 loops, best of 3: 1.1 msec per loop\n\nExplanation: the methods based on `+` (including the implied use in `sum`) are, of necessity, `O(L**2)` when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of M items each: the first M items are copied back and forth `L-1` times, the second M items `L-2` times, and so on; total number of copies is M times the sum of x for x from 1 to L excluded, i.e., `M * (L**2)/2`.\nThe list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once.",
    "question_score": 5474,
    "answer_score": 7465,
    "question_id": 952914,
    "has_full_answer": true,
    "tags": [
      "python",
      "list",
      "multidimensional-array",
      "flatten"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I create a directory, and any missing parent directories?",
    "response": "On Python ≥ 3.5, use `pathlib.Path.mkdir`:\n\nfrom pathlib import Path\nPath(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n\nFor older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:\n\nTry `os.path.exists`, and consider `os.makedirs` for the creation.\n\nimport os\nif not os.path.exists(directory):\n```bash\nos.makedirs(directory)\n```\n\nAs noted in comments and elsewhere, there's a race condition – if the directory is created between the `os.path.exists` and the `os.makedirs` calls, the `os.makedirs` will fail with an `OSError`. Unfortunately, blanket-catching `OSError` and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.\n\nOne option would be to trap the `OSError` and examine the embedded error code (see Is there a cross-platform way of getting information from Python’s OSError):\n\nimport os, errno\n\ntry:\n```bash\nos.makedirs(directory)\n```\nexcept OSError as e:\n```bash\nif e.errno != errno.EEXIST:\n```\n        raise\n\nAlternatively, there could be a second `os.path.exists`, but suppose another created the directory after the first check, then removed it before the second one – we could still be fooled. \n\nDepending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.\n\nModern versions of Python improve this code quite a bit, both by exposing `FileExistsError` (in 3.3+)...\n\ntry:\n```bash\nos.makedirs(\"path/to/directory\")\n```\nexcept FileExistsError:\n```bash\n# directory already exists\npass\n```\n\n...and by allowing a keyword argument to `os.makedirs` called `exist_ok` (in 3.2+).\n\nos.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.",
    "question_score": 5794,
    "answer_score": 7259,
    "question_id": 273192,
    "has_full_answer": true,
    "tags": [
      "python",
      "exception",
      "path",
      "directory",
      "operating-system"
    ],
    "topic": "python"
  },
  {
    "instruction": "Iterating over dictionaries using 'for' loops",
    "response": "`key` is just a variable name.  \n\nfor key in d:\n\nwill simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:\n\nFor Python 3.x:\n\nfor key, value in d.items():\n\nFor Python 2.x:\n\nfor key, value in d.iteritems():\n\nTo test for yourself, change the word `key` to `poop`.\n\nIn Python 3.x, `iteritems()` was replaced with simply `items()`, which returns a set-like view backed by the dict, like `iteritems()` but even better. \nThis is also available in 2.7 as `viewitems()`. \n\nThe operation `items()` will work for both 2 and 3, but in 2 it will return a list of the dictionary's `(key, value)` pairs, which will not reflect changes to the dict that happen after the `items()` call. If you want the 2.x behavior in 3.x, you can call `list(d.items())`.",
    "question_score": 4401,
    "answer_score": 6996,
    "question_id": 3294889,
    "has_full_answer": true,
    "tags": [
      "python",
      "loops",
      "dictionary",
      "key"
    ],
    "topic": "python"
  },
  {
    "instruction": "How slicing in Python works",
    "response": "The syntax is:\na[start:stop]  # items start through stop-1\na[start:]      # items start through the rest of the array\na[:stop]       # items from the beginning through stop-1\na[:]           # a copy of the whole array\n\nThere is also the `step` value, which can be used with any of the above:\na[start:stop:step] # start through not past stop, by step\n\nThe key point to remember is that the `:stop` value represents the first value that is not in the selected slice. So, the difference between `stop` and `start` is the number of elements selected (if `step` is 1, the default).\nThe other feature is that `start` or `stop` may be a negative number, which means it counts from the end of the array instead of the beginning. So:\na[-1]    # last item in the array\na[-2:]   # last two items in the array\na[:-2]   # everything except the last two items\n\nSimilarly, `step` may be a negative number:\na[::-1]    # all items in the array, reversed\na[1::-1]   # the first two items, reversed\na[:-3:-1]  # the last two items, reversed\na[-3::-1]  # everything except the last two items, reversed\n\nPython is kind to the programmer if there are fewer items than you ask for. For example, if you ask for `a[:-2]` and `a` only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.\nRelationship with the `slice` object\nA `slice` object can represent a slicing operation, i.e.:\na[start:stop:step]\n\nis equivalent to:\na[slice(start, stop, step)]\n\nSlice objects also behave slightly differently depending on the number of arguments, similar to `range()`, i.e. both `slice(stop)` and `slice(start, stop[, step])` are supported.\nTo skip specifying a given argument, one might use `None`, so that e.g. `a[start:]` is equivalent to `a[slice(start, None)]` or `a[::-1]` is equivalent to `a[slice(None, None, -1)]`.\nWhile the `:`-based notation is very helpful for simple slicing, the explicit use of `slice()` objects simplifies the programmatic generation of slicing.",
    "question_score": 4682,
    "answer_score": 6624,
    "question_id": 509211,
    "has_full_answer": true,
    "tags": [
      "python",
      "slice",
      "sequence"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I find the index for a given item in a list?",
    "response": ">>> [\"foo\", \"bar\", \"baz\"].index(\"bar\")\n1\n\nSee the documentation for the built-in `.index()` method of the list:\n\nlist.index(x[, start[, end]])\n\nReturn zero-based index in the list of the first item whose value is equal to x. Raises a `ValueError` if there is no such item.\nThe optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.\n\nCaveats\nLinear time-complexity in list length\nAn `index` call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.\nThis problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the `start` and `end` parameters can be used to narrow the search.\nFor example:\n>>> import timeit\n>>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000)\n9.356267921015387\n>>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000)\n0.0004404920036904514\n\nThe second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.\nOnly the index of the first match is returned\nA call to `index` searches through the list in order until it finds a match, and stops there. If there could be more than one occurrence of the value, and all indices are needed, `index` cannot solve the problem:\n>>> [1, 1].index(1) # the `1` index is not found.\n0\n\nInstead, use a list comprehension or generator expression to do the search, with `enumerate` to get indices:\n>>> # A list comprehension gives a list of indices directly:\n>>> [i for i, e in enumerate([1, 2, 1]) if e == 1]\n[0, 2]\n>>> # A generator comprehension gives us an iterable object...\n>>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1)\n>>> # which can be used in a `for` loop, or manually iterated with `next`:\n>>> next(g)\n0\n>>> next(g)\n2\n\nThe list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.\nRaises an exception if there is no match\nAs noted in the documentation above, using `.index` will raise an exception if the searched-for value is not in the list:\n>>> [1, 1].index(2)\nTraceback (most recent call last):\n  File \"\", line 1, in \nValueError: 2 is not in list\n\nIf this is a concern, either explicitly check first using `item in my_list`, or handle the exception with `try`/`except` as appropriate.\nThe explicit check is simple and readable, but it must iterate the list a second time. See What is the EAFP principle in Python? for more guidance on this choice.",
    "question_score": 4474,
    "answer_score": 6090,
    "question_id": 176918,
    "has_full_answer": true,
    "tags": [
      "python",
      "list",
      "indexing"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I execute a program or call a system command?",
    "response": "Use `subprocess.run`:\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \n\nAnother common way is `os.system` but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also `subprocess.run` is generally more flexible (you can get the `stdout`, `stderr`, the \"real\" status code, better error handling, etc.). Even the documentation for `os.system` recommends using `subprocess` instead.\nOn Python 3.4 and earlier, use `subprocess.call` instead of `.run`:\nsubprocess.call([\"ls\", \"-l\"])",
    "question_score": 6263,
    "answer_score": 5938,
    "question_id": 89228,
    "has_full_answer": true,
    "tags": [
      "python",
      "shell",
      "terminal",
      "subprocess",
      "command"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I iterate over rows in a Pandas DataFrame?",
    "response": "`DataFrame.iterrows` is a generator which yields both the index and row (as a Series):\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n```bash\nprint(row['c1'], row['c2'])\n```\n\n10 100\n11 110\n12 120\n\nObligatory disclaimer from the documentation\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n\nLook for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, …\nWhen you have a function that cannot work on the full DataFrame/Series at once, it is better to use `apply()` instead of iterating over the values. See the docs on function application.\nIf you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.\n\nOther answers in this thread delve into greater depth on alternatives to iter* functions if you are interested to learn more.",
    "question_score": 4225,
    "answer_score": 5485,
    "question_id": 16476924,
    "has_full_answer": true,
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "loops"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I use a global variable in a function?",
    "response": "You can use a global variable within other functions by declaring it as `global` within each function that assigns a value to it:\nglobvar = 0\n\ndef set_globvar_to_one():\n```bash\nglobal globvar    # Needed to modify global copy of globvar\nglobvar = 1\n```\n\ndef print_globvar():\n```bash\nprint(globvar)     # No need for global declaration to read value of globvar\n```\n\nset_globvar_to_one()\nprint_globvar()       # Prints 1\n\nSince it's unclear whether `globvar = 1` is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the `global` keyword.\nSee other answers if you want to share a global variable across modules.",
    "question_score": 4039,
    "answer_score": 5268,
    "question_id": 423379,
    "has_full_answer": true,
    "tags": [
      "python",
      "global-variables",
      "scope"
    ],
    "topic": "python"
  },
  {
    "instruction": "What is the difference between @staticmethod and @classmethod in Python?",
    "response": "Maybe a bit of example code will help: Notice the difference in the call signatures of `foo`, `class_foo` and `static_foo`:\nclass A(object):\n```bash\ndef foo(self, x):\n```\n        print(f\"executing foo({self}, {x})\")\n\n```bash\n@classmethod\ndef class_foo(cls, x):\n```\n        print(f\"executing class_foo({cls}, {x})\")\n\n```bash\n@staticmethod\ndef static_foo(x):\n```\n        print(f\"executing static_foo({x})\")\n\na = A()\n\nBelow is the usual way an object instance calls a method. The object instance, `a`, is implicitly passed as the first argument.\na.foo(1)\n# executing foo(, 1)\n\nWith classmethods, the class of the object instance is implicitly passed as the first argument instead of `self`.\na.class_foo(1)\n# executing class_foo(, 1)\n\nYou can also call `class_foo` using the class. In fact, if you define something to be\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. `A.foo(1)` would have raised a TypeError, but `A.class_foo(1)` works just fine:\nA.class_foo(1)\n# executing class_foo(, 1)\n\nOne use people have found for class methods is to create inheritable alternative constructors.\n\nWith staticmethods, neither `self` (the object instance) nor  `cls` (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:\na.static_foo(1)\n# executing static_foo(1)\n\nA.static_foo('hi')\n# executing static_foo(hi)\n\nStaticmethods are used to group functions which have some logical connection with a class to the class.\n\n`foo` is just a function, but when you call `a.foo` you don't just get the function,\nyou get a \"partially applied\" version of the function with the object instance `a` bound as the first argument to the function. `foo` expects 2 arguments, while `a.foo` only expects 1 argument.\n`a` is bound to `foo`. That is what is meant by the term \"bound\" below:\nprint(a.foo)\n# >\n\nWith `a.class_foo`, `a` is not bound to `class_foo`, rather the class `A` is bound to `class_foo`.\nprint(a.class_foo)\n# >\n\nHere, with a staticmethod, even though it is a method, `a.static_foo` just returns\na good 'ole function with no arguments bound. `static_foo` expects 1 argument, and\n`a.static_foo` expects 1 argument too.\nprint(a.static_foo)\n# \n\nAnd of course the same thing happens when you call `static_foo` with the class `A` instead.\nprint(A.static_foo)\n#",
    "question_score": 4762,
    "answer_score": 3992,
    "question_id": 136097,
    "has_full_answer": true,
    "tags": [
      "python",
      "oop",
      "static-methods",
      "python-decorators",
      "class-method"
    ],
    "topic": "python"
  },
  {
    "instruction": "What are metaclasses in Python?",
    "response": "A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.\n\nWhile in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. `type` is the usual metaclass in Python. `type` is itself a class, and it is its own type. You won't be able to recreate something like `type` purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass `type`.\n\nA metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal `__init__` and `__new__` methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.\n\nWhen the `class` statement is executed, Python first executes the body of the `class` statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the `__metaclass__` attribute of the class-to-be (if any) or the `__metaclass__` global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.\n\nHowever, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. `type.__subclasses__()` is an example of a method on the `type` metaclass. You can also define the normal 'magic' methods, like `__add__`, `__iter__` and `__getattr__`, to implement or change how the class behaves.\n\nHere's an aggregated example of the bits and pieces:\n\ndef make_hook(f):\n```bash\n\"\"\"Decorator to turn 'foo' method into '__foo__'\"\"\"\nf.is_hook = 1\nreturn f\n```\n\nclass MyType(type):\n```bash\ndef __new__(mcls, name, bases, attrs):\n```\n\n        if name.startswith('None'):\n            return None\n\n        # Go over attributes and see if they should be renamed.\n        newattrs = {}\n        for attrname, attrvalue in attrs.iteritems():\n            if getattr(attrvalue, 'is_hook', 0):\n                newattrs['__%s__' % attrname] = attrvalue\n            else:\n                newattrs[attrname] = attrvalue\n\n        return super(MyType, mcls).__new__(mcls, name, bases, newattrs)\n\n```bash\ndef __init__(self, name, bases, attrs):\n```\n        super(MyType, self).__init__(name, bases, attrs)\n\n        # classregistry.register(self, self.interfaces)\n        print \"Would register class %s now.\" % self\n\n```bash\ndef __add__(self, other):\n```\n        class AutoClass(self, other):\n            pass\n        return AutoClass\n        # Alternatively, to autogenerate the classname as well as the class:\n        # return type(self.__name__ + other.__name__, (self, other), {})\n\n```bash\ndef unregister(self):\n```\n        # classregistry.unregister(self)\n        print \"Would unregister class %s now.\" % self\n\nclass MyObject:\n```bash\n__metaclass__ = MyType\n```\n\nclass NoneSample(MyObject):\n```bash\npass\n```\n\n# Will print \"NoneType None\"\nprint type(NoneSample), repr(NoneSample)\n\nclass Example(MyObject):\n```bash\ndef __init__(self, value):\n```\n        self.value = value\n```bash\n@make_hook\ndef add(self, other):\n```\n        return self.__class__(self.value + other.value)\n\n# Will unregister the class\nExample.unregister()\n\ninst = Example(10)\n# Will fail with an AttributeError\n#inst.unregister()\n\nprint inst + inst\nclass Sibling(MyObject):\n```bash\npass\n```\n\nExampleSibling = Example + Sibling\n# ExampleSibling is now a subclass of both Example and Sibling (with no\n# content of its own) although it will believe it's called 'AutoClass'\nprint ExampleSibling\nprint ExampleSibling.__mro__",
    "question_score": 7503,
    "answer_score": 3438,
    "question_id": 100003,
    "has_full_answer": true,
    "tags": [
      "python",
      "oop",
      "metaclass",
      "python-class",
      "python-datamodel"
    ],
    "topic": "python"
  },
  {
    "instruction": "Does Python have a string 'contains' substring method?",
    "response": "Use the `in` operator:\nif \"blah\" not in somestring: \n```bash\ncontinue\n```\n\nNote: This is case-sensitive.",
    "question_score": 3586,
    "answer_score": 8614,
    "question_id": 3437059,
    "has_full_answer": true,
    "tags": [
      "python",
      "string",
      "substring",
      "contains"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I sort a dictionary by value?",
    "response": "Python 3.7+ or CPython 3.6\nDicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but it's an implementation detail.\n>>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n\nor\n>>> dict(sorted(x.items(), key=lambda item: item[1]))\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n\nOlder Python\nIt is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list—probably a list of tuples.\nFor instance,\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\n\n`sorted_x` will be a list of tuples sorted by the second element in each tuple. `dict(sorted_x) == x`.\nAnd for those wishing to sort on keys instead of values:\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(0))\n\nIn Python3 since unpacking is not allowed we can use\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=lambda kv: kv[1])\n\nIf you want the output as a dict, you can use `collections.OrderedDict`:\nimport collections\n\nsorted_dict = collections.OrderedDict(sorted_x)",
    "question_score": 3413,
    "answer_score": 7060,
    "question_id": 613183,
    "has_full_answer": true,
    "tags": [
      "python",
      "sorting",
      "dictionary"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I select rows from a DataFrame based on column values?",
    "response": "To select rows whose column value equals a scalar, `some_value`, use `==`:\ndf.loc[df['column_name'] == some_value]\n\nTo select rows whose column value is in an iterable, `some_values`, use `isin`:\ndf.loc[df['column_name'].isin(some_values)]\n\nCombine multiple conditions with `&`:\ndf.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\n\nNote the parentheses. Due to Python's operator precedence rules, `&` binds more tightly than `=`. Thus, the parentheses in the last example are necessary. Without the parentheses\ndf['column_name'] >= A & df['column_name'] <= B\n\nis parsed as\ndf['column_name'] >= (A & df['column_name']) <= B\n\nwhich results in a Truth value of a Series is ambiguous error.\n\nTo select rows whose column value does not equal `some_value`, use `!=`:\ndf.loc[df['column_name'] != some_value]\n\nThe `isin` returns a boolean Series, so to select rows whose value is not in `some_values`, negate the boolean Series using `~`:\ndf = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place replacement\n\nFor example,\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n\nyields\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nIf you have multiple values you want to include, put them in a\nlist (or more generally, any iterable) and use `isin`:\nprint(df.loc[df['B'].isin(['one','three'])])\n\nyields\n     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n\nNote, however, that if you wish to do this many times, it is more efficient to\nmake an index first, and then use `df.loc`:\ndf = df.set_index(['B'])\nprint(df.loc['one'])\n\nyields\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n\nor, to include multiple values from the index use `df.index.isin`:\ndf.loc[df.index.isin(['one','two'])]\n\nyields\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12",
    "question_score": 3614,
    "answer_score": 6608,
    "question_id": 17071871,
    "has_full_answer": true,
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "indexing",
      "filter"
    ],
    "topic": "python"
  },
  {
    "instruction": "Convert bytes to a string in Python 3",
    "response": "Decode the `bytes` object to produce a string:\n>>> b\"abcde\".decode(\"utf-8\")\n'abcde'\n\nThe above example assumes that the `bytes` object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!",
    "question_score": 3866,
    "answer_score": 5847,
    "question_id": 606191,
    "has_full_answer": true,
    "tags": [
      "python",
      "string",
      "python-3.x"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I catch multiple exceptions in one line? (in the \"except\" block)",
    "response": "From Python Documentation:\n\n  An except clause may name multiple exceptions as a parenthesized tuple, for example\n\nexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n```bash\npass\n```\n\nOr, for Python 2 only:\n\nexcept (IDontLikeYouException, YouAreBeingMeanException), e:\n```bash\npass\n```\n\nSeparating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using `as`.",
    "question_score": 3941,
    "answer_score": 5243,
    "question_id": 6470428,
    "has_full_answer": true,
    "tags": [
      "python",
      "exception"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I delete a file or folder in Python?",
    "response": "Use one of these methods:\n\n`pathlib.Path.unlink()` removes a file or symbolic link.\n\n`pathlib.Path.rmdir()` removes an empty directory.\n\n`shutil.rmtree()` deletes a directory and all its contents.\n\nOn Python 3.3 and below, you can use these methods instead of the `pathlib` ones:\n\n`os.remove()` removes a file.\n\n`os.unlink()` removes a symbolic link.\n\n`os.rmdir()` removes an empty directory.",
    "question_score": 3515,
    "answer_score": 5009,
    "question_id": 6996603,
    "has_full_answer": true,
    "tags": [
      "python",
      "file-io",
      "directory",
      "delete-file"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I copy a file?",
    "response": "`shutil` has many methods you can use. One of which is:\nimport shutil\n\nshutil.copyfile(src, dst)\n\n# 2nd option\nshutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp\n\nCopy the contents of the file named `src` to a file named `dst`. Both `src` and `dst` need to be the entire filename of the files, including path.\nThe destination location must be writable; otherwise, an `IOError` exception will be raised.\nIf `dst` already exists, it will be replaced.\nSpecial files such as character or block devices and pipes cannot be copied with this function.\nWith `copy`, `src` and `dst` are path names given as `str`s.\n\nAnother `shutil` method to look at is `shutil.copy2()`. It's similar but preserves more metadata (e.g. time stamps).\nIf you use `os.path` operations, use `copy` rather than `copyfile`. `copyfile` will only accept strings.",
    "question_score": 3928,
    "answer_score": 4835,
    "question_id": 123198,
    "has_full_answer": true,
    "tags": [
      "python",
      "file",
      "copy",
      "filesystems",
      "file-copying"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I access environment variables in Python?",
    "response": "Environment variables are accessed through `os.environ`:\nimport os\nprint(os.environ['HOME'])\n\nTo see a list of all environment variables:\nprint(os.environ)\n\nIf a key is not present, attempting to access it will raise a `KeyError`. To avoid this:\n# Returns `None` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST'))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))",
    "question_score": 3433,
    "answer_score": 4806,
    "question_id": 4906977,
    "has_full_answer": true,
    "tags": [
      "python",
      "environment-variables"
    ],
    "topic": "python"
  },
  {
    "instruction": "How can I add new keys to a dictionary?",
    "response": "You create a new key/value pair on a dictionary by assigning a value to that key\nd = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n\nIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten.",
    "question_score": 3590,
    "answer_score": 4446,
    "question_id": 1024847,
    "has_full_answer": true,
    "tags": [
      "python",
      "dictionary",
      "key",
      "lookup"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I clone a list so that it doesn't change unexpectedly after assignment?",
    "response": "`new_list = my_list` doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both `new_list` and `my_list` refer to the same list after the assignment.\nTo actually copy the list, you have several options:\n\nYou can use the built-in `list.copy()` method (available since Python 3.3):\nnew_list = old_list.copy()\n\nYou can slice it:\nnew_list = old_list[:]\n\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).\n\nYou can use the built-in `list()` constructor:\nnew_list = list(old_list)\n\nYou can use generic `copy.copy()`:\nimport copy\nnew_list = copy.copy(old_list)\n\nThis is a little slower than `list()` because it has to find out the datatype of `old_list` first.\n\nIf you need to copy the elements of the list as well, use generic `copy.deepcopy()`:\nimport copy\nnew_list = copy.deepcopy(old_list)\n\nObviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).\n\nExample:\nimport copy\n\nclass Foo(object):\n```bash\ndef __init__(self, val):\n```\n         self.val = val\n\n```bash\ndef __repr__(self):\n```\n        return f'Foo({self.val!r})'\n\nfoo = Foo(1)\n\na = ['foo', foo]\nb = a.copy()\nc = a[:]\nd = list(a)\ne = copy.copy(a)\nf = copy.deepcopy(a)\n\n# edit orignal list and instance \na.append('baz')\nfoo.val = 5\n\nprint(f'original: {a}\\nlist.copy(): {b}\\nslice: {c}\\nlist(): {d}\\ncopy: {e}\\ndeepcopy: {f}')\n\nResult:\noriginal: ['foo', Foo(5), 'baz']\nlist.copy(): ['foo', Foo(5)]\nslice: ['foo', Foo(5)]\nlist(): ['foo', Foo(5)]\ncopy: ['foo', Foo(5)]\ndeepcopy: ['foo', Foo(1)]",
    "question_score": 3360,
    "answer_score": 4141,
    "question_id": 2612802,
    "has_full_answer": true,
    "tags": [
      "python",
      "list",
      "clone",
      "mutable"
    ],
    "topic": "python"
  },
  {
    "instruction": "How do I get the current time in Python?",
    "response": "Use `datetime`:\n>>> import datetime\n>>> now = datetime.datetime.now()\n>>> now\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(now)\n2009-01-06 15:08:24.789150\n\nFor just the clock time without the date:\n>>> now.time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(now.time())\n15:08:24.789150\n\nTo save typing, you can import the `datetime` object from the `datetime` module:\n>>> from datetime import datetime\n\nThen remove the prefix `datetime.` from all of the above.",
    "question_score": 3971,
    "answer_score": 4011,
    "question_id": 415511,
    "has_full_answer": true,
    "tags": [
      "python",
      "datetime",
      "time"
    ],
    "topic": "python"
  },
  {
    "instruction": "What is the difference between __str__ and __repr__?",
    "response": "Alex Martelli summarized well but, surprisingly, was too succinct.\nFirst, let me reiterate the main points in Alex’s post:\n\nThe default implementation is useless (it’s hard to think of one which wouldn’t be, but yeah)\n`__repr__` goal is to be unambiguous\n`__str__` goal is to be readable\nContainer’s `__str__` uses contained objects’ `__repr__`\n\nDefault implementation is useless\nThis is mostly a surprise because Python’s defaults tend to be fairly useful. However, in this case, having a default for `__repr__` which would act like:\nreturn \"%s(%r)\" % (self.__class__, self.__dict__)\n\nOr in new f-string formatting:\nreturn f\"{self.__class__!s}({self.__dict__!r})\"\n\nwould have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if `__repr__` is defined, and `__str__` is not, the object will behave as though `__str__=__repr__`.\nThis means, in simple terms: almost every object you implement should have a functional `__repr__` that’s usable for understanding the object. Implementing `__str__` is optional: do that if you need a “pretty print” functionality (for example, used by a report generator).\nThe goal of `__repr__` is to be unambiguous\nLet me come right out and say it — I do not believe in debuggers. I don’t really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature — most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is a\nlog(INFO, \"I am in the weird function and a is\", a, \"and b is\", b, \"but I got a null C — using default\", default_c)\n\nBut you have to do the last step — make sure every object you implement has a useful repr, so code like that can just work. This is why the “eval” thing comes up: if you have enough information so `eval(repr(c))==c`, that means you know everything there is to know about `c`. If that’s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about `c` anyway. I usually use an eval-like format: `\"MyClass(this=%r,that=%r)\" % (self.this,self.that)`. It does not mean that you can actually construct MyClass, or that those are the right constructor arguments — but it is a useful form to express “this is everything you need to know about this instance”.\nNote: I used `%r` above, not `%s`. You always want to use `repr()` [or `%r` formatting character, equivalently] inside `__repr__` implementation, or you’re defeating the goal of repr. You want to be able to differentiate `MyClass(3)` and `MyClass(\"3\")`.\nThe goal of `__str__` is to be readable\nSpecifically, it is not intended to be unambiguous — notice that `str(3)==str(\"3\")`. Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be \"2010/4/12 15:35:22\", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class — as long is it supports readability, it is an improvement.\nContainer’s `__str__` uses contained objects’ `__repr__`\nThis seems surprising, doesn’t it? It is a little, but how readable would it be if it used their `__str__`?\n[moshe is, 3, hello\nworld, this is a list, oh I don't know, containing just 4 elements]\n\nNot very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you’re printing a list, just\nprint(\"[\" + \", \".join(lst) + \"]\")\n\n(you can probably also figure out what to do about dictionaries).\nSummary\nImplement `__repr__` for any class you implement. This should be second nature. Implement `__str__` if you think it would be useful to have a string version which errs on the side of readability.",
    "question_score": 3806,
    "answer_score": 3591,
    "question_id": 1436703,
    "has_full_answer": true,
    "tags": [
      "python",
      "magic-methods",
      "repr"
    ],
    "topic": "python"
  },
  {
    "instruction": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?",
    "response": "The `*args` and `**kwargs` are common idioms to allow an arbitrary number of arguments to functions, as described in the section more on defining functions in the Python tutorial.\nThe `*args` will give you all positional arguments as a tuple:\ndef foo(*args):\n```bash\nfor a in args:\n```\n        print(a)        \n\nfoo(1)\n# 1\n\nfoo(1, 2, 3)\n# 1\n# 2\n# 3\n\nThe `**kwargs` will give you all\nkeyword arguments as a dictionary:\ndef bar(**kwargs):\n```bash\nfor a in kwargs:\n```\n        print(a, kwargs[a])  \n\nbar(name='one', age=27)\n# name one\n# age 27\n\nBoth idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:\ndef foo(kind, *args, bar=None, **kwargs):\n```bash\nprint(kind, args, bar, kwargs)\n```\n\nfoo(123, 'a', 'b', apple='red')\n# 123 ('a', 'b') None {'apple': 'red'}\n\nIt is also possible to use this the other way around:\ndef foo(a, b, c):\n```bash\nprint(a, b, c)\n```\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100, **obj)\n# 100 10 lee\n\nAnother usage of the `*l` idiom is to unpack argument lists when calling a function.\ndef foo(bar, lee):\n```bash\nprint(bar, lee)\n```\n\nbaz = [1, 2]\n\nfoo(*baz)\n# 1 2\n\nIn Python 3 it is possible to use `*l` on the left side of an assignment (Extended Iterable Unpacking), though it gives a list instead of a tuple in this context:\nfirst, *rest = [1, 2, 3, 4]\n# first = 1\n# rest = [2, 3, 4]\n\nAlso Python 3 adds a new semantic (refer PEP 3102):\ndef func(arg1, arg2, arg3, *, kwarg1, kwarg2):\n```bash\npass\n```\n\nSuch function accepts only 3 positional arguments, and everything after `*` can only be passed as keyword arguments.\nNote:\nA Python `dict`, semantically used for keyword argument passing, is arbitrarily ordered. However, in Python 3.6+, keyword arguments are guaranteed to remember insertion order.\n\"The order of elements in `**kwargs` now corresponds to the order in which keyword arguments were passed to the function.\" - What’s New In Python 3.6.\nIn fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, and this becomes standard in Python 3.7.",
    "question_score": 3480,
    "answer_score": 3322,
    "question_id": 36901,
    "has_full_answer": true,
    "tags": [
      "python",
      "syntax",
      "parameter-passing",
      "variadic-functions",
      "argument-unpacking"
    ],
    "topic": "python"
  },
  {
    "instruction": "What is __init__.py for?",
    "response": "It used to be a required part of a package (old, pre-3.3 \"regular package\", not newer 3.3+ \"namespace package\").\n\nHere's the documentation.\n\n  Python defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an `__init__.py` file. When a regular package is imported, this `__init__.py` file is implicitly executed, and the objects it defines are bound to names in the package’s namespace. The `__init__.py` file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.\n\nBut just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without `__init__.py`.",
    "question_score": 3887,
    "answer_score": 2088,
    "question_id": 448271,
    "has_full_answer": true,
    "tags": [
      "python",
      "module",
      "package",
      "python-packaging"
    ],
    "topic": "python"
  },
  {
    "instruction": "\"Least Astonishment\" and the Mutable Default Argument",
    "response": "Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.\nAs soon as you think of it this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of \"member data\" and therefore their state may change from one call to the other - exactly as in any other object.\nIn any case, the Effbot (Fredrik Lundh) has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python. I found it very clear, and I really suggest reading it for a better knowledge of how function objects work.",
    "question_score": 3495,
    "answer_score": 1955,
    "question_id": 1132941,
    "has_full_answer": true,
    "tags": [
      "python",
      "language-design",
      "default-parameters",
      "least-astonishment"
    ],
    "topic": "python"
  },
  {
    "instruction": "From inside of a Docker container, how do I connect to the localhost of the machine?",
    "response": "If you are using Docker-for-mac or Docker-for-Windows 18.03+, connect to your MySQL service using the host `host.docker.internal` (instead of the `127.0.0.1` in your connection string).\nIf you are using Docker-for-Linux 20.10.0+, you can also use the host `host.docker.internal` if you started your Docker container with the `--add-host host.docker.internal:host-gateway` option, or added the following snippet in your docker-compose.yml file:\nextra_hosts:\n```bash\n- \"host.docker.internal:host-gateway\"\n```\n\nOtherwise, read below\n\nTLDR\nUse `--network=\"host\"` in your `docker run` command, then `127.0.0.1` in your Docker container will point to your Docker host.\nNote: This mode only works on Docker for Linux, per the documentation.\n\nNote on Docker container networking modes\nDocker offers different networking modes when running containers. Depending on the mode you choose you would connect to your MySQL database running on the Docker host differently.\ndocker run --network=\"bridge\" (default)\nDocker creates a bridge named `docker0` by default. Both the Docker host and the Docker containers have an IP address on that bridge.\nOn the Docker host, type `sudo ip addr show docker0` you will have an output looking like:\n[vagrant@docker:~] $ sudo ip addr show docker0\n4: docker0:  mtu 1500 qdisc noqueue state UP group default\n```bash\nlink/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\ninet 172.17.42.1/16 scope global docker0\n```\n       valid_lft forever preferred_lft forever\n```bash\ninet6 fe80::5484:7aff:fefe:9799/64 scope link\n```\n       valid_lft forever preferred_lft forever\n\nSo here my Docker host has the IP address `172.17.42.1` on the `docker0` network interface.\nNow start a new container and get a shell on it: `docker run --rm -it ubuntu:trusty bash` and within the container type `ip addr show eth0` to discover how its main network interface is set up:\nroot@e77f6a1b3740:/# ip addr show eth0\n863: eth0:  mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n```bash\nlink/ether 66:32:13:f0:f1:e3 brd ff:ff:ff:ff:ff:ff\ninet 172.17.1.192/16 scope global eth0\n```\n       valid_lft forever preferred_lft forever\n```bash\ninet6 fe80::6432:13ff:fef0:f1e3/64 scope link\n```\n       valid_lft forever preferred_lft forever\n\nHere my container has the IP address `172.17.1.192`. Now look at the routing table:\nroot@e77f6a1b3740:/# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         172.17.42.1     0.0.0.0         UG    0      0        0 eth0\n172.17.0.0      *               255.255.0.0     U     0      0        0 eth0\n\nSo the IP address of the Docker host `172.17.42.1` is set as the default route and is accessible from your container.\nroot@e77f6a1b3740:/# ping 172.17.42.1\nPING 172.17.42.1 (172.17.42.1) 56(84) bytes of data.\n64 bytes from 172.17.42.1: icmp_seq=1 ttl=64 time=0.070 ms\n64 bytes from 172.17.42.1: icmp_seq=2 ttl=64 time=0.201 ms\n64 bytes from 172.17.42.1: icmp_seq=3 ttl=64 time=0.116 ms\n\ndocker run --network=\"host\"\nAlternatively you can run a Docker container with network settings set to `host`. Such a container will share the network stack with the Docker host and from the container point of view, `localhost` (or `127.0.0.1`) will refer to the Docker host.\nBe aware that any port opened in your Docker container would be opened on the Docker host. And this without requiring the `-p` or `-P` `docker run` option.\nIP configuration on my Docker host:\n[vagrant@docker:~] $ ip addr show eth0\n2: eth0:  mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n```bash\nlink/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\ninet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n```\n       valid_lft forever preferred_lft forever\n```bash\ninet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n```\n       valid_lft forever preferred_lft forever\n\nAnd from a Docker container in host mode:\n[vagrant@docker:~] $ docker run --rm -it --network=host ubuntu:trusty ip addr show eth0\n2: eth0:  mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n```bash\nlink/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\ninet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n```\n       valid_lft forever preferred_lft forever\n```bash\ninet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n```\n       valid_lft forever preferred_lft forever\n\nAs you can see, both the Docker host and Docker container share the exact same network interface and as such have the same IP address.\n\nConnecting to MySQL from containers\nBridge mode\nTo access MySQL running on the Docker host from containers in bridge mode, you need to make sure the MySQL service is listening for connections on the `172.17.42.1` IP address.\nTo do so, make sure you have either `bind-address = 172.17.42.1` or `bind-address = 0.0.0.0` in your MySQL configuration file (my.cnf).\nIf you need to set an environment variable with the IP address of the gateway, you can run the following code in a container:\nexport DOCKER_HOST_IP=$(route -n | awk '/UG[ \\t]/{print $2}')\n\nThen in your application, use the `DOCKER_HOST_IP` environment variable to open the connection to MySQL.\nNote: if you use `bind-address = 0.0.0.0`, your MySQL server will listen for connections on all network interfaces. That means your MySQL server could be reached from the Internet; make sure to set up firewall rules accordingly.\nNote 2: if you use `bind-address = 172.17.42.1` your MySQL server won't listen for connections made to `127.0.0.1`. Processes running on the Docker host that would want to connect to MySQL would have to use the `172.17.42.1` IP address.\nHost mode\nTo access MySQL running on the docker host from containers in host mode, you can keep `bind-address = 127.0.0.1` in your MySQL configuration and connect to `127.0.0.1` from your containers:\n[vagrant@docker:~] $ docker run --rm -it --network=host mysql mysql -h 127.0.0.1 -uroot -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 36\nServer version: 5.5.41-0ubuntu0.14.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql>\n\nNote: Do use `mysql -h 127.0.0.1` and not `mysql -h localhost`; otherwise the MySQL client would try to connect using a Unix socket.",
    "question_score": 3541,
    "answer_score": 4867,
    "question_id": 24319662,
    "has_full_answer": true,
    "tags": [
      "docker",
      "nginx",
      "docker-container",
      "docker-network"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Copying files from Docker container to host",
    "response": "In order to copy a file from a container to the host, you can use the command\n\ndocker cp :/file/path/within/container /host/path/target\n\nHere's an example:\n\n$ sudo docker cp goofy_roentgen:/out_read.jpg .\n\nHere goofy_roentgen is the container name I got from the following command:\n\n$ sudo docker ps\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                            NAMES\n1b4ad9311e93        bamos/openface      \"/bin/bash\"         33 minutes ago      Up 33 minutes       0.0.0.0:8000->8000/tcp, 0.0.0.0:9000->9000/tcp   goofy_roentgen\n\nYou can also use (part of) the Container ID. The following command is equivalent to the first\n\n$ sudo docker cp 1b4a:/out_read.jpg .",
    "question_score": 3011,
    "answer_score": 4800,
    "question_id": 22049212,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-container",
      "file-copying"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to copy files from host to Docker container?",
    "response": "The `cp` command can be used to copy files.\nOne specific file can be copied TO the container like:\ndocker cp foo.txt container_id:/foo.txt\n\nOne specific file can be copied FROM the container like:\ndocker cp container_id:/foo.txt foo.txt\n\nFor emphasis, `container_id` is a container ID, not an image ID. (Use `docker ps` to view listing which includes `container_id`s.)\nMultiple files contained by the folder `src` can be copied into the `target` folder using:\ndocker cp src/. container_id:/target\ndocker cp container_id:/src/. target\n\nReference: Docker CLI docs for `cp`\nIn Docker versions prior to 1.8 it was only possible to copy files from a container to the host. Not from the host to a container.",
    "question_score": 2571,
    "answer_score": 4180,
    "question_id": 22907231,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-container"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How is Docker different from a virtual machine?",
    "response": "Docker originally used LinuX Containers (LXC), but later switched to runC (formerly known as libcontainer), which runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. Also, it uses a layered filesystem (AuFS) and manages networking.\n\nAuFS is a layered file system, so you can have a read only part and a write part which are merged together. One could have the common parts of the operating system as read only (and shared amongst all of your containers) and then give each container its own mount for writing.\n\nSo, let's say you have a 1 GB container image; if you wanted to use a full VM, you would need to have 1 GB x number of VMs you want. With Docker and AuFS you can share the bulk of the 1 GB between all the containers and if you have 1000 containers you still might only have a little over 1 GB of space for the containers OS (assuming they are all running the same OS image).\n\nA full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources). With Docker you get less isolation, but the containers are lightweight (require fewer resources). So you could easily run thousands of containers on a host, and it won't even blink. Try doing that with Xen, and unless you have a really big host, I don't think it is possible.\n\nA full virtualized system usually takes minutes to start, whereas Docker/LXC/runC containers take seconds, and often even less than a second.\n\nThere are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then Docker/LXC/runC seems to be the way to go.\n\nFor more information, check out this set of blog posts which do a good job of explaining how LXC works.\n\n  Why is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment?\n\nDeploying a consistent production environment is easier said than done. Even if you use tools like Chef and Puppet, there are always OS updates and other things that change between hosts and environments.\n\nDocker gives you the ability to snapshot the OS into a shared image, and makes it easy to deploy on other Docker hosts. Locally, dev, qa, prod, etc.: all the same image. Sure you can do this with other tools, but not nearly as easily or fast.\n\nThis is great for testing; let's say you have thousands of tests that need to connect to a database, and each test needs a pristine copy of the database and will make changes to the data. The classic approach to this is to reset the database after every test either with custom code or with tools like Flyway - this can be very time-consuming and means that tests must be run serially. However, with Docker you could create an image of your database and run up one instance per test, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. Since the tests are running in parallel and in Docker containers they could run all on the same box at the same time and should finish much faster. Try doing that with a full VM.\n\nFrom comments...\n\n  Interesting! I suppose I'm still confused by the notion of \"snapshot[ting] the OS\". How does one do that without, well, making an image of the OS?\n\nWell, let's see if I can explain. You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system: as mentioned above, Docker uses AuFS. AuFS merges the different layers together and you get what you want; you just need to run it. You can keep adding more and more images (layers) and it will continue to only save the diffs. Since Docker typically builds on top of ready-made images from a registry, you rarely have to \"snapshot\" the whole OS yourself.",
    "question_score": 4492,
    "answer_score": 4036,
    "question_id": 16047306,
    "has_full_answer": true,
    "tags": [
      "docker",
      "virtual-machine"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to copy Docker images from one host to another without using a repository",
    "response": "You will need to save the Docker image as a tar file:\ndocker save -o  \n\nThen copy your image to a new system with regular file transfer tools such as `cp`, `scp`, or `rsync` (preferred for big files). After that you will have to load the image into Docker:\ndocker load -i \n\nYou should add filename (not just directory) with -o, for example:\ndocker save -o c:/myfile.tar centos:16\n\nyour image syntax may need the repository prefix (:latest tag is default)\ndocker save -o C:\\path\\to\\file.tar repository/imagename\n\nPS: You may need to `sudo` all commands.",
    "question_score": 2417,
    "answer_score": 3881,
    "question_id": 23935141,
    "has_full_answer": true,
    "tags": [
      "docker"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to get a Docker container's IP address from the host",
    "response": "This solution only works if the container is connected with a single network. The `--format` option of `inspect` comes to the rescue.\nModern Docker client syntax is:\ndocker inspect \\\n  -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id\n\nOld Docker client syntax is:\ndocker inspect \\\n  --format '{{ .NetworkSettings.IPAddress }}' container_name_or_id\n\nThese commands will return the Docker container's IP address.\nAs mentioned in the comments: if you are on Windows, use double quotes `\"` instead of single quotes `'` around the curly braces.",
    "question_score": 2252,
    "answer_score": 3702,
    "question_id": 17157721,
    "has_full_answer": true,
    "tags": [
      "docker",
      "ip-address"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How do I get into a Docker container's shell?",
    "response": "`docker attach` will let you connect to your Docker container, but this isn't really the same thing as `ssh`.  If your container is running a webserver, for example, `docker attach` will probably connect you to the stdout of the web server process.  It won't necessarily give you a shell.\nThe `docker exec` command is probably what you are looking for; this will let you run arbitrary commands inside an existing container.  For example, to run `bash` inside a container:\ndocker exec -it  sh\n\nOf course, whatever command you are running must exist in the container filesystem; if your container doesn't have `sh`, this will fail with something like:\nOCI runtime exec failed: exec failed: unable to start container process:\nexec: \"sh\": executable file not found in $PATH: unknown\n\n[If your container doesn't have `sh` -- which is a common case for minimal images -- you may need to investigate other ways to explore the container filesystem.]\nIn the above command `` is the name or ID of the target container.  It doesn't matter whether or not you're using `docker compose`; just run `docker ps` and use either the ID (a hexadecimal string displayed in the first column) or the name (displayed in the final column).  E.g., given:\n$ docker ps\nd2d4a89aaee9        larsks/mini-httpd   \"mini_httpd -d /cont   7 days ago          Up 7 days                               web                 \n\nI can run:\n$ docker exec -it web ip addr\n1: lo:  mtu 65536 qdisc noqueue state UNKNOWN \n```bash\nlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\ninet 127.0.0.1/8 scope host lo\n```\n       valid_lft forever preferred_lft forever\n```bash\ninet6 ::1/128 scope host \n```\n       valid_lft forever preferred_lft forever\n18: eth0:  mtu 1500 qdisc noqueue state UP \n```bash\nlink/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\ninet 172.17.0.3/16 scope global eth0\n```\n       valid_lft forever preferred_lft forever\n```bash\ninet6 fe80::42:acff:fe11:3/64 scope link \n```\n       valid_lft forever preferred_lft forever\n\nI could accomplish the same thing by running:\n$ docker exec -it d2d4a89aaee9 ip addr\n\nSimilarly, I could start a shell in the container;\n$ docker exec -it web sh\n/ # echo This is inside the container.\nThis is inside the container.\n/ # exit\n$\n\nIn commands shown in this answer, the `-i` and `-t` options (combined as `-it`) are necessary to get an interactive shell:\n\n`-i` keeps stdin connected; if you don't specify `-i`, the shell will simply exit.\n\n`-t` allocates a tty device; if you don't specify `-t`, you won't have a very pleasant interactive experience (there will be no shell prompt or job control, for example).\n\nIf you're specifically using `docker compose`, there is a convenience `docker compose exec` command that works very much like the `docker exec` command, except:\n\nIt defaults to the behavior of `-i` and `-t`\nIt allows you to refer to containers by their service name in your `compose.yaml` file.\n\nFor example, if you have a `compose.yaml` like this:\nservices:\n  web:\n```bash\nimage: docker.io/alpinelinux/darkhttpd\n```\n\nThen you can run:\ndocker compose exec web sh\n\nThe equivalent `docker exec` command would be something like:\ndocker exec -it myproject-web-1 sh",
    "question_score": 2452,
    "answer_score": 3328,
    "question_id": 30172605,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-container"
    ],
    "topic": "docker"
  },
  {
    "instruction": "What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?",
    "response": "You should check the `ADD` and `COPY` documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that `ADD` can do more than `COPY`:\n\n`ADD` allows `` to be a URL\nReferring to comments below, the `ADD` documentation states that:\n\nIf  is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\n\nNote that the Best practices for writing Dockerfiles suggests using `COPY` where the magic of `ADD` is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy `keep_this_archive_intact.tar.gz` into your container, but instead, you spray the contents onto your filesystem.",
    "question_score": 3134,
    "answer_score": 2997,
    "question_id": 24958140,
    "has_full_answer": true,
    "tags": [
      "docker",
      "dockerfile"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to force Docker for a clean build of an image",
    "response": "There's a `--no-cache` option:\n\ndocker build --no-cache -t u12_core -f u12_core .\n\nIn older versions of Docker you needed to pass `--no-cache=true`, but this is no longer the case.",
    "question_score": 1693,
    "answer_score": 2744,
    "question_id": 35594987,
    "has_full_answer": true,
    "tags": [
      "docker"
    ],
    "topic": "docker"
  },
  {
    "instruction": "What is the difference between CMD and ENTRYPOINT in a Dockerfile?",
    "response": "Docker has a default entrypoint which is `/bin/sh -c` but does not have a default command.\nWhen you run docker like this:\n`docker run -i -t ubuntu bash`\nthe entrypoint is the default `/bin/sh -c`, the image is `ubuntu` and the command is `bash`.\nThe command is run via the entrypoint. i.e., the actual thing that gets executed is `/bin/sh -c bash`. This allowed Docker to implement `RUN` quickly by relying on the shell's parser.\nLater on, people asked to be able to customize this, so `ENTRYPOINT` and `--entrypoint` were introduced.\nEverything after the image name, `ubuntu` in the example above, is the command and is passed to the entrypoint. When using the `CMD` instruction, it is exactly as if you were executing\n`docker run -i -t ubuntu `\nThe parameter of the entrypoint is ``.\nYou will also get the same result if you instead type this command `docker run -i -t ubuntu`: a bash shell will start in the container because in the ubuntu Dockerfile a default `CMD` is specified:\n`CMD [\"bash\"]`.\nAs everything is passed to the entrypoint, you can have a very nice behavior from your images. @Jiri example is good, it shows how to use an image as a \"binary\". When using `[\"/bin/cat\"]` as entrypoint and then doing `docker run img /etc/passwd`, you get it, `/etc/passwd` is the command and is passed to the entrypoint so the end result execution is simply `/bin/cat /etc/passwd`.\nAnother example would be to have any cli as entrypoint. For instance, if you have a redis image, instead of running `docker run redisimg redis -H something -u toto get key`, you can simply have `ENTRYPOINT [\"redis\", \"-H\", \"something\", \"-u\", \"toto\"]` and then run like this for the same result: `docker run redisimg get key`.",
    "question_score": 2859,
    "answer_score": 2715,
    "question_id": 21553353,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-entrypoint",
      "docker-cmd"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to fix Docker: Permission denied",
    "response": "If you want to run Docker as a non-root user, then you need to add your user to the `docker` group.\n\nCreate the `docker` group if it does not exist:\n\n$ sudo groupadd docker\n\nAdd your user to the `docker` group:\n\n$ sudo usermod -aG docker $USER\n\nLog in to the new `docker` group (to avoid having to log out and log in again; but if not enough, try to reboot):\n\n$ newgrp docker\n\nCheck if Docker can be run without root:\n\n$ docker run hello-world\n\nReboot if you still get an error:\n$ reboot\n\nFrom the official Docker documentation \"Manage Docker as a non-root user\":\n⚠️ Warning\n\nThe `docker` group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.",
    "question_score": 1366,
    "answer_score": 2571,
    "question_id": 48957195,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-compose"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How do I pass environment variables to Docker containers?",
    "response": "You can pass environment variables to your containers with the `-e` (alias `--env`) flag.\n`docker run -e xx=yy`\nAn example from a startup script:\nsudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\ \n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\  \n--name container_name dockerhub_id/image_name\n\nOr, if you don't want to have the value on the command-line where it will be displayed by `ps`, etc., `-e` can pull in the value from the current environment if you just give it without the `=`:\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\n\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n$ docker run --env-file ./env.list ubuntu bash\n\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #",
    "question_score": 1512,
    "answer_score": 2330,
    "question_id": 30494050,
    "has_full_answer": true,
    "tags": [
      "docker",
      "environment-variables",
      "dockerfile"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to remove old Docker containers",
    "response": "Since Docker 1.13.x you can use Docker container prune:\n\ndocker container prune\n\nThis will remove all stopped containers and should work on all platforms the same way.\n\nThere is also a Docker system prune:\n\ndocker system prune\n\nwhich will clean up all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes, in one command.\n\nFor older Docker versions, you can string Docker commands together with other Unix commands to get what you need. Here is an example on how to clean up old containers that are weeks old:\n\n$ docker ps --filter \"status=exited\" | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rm\n\nTo give credit, where it is due, this example is from https://twitter.com/jpetazzo/status/347431091415703552.",
    "question_score": 1497,
    "answer_score": 1837,
    "question_id": 17236796,
    "has_full_answer": true,
    "tags": [
      "docker"
    ],
    "topic": "docker"
  },
  {
    "instruction": "What is the difference between a Docker image and a container?",
    "response": "An instance of an image is called a container. You have an image, which is a set of layers as you describe. If you start this image, you have a running container of this image. You can have many running containers of the same image.\n\nYou can see all your images with `docker images` whereas you can see your running containers with `docker ps` (and you can see all containers with `docker ps -a`).\n\nSo a running instance of an image is a container.",
    "question_score": 1254,
    "answer_score": 1647,
    "question_id": 23735149,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-container",
      "docker-image"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Should I use Vagrant or Docker for creating an isolated environment?",
    "response": "If your purpose is the isolation, I think Docker is what you want.\n\nVagrant is a virtual machine manager. It allows you to script the virtual machine configuration as well as the provisioning. However, it is still a virtual machine depending on VirtualBox (or others) with a huge overhead. It requires you to have a hard drive file that can be huge, it takes a lot of ram, and performance may be not very good.\n\nDocker on the other hand uses kernel cgroup and namespacing via LXC. It means that you are using the same kernel as the host and the same file system.\nYou can use Dockerfile with the `docker build` command in order to handle the provisioning and configuration of your container. You have an example at docs.docker.com on how to make your Dockerfile; it is very intuitive.\n\nThe only reason you could want to use Vagrant is if you need to do BSD, Windows or other non-Linux development on your Ubuntu box. Otherwise, go for Docker.",
    "question_score": 2207,
    "answer_score": 1188,
    "question_id": 16647069,
    "has_full_answer": true,
    "tags": [
      "vagrant",
      "docker"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How can I delete all local Docker images?",
    "response": "Unix\nTo delete all containers including its volumes use,\n\ndocker rm -vf $(docker ps -aq)\n\nTo delete all the images,\ndocker rmi -f $(docker images -aq)\n\nRemember, you should remove all the containers before removing all the images from which those containers were created.\nWindows - Powershell\ndocker images -a -q | % { docker image rm $_ -f }\n\nWindows - cmd.exe\nfor /F %i in ('docker images -a -q') do docker rmi -f %i",
    "question_score": 1135,
    "answer_score": 2377,
    "question_id": 44785585,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-compose"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to remove old and unused Docker images",
    "response": "(see below for original answer)\n\nUpdate Sept. 2016: Docker 1.13: PR 26108 and commit 86de7c0 introduce a few new commands to help facilitate visualizing how much space the docker daemon data is taking on disk and allowing for easily cleaning up \"unneeded\" excess.\n`docker system prune` will delete all dangling data (containers, networks, and images). You can remove all unused volumes with the `--volumes` option and remove all unused images (not just dangling) with the `-a` option.\nYou also have:\n\n`docker container prune`\n`docker image prune`\n`docker network prune`\n`docker volume prune`\n\nFor unused images, use `docker image prune -a` (for removing dangling and ununsed images).\nWarning: 'unused' means \"images not referenced by any container\": be careful before using `-a`.\nAs illustrated in A L's answer, `docker system prune --all` will remove all unused images not just dangling ones... which can be a bit too much.\nCombining `docker xxx prune` with the `--filter` option can be a great way to limit the pruning (docker SDK API 1.28 minimum, so docker 17.04+)\n\nThe currently supported filters are:\n\n`until ()` - only remove containers, images, and networks created before given timestamp\n`label` (`label=`, `label==`, `label!=`, or `label!==`) - only remove containers, images, networks, and volumes with (or without, in case `label!=...` is used) the specified labels.\n\nSee \"Prune images\" for an example.\n\nWarning: there is no \"preview\" or \"`--dry-run`\" option for those `docker xxx prune` commands.\nThis is requested with `moby/moby` issue 30623 since 2017, but seems tricky to be implemented (Aug. 2022)\n\nHaving a more representative overview of what will be pruned will be quite complicated, for various reasons;\n\nrace conditions (can be resolved by documenting the limitations);\nA container/image/volume/network may not be in use at the time that \"dry run\" is used, but may be in use the moment the actual prune is executed (or vice-versa), so dry run will always be an \"approximation\" of what will be pruned.\nthe more difficult part is due to how objects (containers, images, networks etc.) depend on each other.\nFor example, an image can be deleted if it no longer has references to it (no more tags, no more containers using it); this is the reason that docker system prune deletes objects in a specific order (first remove all unused containers, then remove unused images).\nIn order to replicate the same flow for \"dry-run\", it will be needed to temporarily construct representation of all objects and where they're referenced based on that (basically; duplicate all reference-counters, and then remove references from that \"shadow\" representation).\nFinally; with the work being done on integrating the `containerd` snapshotter (image and layer store), things may change more;\nFor example, images can now be multi-arch, and (to be discussed), \"pruning\" could remove unused variants (architectures) from an image to clean up space, which brings another dimension to calculating \"what can be removed\".\n\nOriginal answer (Sep. 2016)\nI usually do:\n\ndocker rmi $(docker images --filter \"dangling=true\" -q --no-trunc)\n\nI have an [alias for removing those dangling images: `drmi`]13\n\nThe `dangling=true` filter finds unused images\n\nThat way, any intermediate image no longer referenced by a labelled image is removed.\nI do the same first for exited processes (containers)\nalias drmae='docker rm $(docker ps -qa --no-trunc --filter \"status=exited\")'\n\nAs haridsv points out in the comments:\n\nTechnically, you should first clean up containers before cleaning up images, as this will catch more dangling images and less errors.\n\nJess Frazelle (jfrazelle) has the bashrc function:\ndcleanup(){\n```bash\ndocker rm -v $(docker ps --filter status=exited -q 2>/dev/null) 2>/dev/null\ndocker rmi $(docker images --filter dangling=true -q 2>/dev/null) 2>/dev/null\n```\n}\n\nTo remove old images, and not just \"unreferenced-dangling\" images, you can consider `docker-gc`:\n\nA simple Docker container and image garbage collection script.\n\nContainers that exited more than an hour ago are removed.\nImages that don't belong to any remaining container after that are removed.",
    "question_score": 1173,
    "answer_score": 2004,
    "question_id": 32723111,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-image"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How to list containers in Docker",
    "response": "To show only running containers run:\ndocker ps\n\nTo show all containers run:\ndocker ps -a\n\nTo show the latest created container (includes all states) run:\ndocker ps -l\n\nTo show n last created containers (includes all states) run:\ndocker ps -n=-1\n\nTo display total file sizes run:\ndocker ps -s\n\nThe content presented above is from docker.com.\nIn the new version of Docker, commands are updated, and some management commands are added:\ndocker container ls\n\nIt is used to list all the running containers includes all states.\ndocker container ls -a\n\nAnd then, if you want to clean them all,\ndocker rm $(docker ps -aq)\n\nIt is used to list all the containers created irrespective of its state.\nAnd to stop all the Docker containers (force)\ndocker rm -f $(docker ps -a -q)  \n\nHere the container is the management command.",
    "question_score": 964,
    "answer_score": 1890,
    "question_id": 16840409,
    "has_full_answer": true,
    "tags": [
      "docker"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Docker how to change repository name or rename image?",
    "response": "docker image tag server:latest myname/server:latest\n\nor\n\ndocker image tag d583c3ac45fd myname/server:latest\n\nTags are just human-readable aliases for the full image name (`d583c3ac45fd...`). \n\nSo you can have as many of them associated with the same image as you like. If you don't like the old name you can remove it after you've retagged it:\n\ndocker rmi server\n\nThat will just remove the `alias/tag`. Since `d583c3ac45fd` has other names, the actual image won't be deleted.",
    "question_score": 972,
    "answer_score": 1686,
    "question_id": 25211198,
    "has_full_answer": true,
    "tags": [
      "docker",
      "linux-containers"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Docker Compose - How to execute multiple commands?",
    "response": "Figured it out, use `bash -c`.\n\nExample:\n\ncommand: bash -c \"python manage.py migrate && python manage.py runserver 0.0.0.0:8000\"\n\nSame example in multilines:\n\ncommand: >\n```bash\nbash -c \"python manage.py migrate\n&& python manage.py runserver 0.0.0.0:8000\"\n```\n\nOr:\n\ncommand: bash -c \"\n```bash\npython manage.py migrate\n&& python manage.py runserver 0.0.0.0:8000\n```\n  \"",
    "question_score": 1022,
    "answer_score": 1537,
    "question_id": 30063907,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-compose"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Error \"The input device is not a TTY\"",
    "response": "Remove the `-it` from your cli to make it non interactive and remove the TTY. If you don't need either, e.g. running your command inside of a Jenkins or cron script, you should do this.\nOr you can change it to `-i` if you have input piped into the docker command that doesn't come from a TTY. If you have something like `xyz | docker ...` or `docker ... \nOr you can change it to `-t` if you want TTY support but don't have it available on the input device. Do this for apps that check for a TTY to enable color formatting of the output in your logs, or for when you later attach to the container with a proper terminal.\nOr if you need an interactive terminal and aren't running in a terminal on Linux or MacOS, use a different command line interface. PowerShell is reported to include this support on Windows.\n\nWhat is a TTY? It's a terminal interface that supports escape sequences, moving the cursor around, etc, that comes from the old days of dumb terminals attached to mainframes. Today it is provided by the Linux command terminals and ssh interfaces. See the wikipedia article for more details.\nTo see the difference of running a container with and without a TTY, run a container without one: `docker run --rm -i ubuntu bash`. From inside that container, install vim with `apt-get update; apt-get install vim`. Note the lack of a prompt. When running vim against a file, try to move the cursor around within the file.",
    "question_score": 1002,
    "answer_score": 1417,
    "question_id": 43099116,
    "has_full_answer": true,
    "tags": [
      "docker",
      "jenkins",
      "jenkins-pipeline"
    ],
    "topic": "docker"
  },
  {
    "instruction": "docker push error: denied: requested access to the resource is denied",
    "response": "You may need to switch your docker repo to private before docker push.\n\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n\n# you may need log out first `docker logout` ref. https://stackoverflow.com/a/53835882/248616\ndocker login\n\nAccording to the docs:\n\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\n\nSo, this means you have to tag your image before pushing:\n\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\n\nand then you should be able to push it.\n\ndocker push YOUR_DOCKERHUB_NAME/firstimage",
    "question_score": 848,
    "answer_score": 1310,
    "question_id": 41984399,
    "has_full_answer": true,
    "tags": [
      "docker",
      "dockerfile"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Exploring Docker container's file system",
    "response": "Here are a couple different methods...\nA) Use docker exec (easiest)\nDocker version 1.3 or newer supports the command `exec` that behave similar to `nsenter`. This command can run new process in already running container (container must have PID 1 process running already). You can run `/bin/bash` to explore container state:\ndocker exec -t -i mycontainer /bin/bash\n\nsee Docker command line documentation\nB) Use Snapshotting\nYou can evaluate container filesystem this way:\n\n# find ID of your running container:\ndocker ps\n\n# create image (snapshot) from container filesystem\ndocker commit 12345678904b5 mysnapshot\n\n# explore this filesystem using bash (for example)\ndocker run -t -i mysnapshot /bin/bash\n\nThis way, you can evaluate filesystem of the running container in the precise time moment. Container is still running, no future changes are included.\nYou can later delete snapshot using (filesystem of the running container is not affected!):\ndocker rmi mysnapshot\n\nC) Use ssh\nIf you need continuous access, you can install sshd to your container and run the sshd daemon:\ndocker run -d -p 22 mysnapshot /usr/sbin/sshd -D\n \n# you need to find out which port to connect:\ndocker ps\n\nThis way, you can run your app using ssh (connect and execute what you want).\nD) Use nsenter\nUse `nsenter`, see Why you don't need to run SSHd in your Docker containers\n\nThe short version is: with nsenter, you can get a shell into an\nexisting container, even if that container doesn’t run SSH or any kind\nof special-purpose daemon",
    "question_score": 1134,
    "answer_score": 1138,
    "question_id": 20813486,
    "has_full_answer": true,
    "tags": [
      "linux",
      "docker",
      "filesystems"
    ],
    "topic": "docker"
  },
  {
    "instruction": "Run a Docker image as a container",
    "response": "The specific way to run it depends on whether you gave the image a tag/name or not.\n\n$ docker images\nREPOSITORY          TAG                 ID                  CREATED             SIZE\nubuntu              12.04               8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\n\nWith a name (let's use Ubuntu):\n\n$ docker run -i -t ubuntu:12.04 /bin/bash\n\nWithout a name, just using the ID:\n\n$ docker run -i -t 8dbd9e392a96 /bin/bash\n\nPlease see Docker run reference for more information.",
    "question_score": 819,
    "answer_score": 1120,
    "question_id": 18497688,
    "has_full_answer": true,
    "tags": [
      "docker",
      "docker-image"
    ],
    "topic": "docker"
  },
  {
    "instruction": "How do I edit a file after I shell to a Docker container?",
    "response": "As in the comments, there's no default editor set - strange - the `$EDITOR` environment variable is empty. You can log in into a container with:\ndocker exec -it  bash\n\nAnd run:\napt-get update\napt-get install vim\n\nOr use the following Dockerfile:\nFROM  confluent/postgres-bw:0.1\n\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"vim\"]\n\nDocker images are delivered trimmed to the bare minimum - so no editor is installed with the shipped container. That's why there's a need to install it manually.\nEDIT\nI also encourage you to read my post about the topic.",
    "question_score": 870,
    "answer_score": 1105,
    "question_id": 30853247,
    "has_full_answer": true,
    "tags": [
      "docker",
      "command-line-interface",
      "editor"
    ],
    "topic": "docker"
  }
]